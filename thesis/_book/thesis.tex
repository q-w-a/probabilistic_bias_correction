% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
  %% Preamble
%%
  % \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{smiththesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: https://www.ctan.org/
  %%
  \usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% End of CII addition
\usepackage{rotating}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\hypersetup{
     colorlinks=true,
     linkcolor=blue,
     filecolor=blue,
     citecolor = black,      
     urlcolor=cyan,
     }
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino
\usepackage{palatino}
% Syntax highlighting #22

% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Estimating Unobserved COVID-19 Infections in the United States}
\author{Quinn White}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2023}
\division{Mathematics and Natural Sciences}
\advisor{Ben Baumer}
\institution{Smith College}
\degree{Bachelor of Arts}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
\altadvisor{Nicholas Reich}
% End of CII addition

%%% Remember to use the correct department!
\department{Statistical and Data Sciences}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
% As noted by @mirh [2] is needed instead of [3] for 2.12
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
\setlength{\parindent}{0pt}
% turn on hanging indent if param 1 is 1
\ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
% set entry spacing
\ifnum #2 > 0
\setlength{\parskip}{#2\baselineskip}
  \fi
}%
{}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
  \newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
    \newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
      \newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
        \renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
Will add
}

\Dedication{
You can have a dedication here if you wish.
}

\Preface{
I am unsure as to what goes here.
}

\Abstract{
As we have navigated the COVID-19 pandemic, case counts have been a central source of information for understanding transmission dynamics and the effect of public health interventions. However, because the number of cases we observe is limited by the testing effort in a given location, the case counts presented on local or national dashboards are only a fraction of the true infections. Variations in testing rate by time and location impacts the number of cases that go unobserved, which can cloud our understanding of the true COVID-19 incidence at a given time point and can create biases in downstream analyses. Additionally, the number of cases we observe is impacted by the sensitivity and specificity of the diagnostic test. To quantify the number of true infections given incomplete testing and diagnostic test inaccuracy, this work implements probabilistic bias analysis at a biweekly time scale from January 1, 2021 through February 2022. In doing so, we can estimate a range of possible true infections for a given time interval and location. This approach can be applied at the state level across the United States, as well as in some counties where the needed data are available.
}

	\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End of CII addition
%%
%% End Preamble
%%
%
\begin{document}

% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
  \begin{acknowledgements}
    Will add
  \end{acknowledgements}
  \begin{preface}
    I am unsure as to what goes here.
  \end{preface}
  \hypersetup{linkcolor=black}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \listoftables

  \listoffigures
  \begin{abstract}
    As we have navigated the COVID-19 pandemic, case counts have been a central source of information for understanding transmission dynamics and the effect of public health interventions. However, because the number of cases we observe is limited by the testing effort in a given location, the case counts presented on local or national dashboards are only a fraction of the true infections. Variations in testing rate by time and location impacts the number of cases that go unobserved, which can cloud our understanding of the true COVID-19 incidence at a given time point and can create biases in downstream analyses. Additionally, the number of cases we observe is impacted by the sensitivity and specificity of the diagnostic test. To quantify the number of true infections given incomplete testing and diagnostic test inaccuracy, this work implements probabilistic bias analysis at a biweekly time scale from January 1, 2021 through February 2022. In doing so, we can estimate a range of possible true infections for a given time interval and location. This approach can be applied at the state level across the United States, as well as in some counties where the needed data are available.
  \end{abstract}
  \begin{dedication}
    You can have a dedication here if you wish.
  \end{dedication}
\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{motivation}{%
\chapter{Motivation}\label{motivation}}

Placeholder

\hypertarget{background}{%
\chapter{Background}\label{background}}

\hypertarget{probabalistic-bias-analysis}{%
\section{Probabalistic Bias Analysis}\label{probabalistic-bias-analysis}}

Often the focus of quantifying error about an effect estimate focuses on random error rather than the systematic error. For example, typical frequentist confidence intervals are frequent in medical and epidemiological literature, although they have faced rising criticism (Greenland et al., 2016). These confidence intervals quantify the fraction of the times we expect the true value to fall in this interval under the assumption that our model is correct. That is, if we ran an experiment 100 times and computed the effect size each time, we would expect the 95\% confidence interval to contain the true value to 95 of those times, on average. Neyman stressed this in his original publication formalizing the concept of a confidence interval in 1937 (Neyman, 1937). The nuance that the confidence interval is not the probability that the true value falls within this interval, however, is often lost in the discussion of results, in part because the true meaning of a confidence interval is less intuitive.

The aim of quantitative bias analysis is to estimate systematic error to give a range of possible values for the true quantity of interest. In this sense, it is a type of sensitivity analysis. It can be used to estimate various kinds of biases, from misclassification, as is implemented in this work, as well as selection bias and unmeasured confounding (Petersen, Ranker, Barnard-Mayers, MacLehose, \& Fox, 2021). Often, the goal of performing such an analysis is to see how these sources of bias affect our estimates; in particular, under what situations of bias the observed effect would be null.

There are multiple different forms of bias analysis (Lash, Fox, \& Fink, 2009). The most simple case, simple bias analysis, is correcting a point estimate for a single source of error. Multidimensional bias analysis extends this to consider sets of bias parameters, but still provides a corrected point estimate rather than a range of plausible estimates. Probabilistic bias analysis, meanwhile, defines probability distributions for bias parameters to generate a distribution of corrected estimates by repeatedly correcting estimates for bias under different combinations of the parameter values. Then, via Monte Carlo we obtain a distribution of corrected estimates that reflect the corrected values under different scenarios of bias, that is, under different combinations of the bias parameters. This can give us a better idea for the extent of uncertainty about the corrected estimates, although this uncertainty does depend on the specification of the bias parameter distributions. Inherent in bias analysis is the dependence of our results on the specification of bias parameters, which reflect what is known from available data, literature, or theory on the extent of bias that may occur. There is uncertainty about how we define these distributions or values; otherwise, if the precise values of the bias parameters were known, we could simply correct the estimates and probabilistic bias analysis would not be useful.

Although some forms of probabilistic bias analysis can be applied to summarized data, for example, frequencies in a contingency table, the methods are most often implemented with unsummarized data in its original form, as implemented here.

In choosing specific distributions for the bias parameters, different specifications may yield density functions where most of the density is within a similar interval (MAKE PLOT WITH EXAMPLE), which means the choice of the specific distribution will not be sensitive to the particular choice of density.

\hypertarget{bayesian-melding}{%
\section{Bayesian Melding}\label{bayesian-melding}}

\hypertarget{theoretical-background-for-the-approach}{%
\subsection{Theoretical Background for the Approach}\label{theoretical-background-for-the-approach}}

The Bayesian melding approach was proposed by Poole et al. (Poole \& Raftery, 2000).

The Bayesian melding approach enables us to account for both uncertainty from inputs and outputs of a deterministic model. The initial motivation for the approach was to study the population dynamics of whales in the presence of substantial uncertainty around model inputs for population growth (Poole \& Raftery, 2000). However, the framework provided by Poole et al.~can applied in any circumstance where we have uncertainty around some quantities \(\theta\) and \(\phi\) where there is a deterministic function \(M:\theta \to\phi\). Due the utility of Bayesian melding in various contexts, since this deterministic model \(M\) could take on a wide range of forms, the approach has since been applied in various fields, including urban simulations (Ševčíková, Raftery, \& Waddell, 2007), ecology (Robson, 2014), and infectious disease (Powers et al., 2011).

At this point, we can define how Bayesian melding works more formally.

Let \(M: \theta \to \phi\) be the deterministic model defined by the function relating a vector of input parameters \(\theta\) to an output vector \(\phi\), and suppose we have a prior on \(\theta\) denoted \(q_1(\theta)\) and a prior on \(\phi\) denoted \(q_2(\phi)\).

However, note that we actually have two distinct priors on \(\phi\). There is the prior formed by the distribution induced on \(\phi\) by the prior for \(\theta\) and the function \(M\), where we denote this induced prior \(q^*_1(\phi)\).
If \(M^{-1}\) exists, we can write this induced prior \(q_1^*(\phi) = q_1(M^{-1}(\phi)) |J(\phi)|\). This result follows from the fact \(M(\theta) = \phi\), so we apply a change of variables to obtain the distribution of \(\phi\) from the distribution of \(\theta\). This is a generalization to the multivariate case of the change of variables result often covered in probability courses in the univariate case. That is, if we have a continuous random variable \(X\) with probability density function \(f_X\) and \(Y=g(X)\) for a differentiable monotonic function, then the probability density function of \(Y\) is \(f_Y(y) = f_X(g^{-1}(y)) \Big| \frac{dx}{dy} \Big|\). In practice, \(M^{-1}\) rarely exists exists since \(\theta\) is often of higher dimensionality then \(\phi\), in which cases \(M\) is not invertible. This means we generally approximate \(q^*_1(\phi)\) without acquiring its analytical form.

In addition to this induced prior, we have the prior \(q_2(\phi)\), which does not involve \(M\) nor the inputs \(\theta\). Since these priors are based on different sources of information and may reflect different uncertainties, often it useful to use both sources of information to inform our estimates. To do so, we need to combine the distributions for \(q^*_1(\phi)\) and \(q_2(\phi)\) to create a pooled distribution.

Multiple pooling strategies exist for distinct distributions, but one requirement for a Bayesian analysis is that the distribution should be independent of the order in which the prior is updated and the combining of the prior distributions. That is, updating the prior distributions using Bayes' theorem and then combining distributions should yield the same result as combining distributions and then updating this combined distribution; pooling methods that have this property are deemed externally Bayesian. Logarithmic pooling has been shown to be externally Bayesian under some conditions, which are likely to hold in most settings. Furthermore, logarithmic pooling has actually been shown to be the only pooling method where this holds (Genest, McConway, \& Schervish, 1986).

The logarithmically pooled prior for \(\phi\) by pooling \(q^*_1(\phi)\) and \(q_2(\phi)\) is proportional to

\[q^*_1(\phi)^{\alpha} q_2(\phi)^{1-\alpha}\]

where \(\alpha \in [0,1]\) is a pooling weight. Commonly, a choice of \(\alpha = 0.5\) is used to give the priors equal weight. In this case, logarithmic pooling may be referred to as geometric pooling since it is equivalent to taking a geometric mean.

If \(M\) is invertible, we can obtain the contrained distributions for the model inputs by simply inverting. However, this is rare, so we have to think about how to proceed in the noninvertible case.

To get intuition for a valid strategy, consider a mapping \(M: \theta \to \phi\) for \(\theta \in \mathbb{R}\) and \(\phi \in \mathbb{R}\)
defined as follows. Note the choice of \(q_1,q_2\) does not matter here as long as they are valid densities.
\begin{table}[H]
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
$\theta$ & $q_1(\theta)$ & $\phi$ & $q_2(\phi)$\\
\hline
1 & 0.3 & 1 & 0.4\\
\hline
2 & 0.2 & 2 & 0.6\\
\hline
3 & 0.5 & 2 & 0.6\\
\hline
\end{tabular}
\end{table}
We see that \(M\) is not invertible since \(\theta=1\) and \(\theta = 2\) both map to \(\phi=2\), which implies the inverse \(M^{-1}\) would not be well defined.

We can compute \(q_1^*(\phi)\) using our function \(M\) and taking \(q_1^*(\phi) = q_1(M^{-1}(\phi))\); in the continuous case we need to multiply by \(|J(\phi)|\), but not in the discrete case (Blitzstein \& Hwang, 2019).

So we have \(q^*_1(1) = q_1(1) = 0.3\) since \(M(1)=1\), and \(q^*_1(2) = q_1(2) + q_1(3) = 0.2 + 0.5=0.7\) since \(M(2) = 2\) and \(M(3)=2\).

Then, we can compute the logarithmically pooled pooled prior with \(\alpha=0.5\) by taking \(q_1^*(\phi)^{\alpha} q_2(\phi)^{1-\alpha}\).

For \(\phi = 1\), we have \(q_1^*(\phi)^{\alpha} q_2(\phi)^{1-\alpha} = (0.3)^{0.5}(0.4)^{0.5} = 0.3464\)
For \(\phi = 2\), we have \(q_1^*(\phi)^{\alpha} q_2(\phi)^{1-\alpha} = (0.6)^{0.5}(0.7)^{0.5} = 0.6481\)

To make this a valid density, however, these probabilities must sum to 1, so we renormalize by dividing by (0.3464 + 0.6481). This gives us the pooled prior \(q^{\sim[\phi]}(\phi)\) as

\(0.3464 / (0.3464 + 0.6481) = 0.3483\) for \(\phi =1\) and \(0.6481 / (0.3464 + 0.6481) =0.6517\) for \(\phi=2\).

Summarizing these results, we have
\begin{table}[H]
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
$\phi$ & $q_2(\phi)$ & $q_1^*(\phi)$ & $q^{\sim[\phi]}(\phi)$\\
\hline
1 & 0.4 & 0.3 & 0.3483\\
\hline
2 & 0.6 & 0.7 & 0.6517\\
\hline
\end{tabular}
\end{table}
However, we want the pooled prior on the inputs \(\theta\), that is, \(q^{\sim[\theta]}(\theta)\).

Poole et al.~reasoned as follows. Since \(M\) uniquely maps \(\theta=1\) to \(\phi =1\), the probability that \(\theta=1\) should be equal to the probability \(\phi = 1\). That is, we should have \(q^{\sim[\theta]}(1) = q^{\sim[\phi]}(1)\).

However, the relationship for \(\theta=2\) or \(\theta=3\) to \(\phi\) is not one to one, but since \(M(2)=2\) and \(M(3)=2\), the sum of the probabilities for \(\theta=1\) and \(\theta=2\) should be equal to that for \(\phi=2\), that is, \(q^{\sim[\theta]}(2) + q^{\sim[\theta]}(3) = q^{\sim[\phi]}(2) = 0.6517\).

The challenge here is how we divide the probability for \(q^{\sim[\phi]}(2)\), which is defined, among \(q^{\sim[\theta]}(2)\) and \(q^{\sim[\theta]}(3)\). The prior for \(\phi\) yields no information to assist in this choice, because knowing which value \(\phi\) takes on does not give us any information about whether \(\theta=2\) or \(\theta=3\). Thus, the information we have about \(\theta\) must be taken from \(q_1(\theta)\).

That is, we can assign a probability for \(q^{\sim[\theta]}(2)\) by considering the probability that \(\theta = 2\) relative to the probability \(\theta =3\), computing

\[q^{\sim[\theta]}(2) = q^{\sim[\phi]}(2) \Big( \frac{q_1(2)}{q_1(2) + q_1(3)}\Big).\]

That is, if the probability \(\theta\) takes on the value \(2\) is lower in this case than the probability \(\theta=3\) which we know from the prior on \(\theta\), \(q_1(\theta)\), then the pooled prior on \(\theta\), \(q^{\sim[\theta]}(2)\), should reflect this.

Using this reasoning, we have \(q^{\sim[\theta]}(2) = (0.7) \frac{0.2}{0.2+0.5} = 0.1862\) and \(q^{\sim[\theta]}(3) = (0.7) \frac{0.5}{0.2+0.5} = 0.4655\).

The result in this simple example, using \(q_1(\theta)\) to determine how to distribute the probability for values of \(\phi\) where multiple \(\theta\) map to \(\phi\), can be used to derive general formulas to compute \(q^{\sim[\theta]}(\theta)\) for discrete and continuous distributions (Poole \& Raftery, 2000).

\hypertarget{implementation-through-the-sampling-importance-resampling-algorithm}{%
\subsection{Implementation through the Sampling-Importance-Resampling Algorithm}\label{implementation-through-the-sampling-importance-resampling-algorithm}}

The steps are:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We draw \(\theta\) from its prior distribution \(q_1(\theta)\), where we note \(\theta\) can be multidimensional.
\item
  For every \(\theta_i\) we compute \(\phi_i = M(\theta_i)\).
\item
  Since \(q_1^*(\phi)\) is unlikely to have an analytical form, we can compute it via a density approximation by computing \(M(\theta)\) for our sampled values of \(\theta\) and then estimating the density from this sample using kernel density estimation.
\item
  Construct weights proportional to the ratio of the prior on \(\phi\) evaluated at \(M(\theta_i)\) to the induced prior \(q_1^*\) evaluated at \(M(\theta_i)\). Note that this is applying the same logic as considering \(q_1(\theta)/q^*_1(\theta)\), as discussed in the previous concrete example, but representing these probabilities in \(\phi\) space.
\item
  Sample values from step (1) with probabilities proportional to the weights from (4).
\end{enumerate}
\hypertarget{bayesian-melding-applied-to-covid-19-misclassification}{%
\subsection{Bayesian Melding Applied to COVID-19 Misclassification}\label{bayesian-melding-applied-to-covid-19-misclassification}}

In this work, we can relate the inputs \(\theta = \{P(S_1|untested), \alpha, \beta \}\) and \(\phi = P(S_0|test +,untested)\) by the deterministic model \(M: \theta \to \phi\) given by
\[P(S_0|test+, untested) = \frac{\beta(1 - P(S_1|untested))}{\beta(1-P(S_1|untested)) + \alpha P(S_1|untested)}.\] The derivation of \(M\) is in the following section.

Now, we have two distributions on \(\phi\): the distribution based on data on the asymptomatic rate of infection of COVID-19, and the distribution formed by taking \(M(\theta)\) where \(\theta\) represents the values from the defined distributions of \(\alpha,\beta,\) and \(P(S_1|untested)\). With Bayesian melding, we pool these distributions using logarithmic pooling, and then implement the sampling-importance-resampling algorithm to obtain constrained distributions of the inputs \(\theta\) that are in accordance with information about the asymptomatic rate of the virus.

Due to the uncertainty around our definitions of \(\alpha\) and \(\beta\), it is particularly useful to leverage the information we have about the asymptomatic rate of the virus \(P(S_0|test +,untested)\) because a large collection of studies has been published in this area. In a meta-analysis pooling data from 95 studies, the pooled estimate among the confirmed population that was asymptomatic was 40.50\% {[}95\% CI, 33.50\%-47.50\%{]} (Ma et al., 2021). Another meta-analysis including 350 studies estimated the asymptomatic percentage as 36.9\% {[}95\% CI: 31.8 to 42.4\%{]} (Sah et al., 2021).

To clarify the use of this method, we can look at the distributions before and after applying Bayesian melding.

\includegraphics[width=0.95\linewidth]{./figure/melded}

Comparing these priors above, we see that although they have shared support, some values from the induced distribution we acquire by using \(M\) to generate values of \(\phi\) from sampled values of \(\theta\) are very unlikely to be in accordance with the information we know about SARS-CoV-2 asymptomatic infection. This is where Bayesian melding comes into play. Pooling these distributions enable us to take both the prior on \(q_2(\phi)\) from published analyses on asymptomatic infection, and the induced prior, \(q_1^*(\phi)\), into account to constrain the distributions of \(\phi\) and \(\theta\) to be in accordance.

\hypertarget{derivation-of-m}{%
\subsection{\texorpdfstring{Derivation of \(M\)}{Derivation of M}}\label{derivation-of-m}}

Recall that we have the function \(M: \theta \to \phi\) that relates the random variables \(\theta = \{P(S_1|untested), \alpha\}\) to the test positivity rate in the asymptomatic untested population, that is, \(\phi = P(S_0|test_+, \text{untested})\). \(M\) is defined as
\begin{align*}  &= 
 P(S_0|test_+, \text{untested}) = \dfrac{\beta (1- P(S_1|\text{untested}))}{\beta(1- P(S_1|\text{untested})) + \alpha(P(S_1|\text{untested})} \hspace{ 15 mm }
\end{align*}
\noindent Since we have \(\alpha = \frac{P(test_+|S_1, \text{untested})}{P(test_+|tested)}\) and \(\beta = \dfrac{P(test_+|S_0, \text{untested})}{P(test_+|tested)}\), we can write
\begin{align*}  &= \dfrac{\dfrac{P(test_+|S_0, \text{untested})}{P(test_+|tested)}(1 - P(S_1|\text{untested}))}{\dfrac{P(test_+|S_0, \text{untested})}{P(test_+|tested)}(1-P(S_1|\text{untested})) + \dfrac{P(test_+|S_1, \text{untested})}{P(test_+|tested)} P(S_1|\text{untested})}
\end{align*}
and cancelling out the term \(P(test_+|tested)\) we have

\[ = \dfrac{{P(test_+|S_0, \text{untested})}(1 - P(S_1|\text{untested}))}{P(test_+|S_0, \text{untested})(1-P(S_1|\text{untested})) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}.\]

\noindent Since \(P(S_0|\text{untested}) = 1 - P(S_1|\text{untested})\),
\begin{align*} 
&=  \dfrac{{P(test_+|S_0, \text{untested})}P(S_0|\text{untested})}{P(test_+|S_0, \text{untested})P(S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}.
\end{align*}
Applying the definition of conditional probability to the term \textbackslash{} \(P(test_+|S_0, \text{untested})P(S_0|\text{untested})\) in the numerator,
\begin{align*}
&=
    \dfrac{\Big( \dfrac{P(test_+,S_0, \text{untested})}{P(S_0, \text{untested})} \Big) \Big(\dfrac{P(S_0, \text{untested})}{P(\text{untested})}\Big)}{P(test_+|S_0, \text{untested})P(S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}\\ 
    &= \dfrac{\Big( \dfrac{P(test_+,S_0, \text{untested})}{P(S_0, \text{untested})} \Big) \Big(\dfrac{P(S_0, \text{untested})}{P(\text{untested})}\Big)}{P(test_+|S_0, \text{untested})P(S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}\\
    &=  \dfrac{\dfrac{P(test_+,S_0, \text{untested})}{P(\text{untested})}}{P(test_+|S_0, \text{untested})P(S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}\\
    &=  \dfrac{{P(test_+,S_0|\text{untested})}}{P(test_+|S_0, \text{untested})P(S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})}.
\end{align*}
\noindent We can substitute this result in for the \(P(test_+|S_0, \text{untested})P(S_0|\text{untested})\) term in the denominator to yield
\begin{align*}
  &=  \dfrac{{P(test_+,S_0|\text{untested})}}{P(test_+,S_0|\text{untested}) + P(test_+|S_1, \text{untested}) P(S_1|\text{untested})} \hspace{ 20 mm }
\end{align*}
With same reasoning, we can simplify \textbackslash{}\(P(test_+|S_1, \text{untested})P(S_1|\text{untested}) = P(S_1, test_+|\text{untested})\), giving us
\begin{align*}
  &=  \dfrac{{P(test_+,S_0|\text{untested})}}{P(test_+,S_0|\text{untested}) +  P(S_1, test_+|\text{untested})} \hspace{45 mm }\\ 
   &=  \dfrac{{P(test_+,S_0|\text{untested})}}{P(test_+|\text{untested}) } \\
   &= \dfrac{\dfrac{P(S_0, test_+, \text{untested})}{P(\text{untested})}}{ \dfrac{P(test_+,\text{untested})}{P(\text{untested})}} \\ 
  &=\dfrac{P(S_0, test_+, \text{untested})}{P(test_+,\text{untested})} \\
  &= P(S_0 |test_+, \text{untested}).
\end{align*}
\noindent Hence, we have
\begin{align*}
P(S_0 |test_+, \text{untested}) = \dfrac{\beta (1- P(S_1|\text{untested}))}{\beta(1- P(S_1|\text{untested})) + \alpha(P(S_1|\text{untested})}
\end{align*}
\noindent as desired.
\qed

\newpage

\hypertarget{loess-smoothing}{%
\section{LOESS Smoothing}\label{loess-smoothing}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Locally estimated scatterplot smoothing (LOESS) fits a collection of local regression models to obtain a smooth curve through the observed data (Figure \ref{fig:loess}). It is highly flexible in the sense that we do not have to specify the functional relationship between the predictor and response variable for the entire range of the predictor, which may be impossible in various settings. It is particularly with time series data with substantial noise.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}\label{fig:loess}}
\end{figure}
To perform LOESS smoothing, we estimate a set of local regressions (\textbf{chambers1997?}). To do this, we must specify the span; this smoothing parameter is the fraction of the data that is used for the local polynomial fit. With a smaller span, the resulting curve will fit the trends more closely, while a larger span reflects broader trends (Figure \ref{fig:smooth-spans}).
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/unnamed-chunk-15-1.pdf}
\caption{\label{fig:unnamed-chunk-15}\label{fig:smooth-spans}}
\end{figure}
\hypertarget{fitting-the-loess-curve}{%
\subsection{Fitting the LOESS Curve}\label{fitting-the-loess-curve}}

To introduce some notation for the model at hand, we have a dependent variable \(\mathbf y\) and independent variable \(\mathbf x\), where \(\mathbf y\) and \(\mathbf x\) are related by some unknown function \(g\), that is, \(y = g(x) + \boldsymbol \epsilon\)\footnote{Recall we use bold type for vectors, e.g., \(\mathbf x \in \mathbb R^n\) is a vector with observations \(x_i \in \mathbb R\).}. When we want to use LOESS smoothing to estimate \(g\), often this function is complex, so we break up the problem into estimating a set of local regressions.

To obtain a predicted value \(\hat g(x^*)\) for a particular value of the independent variable \(x^*\), we fit a polynomial with greatest weight placed on points in the neighborhood of \(x^*\), where the width of this neighborhood is defined by the choice of smoothing span. Let \(\alpha \in (0,1]\) denote the chosen smoothing span.

For a particular value of \(x^*\), we estimate the predicted value \(\hat g(x^*)\) by fitting a local regression. We first compute the weights by computing the vector of distances from this point \(x^*\), that is,

\[\Delta (x^*) = |\mathbf x -x^* | \]

We define \(q = \text{floor}(\alpha n)\), and take \(\Delta_q(x^*) \in \mathbb R\) to be the \(q^th\) smallest distance of \(\Delta (x^*)\).

The vector of weights is then
\[T(\Delta(x^*), \Delta_q(x^*))\]

where \(T\) is the tricube weight function given by

\[
T(x) = \begin{cases} (1-(x)^3)^3 \hspace{9 mm}  \text{ for } |x| < 1\\
0  \hspace{25 mm} \text{ for |x| } \geq 1 \end{cases}.
\]
Essentially, this process gives weight to points in the neighborhood of \(x^*\). Consider \(x^* = 500\) and \(\text{smoothing span} = \alpha = .2\).

Then the weights we obtain are given in Figure \ref{fig:weights}.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/unnamed-chunk-16-1.pdf}
\caption{\label{fig:unnamed-chunk-16}\label{fig:weights} We see that the only values with nonzero weights are those within the interval \((500 - \alpha (n), 500 - \alpha (n))\), that is, the proportion \(\alpha\) of the data points closest to \(x^*\).}
\end{figure}
We fit a linear regression with polynomial terms, typically with degree up to 2, with these weights.

For example, fitting the model for this same \(x^*=500\), we obtain the polynomial in Figure \ref{fig:ex-poly}.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/unnamed-chunk-17-1.pdf}
\caption{\label{fig:unnamed-chunk-17}\label{fig:ex-poly}}
\end{figure}
By fitting the model for every point in \(\mathbf x\), we obtain the smoothed line shown in red in Figure \ref{fig:loess-all}.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/loess-all-1.pdf}
\caption{\label{fig:loess-all}\label{loess-all}}
\end{figure}
Smoothing methods are sensitive to the choice of smoothing parameter \(h\), which represents the fraction of the data that is used for the local polynomial fit.

Methods exist for picking the smoothing parameter \(h\) that minimizes the mean squared error between the predicted values from the estimated line and observed values of the dependent variable, for example, leave-one-out cross-validation or generalized cross-validation.

However, for this work, we used LOESS smoothing to smooth survey data from the COVID-19 Trends and Impact Survey (\textbf{reinhart2021?}).
We choose the smoothing parameter for each variable based on domain knowledge regarding the level of noise present for each variable of interest. For example, there is substantial noise in the screening test positivity data that reflect trends that do not represent meaningful differences in the screening test positivity. Some trends in the screening sensitivity may be due to scheduled workplace screenings happening at regular time intervals, and some of the variation may be due to the frequency of screening testing due to other variables, such as the access and cost of testing.

Since the ratio \(\frac{\text{screening test positivity}}{\text{overall test positivity}}\) is used to estimate \(\beta = \frac{P(test + | S_0, untested)}{P(test + |tested)}\), the variability in the screening positivity creates substantial variability in our estimates of \(\beta\).

In light of this variability and the presence of other trends regarding the screening test positivity, we set the span to \(\frac{4}{12} = 0.33\) to fit the local regressions for 4-month intervals with the aim to capture the broader trends over time.

INCLUDE FIGURE OF SMOOTHED ESTIMATES HERE

There was less variabiity in the smoothing span for the weighted percentage of COVID-like Illness, the estimate of \(P(S_1|untested)\). Hence, we set the smoothing parameter to \(0.2\) detect trends at a finer time scale.

Sensitivity analyses with modified versions of the smoothing span of \(\beta\) are included in the appendix in the section INCLUDE SECTION.

\hypertarget{kernel-density-estimation}{%
\section{Kernel Density Estimation}\label{kernel-density-estimation}}

\hypertarget{sampling-importance-resampling}{%
\section{Sampling Importance Resampling}\label{sampling-importance-resampling}}

\hypertarget{definition-of-prior-distributions-for-bias-parameters}{%
\chapter{Definition of Prior Distributions for Bias Parameters}\label{definition-of-prior-distributions-for-bias-parameters}}

Placeholder

\hypertarget{background-on-the-beta-distribution}{%
\section{Background on the Beta Distribution}\label{background-on-the-beta-distribution}}

\hypertarget{background-on-the-gamma-distribution}{%
\section{Background on the Gamma Distribution}\label{background-on-the-gamma-distribution}}

\hypertarget{definition-of-prior-distributions-for-incomplete-testing-correction}{%
\section{Definition of Prior Distributions for Incomplete Testing Correction}\label{definition-of-prior-distributions-for-incomplete-testing-correction}}

\hypertarget{defining-ps_1untested}{%
\subsection{\texorpdfstring{Defining \(P(S_1|Untested)\)}{Defining P(S\_1\textbar Untested)}}\label{defining-ps_1untested}}

\hypertarget{defining-alpha}{%
\subsection{\texorpdfstring{Defining \(\alpha\)}{Defining \textbackslash alpha}}\label{defining-alpha}}

\hypertarget{defining-beta}{%
\subsection{\texorpdfstring{Defining \(\beta\)}{Defining \textbackslash beta}}\label{defining-beta}}

\hypertarget{defining-ps_0testuntested}{%
\subsection{\texorpdfstring{Defining \(P(S_0|test+,untested)\)}{Defining P(S\_0\textbar test+,untested)}}\label{defining-ps_0testuntested}}

\hypertarget{definition-of-priors-for-test-inaccuracy-correction}{%
\section{Definition of Priors for Test Inaccuracy Correction}\label{definition-of-priors-for-test-inaccuracy-correction}}

\hypertarget{defining-test-sensitivity-s_e}{%
\subsection{\texorpdfstring{Defining Test Sensitivity (\(S_e\))}{Defining Test Sensitivity (S\_e)}}\label{defining-test-sensitivity-s_e}}

\hypertarget{defining-test-specificity-s_p}{%
\section{\texorpdfstring{Defining Test Specificity (\(S_p\))}{Defining Test Specificity (S\_p)}}\label{defining-test-specificity-s_p}}

\hypertarget{summary-table-of-bias-parameter-distributions}{%
\section{Summary Table of Bias Parameter Distributions}\label{summary-table-of-bias-parameter-distributions}}

\hypertarget{correction-for-incomplete-testing}{%
\section{Correction for Incomplete Testing}\label{correction-for-incomplete-testing}}

\hypertarget{correction-for-diagnostic-test-inaccuracy}{%
\section{Correction for Diagnostic Test Inaccuracy}\label{correction-for-diagnostic-test-inaccuracy}}

\hypertarget{derivation-of-formula-for-correction-for-diagnostic-test-inaccuracy}{%
\subsection{Derivation of Formula for Correction for Diagnostic Test Inaccuracy}\label{derivation-of-formula-for-correction-for-diagnostic-test-inaccuracy}}

\hypertarget{details-of-implementation}{%
\chapter{Details of Implementation}\label{details-of-implementation}}
\begin{itemize}
\tightlist
\item
  Describe each step here
\item
  Mention reproducible workflow with make
\end{itemize}
\hypertarget{comparison-to-the-covidestim-model}{%
\chapter{Comparison to the Covidestim Model}\label{comparison-to-the-covidestim-model}}

Placeholder

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

\hypertarget{the-covidestim-model}{%
\subsection{The Covidestim Model}\label{the-covidestim-model}}

\hypertarget{assumptions}{%
\subsection{Assumptions}\label{assumptions}}

\hypertarget{comparison-to-other-indicators}{%
\section{Comparison to Other Indicators}\label{comparison-to-other-indicators}}

\hypertarget{seropositivity-data}{%
\section{Seropositivity Data}\label{seropositivity-data}}

\hypertarget{county-level}{%
\section{County-level}\label{county-level}}

\hypertarget{state-level}{%
\section{State-level}\label{state-level}}

\appendix

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{derivation-of-the-mean-and-variance-of-the-beta-distribution}{%
\section{Derivation of the Mean and Variance of the Beta Distribution}\label{derivation-of-the-mean-and-variance-of-the-beta-distribution}}

To add

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

Placeholder

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-blitzsteinIntroductionProbability2019}{}}%
Blitzstein, J. K., \& Hwang, J. (2019). \emph{Introduction to probability} (Second edition). Boca Raton: CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-genest1986}{}}%
Genest, C., McConway, K. J., \& Schervish, M. J. (1986). Characterization of Externally Bayesian Pooling Operators. \emph{The Annals of Statistics}, \emph{14}(2), 487--501. Retrieved from \url{https://www.jstor.org/stable/2241231}

\leavevmode\vadjust pre{\hypertarget{ref-greenland2016}{}}%
Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., \& Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. \emph{European Journal of Epidemiology}, \emph{31}(4), 337--350. http://doi.org/\href{https://doi.org/10.1007/s10654-016-0149-3}{10.1007/s10654-016-0149-3}

\leavevmode\vadjust pre{\hypertarget{ref-lash2009}{}}%
Lash, T. L., Fox, M. P., \& Fink, A. K. (2009). \emph{Applying Quantitative Bias Analysis to Epidemiologic Data}. New York, NY: Springer New York. http://doi.org/\href{https://doi.org/10.1007/978-0-387-87959-8}{10.1007/978-0-387-87959-8}

\leavevmode\vadjust pre{\hypertarget{ref-ma2021}{}}%
Ma, Q., Liu, J., Liu, Q., Kang, L., Liu, R., Jing, W., \ldots{} Liu, M. (2021). Global Percentage of Asymptomatic SARS-CoV-2 Infections Among the Tested Population and Individuals With Confirmed COVID-19 Diagnosis: A Systematic Review and Meta-analysis. \emph{JAMA Network Open}, \emph{4}(12), e2137257. http://doi.org/\href{https://doi.org/10.1001/jamanetworkopen.2021.37257}{10.1001/jamanetworkopen.2021.37257}

\leavevmode\vadjust pre{\hypertarget{ref-neyman1937}{}}%
Neyman, J. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. \emph{Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences}, \emph{236}(767), 333--380. http://doi.org/\href{https://doi.org/10.1098/rsta.1937.0005}{10.1098/rsta.1937.0005}

\leavevmode\vadjust pre{\hypertarget{ref-petersen2021}{}}%
Petersen, J. M., Ranker, L. R., Barnard-Mayers, R., MacLehose, R. F., \& Fox, M. P. (2021). A systematic review of quantitative bias analysis applied to epidemiological research. \emph{International Journal of Epidemiology}, \emph{50}(5), 1708--1730. http://doi.org/\href{https://doi.org/10.1093/ije/dyab061}{10.1093/ije/dyab061}

\leavevmode\vadjust pre{\hypertarget{ref-poole2000}{}}%
Poole, D., \& Raftery, A. E. (2000). Inference for Deterministic Simulation Models: The Bayesian Melding Approach. \emph{Journal of the American Statistical Association}, \emph{95}(452), 1244--1255. http://doi.org/\href{https://doi.org/10.1080/01621459.2000.10474324}{10.1080/01621459.2000.10474324}

\leavevmode\vadjust pre{\hypertarget{ref-powers2011}{}}%
Powers, K. A., Ghani, A. C., Miller, W. C., Hoffman, I. F., Pettifor, A. E., Kamanga, G., \ldots{} Cohen, M. S. (2011). The role of acute and early HIV infection in the spread of HIV and implications for transmission prevention strategies in Lilongwe, Malawi: a modelling study. \emph{The Lancet}, \emph{378}(9787), 256--268. http://doi.org/\href{https://doi.org/10.1016/S0140-6736(11)60842-8}{10.1016/S0140-6736(11)60842-8}

\leavevmode\vadjust pre{\hypertarget{ref-robson2014}{}}%
Robson, B. J. (2014). When do aquatic systems models provide useful predictions, what is changing, and what is next? \emph{Environmental Modelling \& Software}, \emph{61}, 287--296. http://doi.org/\href{https://doi.org/10.1016/j.envsoft.2014.01.009}{10.1016/j.envsoft.2014.01.009}

\leavevmode\vadjust pre{\hypertarget{ref-sah2021}{}}%
Sah, P., Fitzpatrick, M. C., Zimmer, C. F., Abdollahi, E., Juden-Kelly, L., Moghadas, S. M., \ldots{} Galvani, A. P. (2021). Asymptomatic SARS-CoV-2 infection: A systematic review and meta-analysis. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(34), e2109229118. http://doi.org/\href{https://doi.org/10.1073/pnas.2109229118}{10.1073/pnas.2109229118}

\leavevmode\vadjust pre{\hypertarget{ref-sevcikova2007}{}}%
Ševčíková, H., Raftery, A. E., \& Waddell, P. A. (2007). Assessing uncertainty in urban simulations using Bayesian melding. \emph{Transportation Research Part B: Methodological}, \emph{41}(6), 652--669. http://doi.org/\href{https://doi.org/10.1016/j.trb.2006.11.001}{10.1016/j.trb.2006.11.001}

\end{CSLReferences}

% Index?

\end{document}
