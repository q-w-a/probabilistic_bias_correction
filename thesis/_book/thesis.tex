% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
  %% Preamble
%%
  % \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{smiththesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: https://www.ctan.org/
  %%
  \usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{multicol}
\usepackage{float}
\floatplacement{figure}{H}
% End of CII addition
\usepackage{rotating}
%\hypersetup{
%    colorlinks=true,
%    linkcolor=blue,
%    filecolor=magenta,      
 %   urlcolor=blue,
 %   pdftitle={Overleaf Example},
 %   pdfpagemode=FullScreen,
 %   }

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%    filecolor=blue,
%     citecolor = blue,      
%     urlcolor=cyan,
 %    }
\hypersetup{colorlinks=true,
            linkcolor=blue,
            citecolor=black,
            urlcolor=blue}
% End of CII addition

% Added by CII
\usepackage[font=scriptsize]{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino
\usepackage{palatino}
\usepackage{tcolorbox}

% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Using Sensitivity Analyses to Approximate Total COVID-19 Infections: State and County level in the United States, March 2021 - March 2022}
\author{Quinn White}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2023}
\division{Mathematics and Natural Sciences}
\advisor{Ben Baumer}
\institution{Smith College}
\degree{Bachelor of Arts}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
\altadvisor{Nicholas Reich}
% End of CII addition

%%% Remember to use the correct department!
\department{Statistical and Data Sciences}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
% As noted by @mirh [2] is needed instead of [3] for 2.12
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
\setlength{\parindent}{0pt}
% turn on hanging indent if param 1 is 1
\ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
% set entry spacing
\ifnum #2 > 0
\setlength{\parskip}{#2\baselineskip}
  \fi
}%
{}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
  \newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
    \newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
      \newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
        \renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
This project would not have been possible without the the incredible mentorship I received from Professor Ben Baumer at Smith College and Professor Nick Reich at University of Massachusetts Amherst. To begin, Ben, you were essential for getting this project off the ground. In fact, your involvement begins well before that. Sitting in your class my first year of college was the first time I became really excited about coding and data science, which has certainly shaped the path I am on today. The skills I learned in your classes translated directly into research contexts: in particular, the organization to structure a long-term coding project like this, the ability to address questions via coding, and the importance of thinking through how to put together effective data visualizations. This background enabled me to have research experiences as an undergraduate that have motivated me to pursue graduate school.

Nick, I would like to thank you for making this thesis experience better than I ever could have imagined. From the first meeting we had, it was clear to me that I was incredibly lucky to get the chance to work with you this year, and the experience has meant more to me than I can fully express here. I have learned so much from our conversations along the way on how to think deeply about the questions that arose, and I would always leave our meetings excited to look further into the ideas we discussed. This experience has made me truly appreciate how much mentorship shapes a research experience, and has made me excited to engage in further research in graduate school.

Beyond advising me on this project, I also really appreciate that you welcomed me to join lab meetings for your research group. On that note, I would like to thank all members of the Reich lab. Everyone was so welcoming and kind from the very start -- I was taken aback when one member, Martha, offered to give me rides over to University of Massachusetts to join the lab meetings, which was a wonderful way to start the day. I always enjoyed the presentations in these meetings; even if a topic was entirely new to me, I could learn so much by hearing the kinds of questions people asked and concerns they raised. You all helped me grow as a scientist, and I am so grateful for your support.

Lastly, I would like to thank my friends and family, particularly my sister. Sienna, you have been involved from the very first phone call I when shared my idea for this project, and even joined my defense on Zoom. You been so encouraging and have made me laugh throughout the entire process.
}

\Dedication{

}


\Abstract{
As we have navigated the COVID-19 pandemic, case counts have been a central source of information for understanding transmission dynamics and the effect of public health interventions. However, because the number of cases we observe is limited by the testing effort in a given location, the case counts presented on local or national dashboards are only a fraction of the true infections. Variation in testing rate by time and location impacts the number of cases that go unobserved, which can cloud our understanding of the true COVID-19 incidence at a given time point and can create biases in downstream analyses. Additionally, the number of cases we observe is impacted by the sensitivity and specificity of the diagnostic test. To quantify the number of true infections given incomplete testing and diagnostic test inaccuracy, we implement probabilistic bias analysis at a biweekly time scale from January 1, 2021 through February 2022. In doing so, we estimate a range of possible true infections for every given time interval and location considered. This approach can be applied at the state level across the United States, as well as in some counties where the needed data are available.
}

	\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End of CII addition
%%
%% End Preamble
%%
%
\begin{document}

% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
  \begin{acknowledgements}
    This project would not have been possible without the the incredible mentorship I received from Professor Ben Baumer at Smith College and Professor Nick Reich at University of Massachusetts Amherst. To begin, Ben, you were essential for getting this project off the ground. In fact, your involvement begins well before that. Sitting in your class my first year of college was the first time I became really excited about coding and data science, which has certainly shaped the path I am on today. The skills I learned in your classes translated directly into research contexts: in particular, the organization to structure a long-term coding project like this, the ability to address questions via coding, and the importance of thinking through how to put together effective data visualizations. This background enabled me to have research experiences as an undergraduate that have motivated me to pursue graduate school.

    Nick, I would like to thank you for making this thesis experience better than I ever could have imagined. From the first meeting we had, it was clear to me that I was incredibly lucky to get the chance to work with you this year, and the experience has meant more to me than I can fully express here. I have learned so much from our conversations along the way on how to think deeply about the questions that arose, and I would always leave our meetings excited to look further into the ideas we discussed. This experience has made me truly appreciate how much mentorship shapes a research experience, and has made me excited to engage in further research in graduate school.

    Beyond advising me on this project, I also really appreciate that you welcomed me to join lab meetings for your research group. On that note, I would like to thank all members of the Reich lab. Everyone was so welcoming and kind from the very start -- I was taken aback when one member, Martha, offered to give me rides over to University of Massachusetts to join the lab meetings, which was a wonderful way to start the day. I always enjoyed the presentations in these meetings; even if a topic was entirely new to me, I could learn so much by hearing the kinds of questions people asked and concerns they raised. You all helped me grow as a scientist, and I am so grateful for your support.

    Lastly, I would like to thank my friends and family, particularly my sister. Sienna, you have been involved from the very first phone call I when shared my idea for this project, and even joined my defense on Zoom. You been so encouraging and have made me laugh throughout the entire process.
  \end{acknowledgements}

% https://github.com/ismayc/thesisdown/issues/32 
  {
    \hypersetup{linkcolor=black}
    \setcounter{tocdepth}{2}
    \tableofcontents
  }


  \begin{abstract}
    As we have navigated the COVID-19 pandemic, case counts have been a central source of information for understanding transmission dynamics and the effect of public health interventions. However, because the number of cases we observe is limited by the testing effort in a given location, the case counts presented on local or national dashboards are only a fraction of the true infections. Variation in testing rate by time and location impacts the number of cases that go unobserved, which can cloud our understanding of the true COVID-19 incidence at a given time point and can create biases in downstream analyses. Additionally, the number of cases we observe is impacted by the sensitivity and specificity of the diagnostic test. To quantify the number of true infections given incomplete testing and diagnostic test inaccuracy, we implement probabilistic bias analysis at a biweekly time scale from January 1, 2021 through February 2022. In doing so, we estimate a range of possible true infections for every given time interval and location considered. This approach can be applied at the state level across the United States, as well as in some counties where the needed data are available.
  \end{abstract}

\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

~~Throughout the COVID-19 pandemic, observed infections have guided decisions at both the individual and government levels. At the state-level, policies on phased reopening, for example, often include criteria on COVID-19 cases (California Department of Public Health, 2021; Charles D. Baker, 2021; Tom Wolf, 2020).

To make this data accessible to the public, several organizations, including the CDC (Disease Control and Prevention, 2020), Johns Hopkins University (Dong, Du, \& Gardner, 2020), and the New York Times (The New York Times, 2022), compiled comprehensive dashboards presenting key metrics such as positive tests and test positivity rates across states.

However, our interpretation of the number of positive tests as a measure of transmission is limited by the fact that testing rates impact these trends. The number of positive tests we observe in a state, for instance, will be a result of that state's testing capacity and testing behavior of its population. This means that trends in the detected infections may not always reflect trends in the true number of infections. As we see in Figure \ref{fig:test-rate-all-states}, testing rates vary considerably by state and over time. Comparing infection burden in a state like Rhode Island (highlighted in red) to Mississippi by is complicated by the differences in testing rates in these states, which underscores the relevance of considering incomplete testing when estimating true infections.

The importance of considering testing rate led John Hopkins University to organize the most comprehensive testing database available in the United States (Dong et al., 2020), which enabled us to see that testing rate varied substantially by state and time throughout 2021 and 2022. After three years of collecting broadly used COVID-19 data, Johns Hopkins ceased its collection efforts on March 10, 2023.
\begin{figure}
\includegraphics[width=1\linewidth]{./figure/testing_all} \caption{\label{fig:test-rate-all-states}Weekly testing rate for each state across the United States, where we define testing rate as the total number of tests in that week for a given state over the state's population size. Each line corresponds to a single state. Highlighted in color are the states at the extremes, where Rhode Island has the highest median testing rate and Mississippi has the lowest. Seeing the extent to which testing rate varies over time and by state underscores the importance of thinking about incomplete testing when estimating the true number of infections.}\label{fig:unnamed-chunk-6}
\end{figure}
As we study the impact and transmission of SARS-CoV-2 as well as the efficacy of different interventions, we often turn to case counts for information. In this way, case counts form the basis for numerous types of analyses that inform our understanding of the pandemic. This means that bias in case counts due to unobserved infections can greatly impact our understanding of the pandemic.

One way testing rates can influence our understanding of COVID-19 is when we are seeking to make comparisons across different locations.

The government response to the pandemic has differed greatly by state, with a range of different policies and timelines as local governments weighted complex tradeoffs. The variability in state-level policies sparked several questions related to the consequences of these policies. Comparing case counts enables us to compare the impact of state-level management of the pandemic. For example, Kaufman et al. (2021) use cumulative case counts to study the effect of state-level social distancing policies. At the county scale, Jiang, Roy, Pollock, Shah, \& McCoy (2022) evaluate the association between stay-at-home orders and daily incident cases, and Kao et al. (2023) look at how the duration of multiple policy interventions -- face mask mandates, stay-at-home orders, and gathering bans -- affected monthly incidence.

The bias in case counts is particularly important for inference related to government interventions. With regard to government interventions, it is highly likely that lower testing resources may be related to less stringent policies in other respects. If this is the case, then lower cases may be observed in locations with less stringent policies as an artifact of inadequate testing rather than lower transmission. As a result, when we estimate the effect of a policy intervention based on observed cases, we may be underestimating the true impact.

Besides interventions, there has been substantial concern over the disparities in the impact of COVID-19. As a result, it is important to understand the relationship between various socioeconomic variables and case burden. J. T. Chen \& Krieger (2021a) show a consistent monotonic relationship between the percent poverty and cumulative case burden at the zip-code tabulation area level in Illinois, with higher percent poverty associated with a higher case burden. Similarly, Karmakar, Lantz, \& Tipirneni (2021a) show in a cross-sectional analysis that for counties in the U.S., incident cases were associated with higher social vulnerability index. This social vulnerability index is defined by the Centers for Disease Control and Prevention (CDC), and includes information from a collection of census variables related to poverty, unemployment, and racial and ethnic minority status.

However, relying on observed cases as we study disparities in the impact of COVID-19 also may be problematic, since counties with higher social vulnerability (due to, for example, low economic resources) or inadequate access to health care may also have lower testing resources, which may bias our comparisons to counties where testing is more adequate. In this sense, relying on observed infections may result in an underestimation of the impact of disparities, since the observed counts in counties with high social vulnerability may be systematically lower than in counties with lower social vulnerability.

We also use cases to study the effect of vaccination at the population scale. Work in this area has been expansive. Harris show an inverse relationship between cross-sectional COVID-19 incidence and county-level vaccination coverage during the Delta surge considering a sample of the counties with the largest population size (Harris, 2022), and Cuadros et al. (2022) find a similar trend in counties across the United States. Nevertheless, as the virus has evolved, the relationship between transmission and case counts has shifted, particularly with the evolution of the highly transmissible Omicron variant. McLaughlin, Wiemken, Khan, \& Jodar (2022) find that there wasn't a relationship between the percentage of the population fully vaccinated and case counts, contrasting findings from other waves. However, they did also find that higher booster uptake rates are associated with meaningful decreases in case counts, and higher vaccination rates and booster rates are both associated with decreases in COVID-19 mortality.

Beyond the efficacy of vaccines at the individual level, these studies also demonstrate that we can use case data to quantify the impact of vaccination efforts as a public health intervention. Coupled with information about genetic variants that are circulating, they also can extend our knowledge about the effect of this intervention across different phases of the pandemic. That said, studying Omicron and subsequent variants is particularly challenging due to the rise in use of at-home rapid antigen COVID-19 tests, which increased dramatically during the Omicron wave (Rader et al., 2022). The availability a cheaper and more convenient alternative to PCR testing likely changes the population choosing to get a PCR test, since if someone tests positive on an antigen test, it would not be recommended to get a PCR test. Furthermore, in contrast to PCR test results, the results of antigen tests are not reported. This shift in testing behavior makes Omicron and later variants more difficult to study.

Looking to the future, infection counts also may be informative as we better understand the impacts of long COVID-19\footnote{The syndrome goes by a number of names, including long-haul COVID-19, post-acute post-acute sequelae SARS-CoV-2 infection (PASC), among others.} on a population scale. There is increased concern over the poorly characterized but widespread phenomenon of lingering COVID-19 symptoms, which includes but is not limited to symptoms of fatigue, dyspnea, chest pain, and palpitation. The heterogeneity of presentations and definitions has complicated research on the syndrome, yet its impact has been pervasive. In light of this, the NIH has made the Researching COVID to Enhance Recovery (RECOVER) Initiative to better understand and treat long COVID-19.

Infection counts are particularly relevant for the study of long COVID-19 at the population scale because, contrary to what we might expect, the severity of the initial COVID-19 infection is not associated with the persistence of several symptoms, including anosmia, chest pain, cough, and palpitation (Dirican \& Bal, 2022). Since lingering symptoms can be problematic even with mild cases, trying to characterize the cumulative burden of COVID-19 through a proxy such as hospitalization counts would not capture the full impact.

Ultimately, COVID-19 infections are a key metric that informs our understanding of the pandemic. Infection counts are interesting in itself to quantify the reach of the pandemic across different time periods, and they are also the inputs to an extensive array of analyses that aid our understanding of public health interventions, disparities in the impact of the virus, and differences in the dynamics among circulating genetic variants. This underlies the importance of quantifying the underestimation of COVID-19 infections and how the extent of underestimation differs across time and space.

\hypertarget{overview}{%
\chapter{Overview of Approach}\label{overview}}

Our work extends the analyses presented in the publication \emph{Substantial underestimation of SARS-CoV-2 infection in the United States} (Wu et al., 2020). Wu \emph{et al.} considered a single time interval early in the pandemic, estimating the true number of infections as of April 18, 2020 at the state level. When we consider the estimates, we can look at both the estimates for total infections by state, but also the ratio of the estimated total cases to the observed cases. This enables us to think about the way case ascertainment varies by state, as we see in Figure
\ref{fig:originalfigwu}.
\begin{figure}
\includegraphics[width=0.5\linewidth]{./figure/figure_original_case_ratio} \caption{\label{fig:originalfigwu}Figure from Wu et al. (2020) showing the ratio of total estimated infections as of April 18, 2020 to the number of cases confirmed by a positive PCR test. Total estimated infections were obtained by a correction for incomplete testing and imperfect diagnostic test accuracy.}\label{fig:unnamed-chunk-7}
\end{figure}
The core idea of the approach is to break up the unobserved infections into unobserved infections among those with no or mild symptoms or those with moderate to severe symptoms. We denote this symptom status by an indicator variable, where \(S_1\) represents having moderate to severe symptoms and \(S_0\) represents having no or mild symptoms. In what follows, \(\text{test}_+\) denotes the event that an individual \emph{would} test positive if they were tested, not that they actually did. For example, \(\Pr(\text{test}_+|S_1,\text{untested})\) represents the probability a symptomatic untested individual would test positive if they were tested.

We let \(N^*\) denote the number of positive tests we would obtain if we tested the entire population. That is, \(N^*\) is the sum of the observed positive tests and the number of positive tests there would be among the untested population if they were tested. We use this notation to distinguish the number of positive tests, which is affected by testing inaccuracy, from the number of active SARS-CoV-2 infections, \(N^+\).

Our first goal is to estimate the number of untested individuals who would test positive if they were tested, \(N^*_{\text{untested}}\). Let \(N_{\text{untested}}\) denote the number of

To estimate the number of untested individuals with moderate to severe COVID-19-like symptoms who would test positive untested population, we can take
\begin{align*}
N^*_{\text{untested},S_1} &=N_{\text{untested},S_1} \; \Pr(\text{test}_+ | S_1,\text{untested})\\
&=N_{\text{untested}} \; \Pr(S_1|\text{untested}) \; \Pr(\text{test}_+ | S_1,\text{untested}).
\end{align*}
Similarly, we can estimate the asymptomatic (or mild) infections among the untested population as
\begin{align*}
N^*_{\text{untested},S_0} &=N_{\text{untested},S_0} \; \Pr(\text{test}_+ | S_0,\text{untested})\\
&=N_{\text{untested}} \Pr(S_0|\text{untested}) \; \Pr(\text{test}_+ | S_0,\text{untested})\\
&=N_{\text{untested}}(1-\Pr(S_1|\text{untested})) \; \Pr(\text{test}_+ | S_0,\text{untested}).
\end{align*}
Taking the sum of the positive tests gives us the total:

\[N^*_{\text{untested}} = N^*_{\text{untested},S_1} + N^*_{\text{untested},S_0}.\]
This allows us to obtain the estimated number of positive tests as

\[N^* = N^*_{\text{untested}} +N^*_{\text{tested}}\]
where \(N^*_{\text{tested}}\) is the number of positive tests in a given location.

At this point, we can apply a simple epidemiology formula to that corrects for the test specificity and sensitivity of the diagnostic test (Rothman, Greenland, \& Lash, 2008). Denoting the sensitivity \(S_e\) and specificity \(S_p\), this formula is given by

\[\text{Number Truly Positive} = \dfrac{N^*+* - (1-S_p) \times N}{S_e+S_p-1}.\]

The uncertainty inherent in this estimation process is in the quantities \(\Pr(S_1|\text{untested})\), \(\Pr(\text{test}_+| S_1,\text{untested})\), and \(\Pr(\text{test}_+ | S_0,\text{untested})\).

It is particularly difficult to think about how we would estimate \(\Pr(\text{test}_+ | S_0,\text{untested})\) or \(\Pr(\text{test}_+ | S_1,\text{untested})\) directly because there is a lack of data on these quantities.

Instead, we define random variables to relate the (hypothetical) test positivities among the untested symptomatic and asymptomatic populations to the observed positivity rate \(\Pr(\text{test}_+|\text{tested})\).

In particular, we define
\begin{align*}
\alpha &= \dfrac{\Pr(\text{test}_+  |S_1, \text{untested})}{\Pr(\text{test}_+ | \text{tested})}\\
\beta  &= \dfrac{\Pr(\text{test}_+ |S_0, \text{untested})}{\Pr(\text{test}_+ | \text{tested})}.
\end{align*}
We can think of \(\alpha\) and \(\beta\) as variables that correct the observed test positivity rate to estimate the test positivity rate among the symptomatic and asymptomatic partitions of the population respectively.

We can think of \(\alpha\) as the correction factor for estimating \(\Pr(\text{test}_+|S_1,\text{untested})\) from the test positivity \(\Pr(\text{test}_+ |\text{tested})\).

We can define \(\beta\) analogously for the asymptomatic case, where
\[\beta =  \dfrac{\Pr(\text{test}_+ |S_0, \text{untested})}{\Pr(\text{test}_+ | \text{tested})},\]
so we have \(\Pr(\text{test}_+ |S_0, \text{untested}) = \beta \; \Pr(\text{test}_+ | \text{tested})\).

This formulation enables us to estimate \(\Pr(\text{test}_+ |S_0, \text{untested})\) and \(\Pr(\text{test}_+ |S_1, \text{untested})\) with information from the observed test positivity rate among the tested population, which means it can reflect differences in transmission dynamics by the location and time interval considered.

We expect \(\alpha\) to be higher than \(\beta\) to reflect that the test positivity rate among the symptomatic untested population is higher than that of the asymptomatic untested population. The specification of these distributions is discussed in greater detail in the \protect\hyperlink{defpriors}{Definition of Prior Distributions for the Bias Parameters} section.

Because of the uncertainty around \(\alpha\) and \(\beta\), it is useful to relate these parameters to the asymptomatic rate of the virus, \(\Pr(S_0|\text{test}_+, \text{untested})\). This is possible because once we specify \(\alpha\), \(\beta\), and \(P(S_1|\text{untested})\), we can use the laws of probability to obtain \(P(S_0|\text{untested},\text{test}_+)\). Due to the importance of asymptomatic transmission to controlling the pandemic, the asymptomatic rate has been an area of substantial interest. This has led to extensive studies on the topic, including multiple meta-analyses summarizing these results (Ma et al., 2021a; Sah et al., 2021a).

We can represent the relationship between \(\theta = \{ \alpha, \beta, \Pr(S_1|\text{untested})\}\) and \(\phi = \{\; \Pr(S_0|\text{test}_+, \text{untested})\;\}\) by the deterministic function
\(M: \theta \to \phi\) defined as:
\[\Pr(S_0|\text{test}_+, \text{untested}) = \dfrac{\beta(1 - \Pr(S_1|\text{untested}))}{\beta(1-\Pr(S_1|\text{untested})) + \alpha \Pr(S_1|\text{untested})}.\]

When we have prior knowledge about the distributions of the inputs and output of a deterministic function, we can use \protect\hyperlink{meld}{Bayesian melding} to generate constrained distributions for the inputs and outputs that are in concordance with one another. In essence, this approach considers the distinct distributions we have for \(\phi\), the distribution informed by previous literature on the asymptomatic rate, and the distribution formed by evaluating \(M\) at values of \(\theta\). We can combine these distributions with logarithmic pooling to yield a constrained distribution for \(\phi=\Pr(S_0|\text{test}_+, \text{untested})\), and then can approximate the inverted distribution to obtain constrained distributions for the inputs \(\theta = \{\Pr(S_1|\text{untested}), \alpha, \beta \}\).

To make this more concrete, if we do not apply melding, when we sample from \(\alpha, \beta,\) and \(\Pr(S_1|\text{untested})\), we might sample values at or near \(\alpha = 0.935\), \(\beta = 0.167\), and \(\Pr(S_1|\text{untested} ) = 0.0123\) very often. However, when we calculate \(M(\theta)\), this would indicate that the asymptomatic rate is 94\%, a value that is not supported by studies on the asymptomatic rate of COVID-19, which typically produce estimates between 30\% and 60\%. Melding allows us to place greater density on the values of \(\alpha, \beta,\) and \(\Pr(S_1|\text{untested})\) that are in accordance with available data on the asymptomatic rate.

We can summarize the full implementation process in Figure \ref{fig:diagram}. We repeat this process for every geographic unit (a state or county) and time interval (a 2-week interval).
\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{./figure/analysis_diagram} 

}

\caption{\label{fig:diagram}Full implementation of probabilistic bias analysis, first correcting for incomplete testing, and then correcting for imperfect test accuracy.}\label{fig:unnamed-chunk-8}
\end{figure}
We divide the time period into 2-week intervals specifically due to the duration of test positivity, which is about two weeks on average (Kojima, Roshani, \& Klausner, 2022; Mallett et al., 2020). Because
\[ \text{Prevalence} = (\text{Incidence Rate}) \times (\text{Average Duration of Disease}), \]

\noindent prevalent infections and incident infections are approximately equivalent on the two-week time scale, which enables us to think of our estimates for each two-week period as incident infections.

With the implementation of Wu et al. (2020), \(\alpha, \beta, \text{and} \Pr(S_1|\text{untested})\) are assumed to be independent and identically distributed across states. However, because we are considering a wider time interval over all of 2021 and into early 2022, it makes sense to vary these parameters by time and location. Due to the availability of data to inform \(\beta\) and \(\Pr(S_1|\text{untested})\), we allow these parameters to vary by time and location, as discussed further in \protect\hyperlink{defpriors}{Chapter 4: Definition of Prior Distributions}.

When we allow \(\beta\) and \(\Pr(S_1|\text{untested})\) to vary over time and location, rather than implementing Bayesian melding once and using the same melded distribution for each time interval, we must implement melding for each time interval separately.

\hypertarget{outline-of-following-sections}{%
\section{Outline of Following Sections}\label{outline-of-following-sections}}

In what follows, we first give the necessary background to understand each component of our approach (Chapter 3). Then, we describe the specification of the prior for each bias parameter in the probabilistic bias analysis, summarizing available data to inform each prior distribution (Chapter 4). Next, we describe each step of the bias analysis to make the process more concrete (Chapter 5). We go on to discuss the choice to compare the probabilistic bias analysis estimates with an existing model, Covidestim (Chitwood et al., 2022) in Chapter 6, and then present results in Chapter 7. Lastly, we summarize the main conclusions and discuss limitations as well as future directions for our approach (Chapter 8).

\hypertarget{background}{%
\chapter{Background}\label{background}}

\hypertarget{probabalistic-bias-analysis}{%
\section{Probabalistic Bias Analysis}\label{probabalistic-bias-analysis}}

Often the focus of quantifying error about an effect estimate focuses on random error rather than the systematic error. For example, typical frequentist confidence intervals are common in medical and epidemiological literature, although they have faced rising criticism (Greenland et al., 2016). These confidence intervals quantify the fraction of the times we expect the true value to fall in this interval under the assumption that our model is correct. That is, if we ran an experiment 100 times and computed the effect size each time, we would expect the 95\% confidence interval to contain the true value to 95 of those times, on average.\footnote{Neyman stressed this in his original publication formalizing the concept of a confidence interval in 1937 (Neyman, 1937). The nuance that we cannot interpret a 95\% confidence interval as indicating that the probability the true value falls within this interval is 95\%, however, is often lost in the discussion of results, in part because the true meaning of a confidence interval is less intuitive.}

In contrast to focusing on random error, the aim of quantitative bias analysis is to consider systematic error to give a range of possible values for the true quantity of interest. In this sense, it is a type of sensitivity analysis. It can be used to estimate various kinds of biases, from misclassification, as is implemented in this work, as well as selection bias and unmeasured confounding (Petersen, Ranker, Barnard-Mayers, MacLehose, \& Fox, 2021). Often, the goal of performing such an analysis is to see how these sources of bias affect our estimates; in particular, under what situations of bias the observed effect would be null.

There are multiple different forms of bias analysis (Lash, Fox, \& Fink, 2009a). The most simple case, simple bias analysis, is correcting a point estimate for a single source of error. Multidimensional bias analysis extends this to consider sets of bias parameters, but still provides a corrected point estimate rather than a range of plausible estimates. Probabilistic bias analysis, meanwhile, defines probability distributions for bias parameters to generate a distribution of corrected estimates by repeatedly correcting estimates for bias under different combinations of the parameter values. Then, via Monte Carlo we obtain a distribution of corrected estimates that reflect the corrected values under different scenarios of bias, that is, under different combinations of the bias parameters. This can give us a better idea for the extent of uncertainty about the corrected estimates, although this uncertainty does depend on the specification of the bias parameter distributions. Inherent in bias analysis is the dependence of our results on the specification of bias parameters, which reflect what is known from available data, literature, or theory on the extent of bias that may occur. There is uncertainty about how we define these distributions or values; otherwise, if the precise values of the bias parameters were known, we could simply correct the estimates and probabilistic bias analysis would not be useful.

Although some forms of probabilistic bias analysis can be applied to summarized data, for example, frequencies in a contingency table, the methods are most often implemented with unsummarized data in its original form, as implemented here.

In choosing specific distributions for the bias parameters, different specifications may yield density functions where most of the density is within a similar interval, which means the choice of the specific distribution will not be sensitive to the particular choice of density.

\hypertarget{bayesian-melding}{%
\section{Bayesian Melding}\label{bayesian-melding}}

As mentioned in the \protect\hyperlink{overview}{Overview of the Approach}, we use Bayesian melding to incorporate information on the asymptomatic rate of infection to inform the prior distributions for \(\alpha\), \(\beta\), and \(\Pr(S_1|\text{untested})\). In this section, we discuss Bayesian melding in general terms, which provides the background needed to understand how we apply Bayesian melding with bias parameters of the probabilistic bias analysis, detailed in the \protect\hyperlink{meld}{following section}.

The Bayesian melding approach was proposed by Poole et al. (Poole \& Raftery, 2000).

This approach enables us to account for both uncertainty from inputs and outputs of a deterministic model. The initial motivation for the approach was to study the population dynamics of whales in the presence of substantial uncertainty around model inputs for population growth (Poole \& Raftery, 2000). However, the framework provided by Poole et al.~can applied in any circumstance where we have uncertainty around some quantities \(\theta\) and \(\phi\) where there is a deterministic function \(M:\theta \to\phi\). Due the utility of Bayesian melding in various contexts, since this deterministic model \(M\) could take on a wide range of forms, the approach has since been applied in various fields, including urban simulations (Ševčíková, Raftery, \& Waddell, 2007), ecology (Robson, 2014), and infectious disease (Powers et al., 2011).

Let \(M: \theta \to \phi\) be the deterministic model defined by the function relating a vector of input parameters \(\theta\) to an output vector \(\phi\), and suppose we have a prior on \(\theta\) denoted \(f_\theta(\theta)\) and a prior on \(\phi\) denoted \(f_\phi^{direct}(\phi)\).

However, note that we actually have two distinct priors on \(\phi\). There is the prior formed by the distribution induced on \(\phi\) by the prior for \(\theta\) and the function \(M\), where we denote this induced prior \(f_\phi^{induced}(\phi)\). Secondly, there is the prior on the output \(\phi\) that does not incorporate information from \(\theta\); for our work, we refer to this prior as the direct prior. Generally, these priors are based on different sources of information.

If \(M^{-1}\) exists, we apply a change of variables to obtain the distribution of \(\phi\) from the distribution of \(\theta\). That is, since \(M(\theta) =\phi\), we can write the induced prior \[f_\phi^{induced}(\phi) = f_\theta(M^{-1}(\phi)) |J(\phi)|, \tag{1}\] where \(J(\phi)\) is the Jacobian of the transformation from \(\theta \to \phi\), with \(J(\phi)=\frac{d\theta}{d\phi}\).\footnote{In the continuous case we need to multiply by \(|J(\phi)|\), but not in the discrete case (Blitzstein \& Hwang, 2019).}

However, in practice, \(M^{-1}\) rarely exists since \(\theta\) is often of higher dimensionality than \(\phi\), in which case \(M\) is not invertible. This means we generally approximate \(f_\phi^{induced}\) without acquiring its analytical form.

In addition to this induced prior, we have the prior \(f_\phi^{direct}(\phi)\), which does not involve \(M\) nor the inputs \(\theta\). Since these priors are based on different sources of information and may reflect different uncertainties, often it useful to use both sources of information to inform our estimates. To do so, we need to combine the distributions for \(f_\phi^{induced}\) and \(f_\phi^{direct}\) to create a pooled distribution.

Multiple pooling strategies exist for distinct distributions, but one requirement for a Bayesian analysis is that the distribution should be independent of the order in which the prior is updated and the combining of the prior distribution. That is, updating the prior distributions using Bayes' theorem and then combining distributions should yield the same result as combining distributions and then updating this combined distribution; pooling methods that have this property are deemed externally Bayesian. Logarithmic pooling has been shown to be externally Bayesian under some conditions, which are likely to hold in most settings. Furthermore, logarithmic pooling has actually been shown to be the only pooling method where this holds (Genest, McConway, \& Schervish, 1986). For this reason, Poole \emph{et al.} recommend proceeding with logarithmic pooling for Bayesian melding.

The logarithmically pooled prior for \(\phi\) by pooling the two distributions \(f_\phi^{induced}\) and \(f_\phi^{direct}\) is

\[f_\phi^{pooled} (\phi) = t(\boldsymbol{\alpha}) (f_\phi^{induced}(\phi))^{\alpha} (f_\phi^{direct}(\phi))^{1-\alpha}.\]

The pooling weights are given by \(\boldsymbol{\alpha} = (\alpha, \;\;1-\alpha)\) where \(\alpha \in [0,1]\), and \(t(\boldsymbol{\alpha})\) is the normalizing constant corresponding to the set of pooling weights. More precisely, \(t(\boldsymbol \alpha)\) is given by
\[t(\boldsymbol \alpha) = \Big[ \int_\Phi  (f_\phi^{induced}(\phi))^{\alpha} (f_\phi^{direct}(\phi))^{1-\alpha} d\phi \Big]^{-1}.\]

Multiplying by \(t(\boldsymbol \alpha)\) ensures \(f_\phi^{pooled} (\phi)\) is a valid probability density.

Commonly, a choice of \(\alpha = 0.5\) is used to give the priors equal weight. In this case, logarithmic pooling may be referred to as geometric pooling since it is equivalent to taking a geometric mean.

At this point, we see that we can obtain the pooled distribution on \(\phi\) by taking the logarithmic pool of \(f^{induced}_\phi(\phi)\) and \(f^{direct}_\phi(\phi)\). However, we still need to consider how to obtain the melded distributions of the inputs \(\theta\) when \(M\) is not invertible.

\hypertarget{simple-discrete-example}{%
\subsection{Simple Discrete Example}\label{simple-discrete-example}}

To develop intuition the strategy Poole et al.~recommend for handling the case where \(\phi\) is not invertible, we consider a mapping \(M: \theta \to \phi\) for \(\theta \in \mathbb{R}\) and \(\phi \in \mathbb{R}\)
defined as follows (Figure \ref{fig:dex}). Note the choice of \(f_\theta,f_\phi^{direct}\) does not matter here as long as they are valid densities.
\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{thesis_files/figure-latex/unnamed-chunk-11-1} 

}

\caption{\label{fig:dex}A simple discrete example where $M$ is not invertible, since $M(2)=M(3)=2$.}\label{fig:unnamed-chunk-11}
\end{figure}
\begin{table}[H]

\caption{\label{tab:unnamed-chunk-12}\label{table:example-dist}Specifing the Distributions of $f_\theta(\theta)$ and $f_\phi^{direct}(\phi)$}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
$\theta$ & $f_\theta(\theta)$ & $M(\theta)=\phi$ & $f_\phi^{direct}(\phi)$\\
\hline
1 & 0.3 & 1 & 0.4\\
\hline
2 & 0.2 & 2 & 0.6\\
\hline
3 & 0.5 & 2 & 0.6\\
\hline
\end{tabular}
\end{table}
We see that \(M\) is not invertible since \(\theta=1\) and \(\theta = 2\) both map to \(\phi=2\), which implies the inverse \(M^{-1}\) would not be well defined.

We can generate a sample from the density \(f_\phi^{induced}\) by sampling from \(f_\theta\) and computing \(M(\theta)\).

That is, we have
\begin{align*}
f_\phi^{induced}(1) &= f_{\theta}(1) = 0.3 & \text{ (since $\theta = 1$ maps to $\phi = 1$) } \\
f_\phi^{induced}(2) &= f_{\theta}(2) +  f_{\theta}(3) = 0.2 + 0.5=  0.7 & \text{ (since $\theta = 2$ and $\theta=3$ both map to $\phi = 2$) .}
\end{align*}
Then, we can compute the logarithmically pooled pooled prior. If we have weights \(\alpha=\{0.5,\;0.5\}\), then the normalizing constant \(t(\alpha)\) is given by
\begin{align*} 
t(\alpha) &= \left[ \sum_\Phi  (f_\phi^{induced}(\phi))^{\alpha} (f_\phi^{direct}(\phi))^{1-\alpha} \right]^{-1} \\
&=  \left[ \sum_{\phi \in \{1,2\}}  (f_\phi^{induced}(\phi))^{0.5} (f_\phi^{direct}(\phi))^{0.5} \right]^{-1} \\
&= \Big[ (f_\phi^{induced}(1))^{0.5} (f_\phi^{direct}(1))^{0.5} + (f_\phi^{induced}(2))^{0.5} (f_\phi^{direct}(2))^{0.5} \Big]^{-1} . \tag{1}\\
\end{align*}
\noindent Using values of \(f^{induced}\) calculated above and \(f^{direct}\) from Table \ref{table:example-dist}, we obtain
\begin{align*}
f_\phi^{induced}(1))^{0.5} (f_\phi^{direct}(1))^{0.5}&= (0.3)^{0.5}(0.4)^{0.5} =0.3464\\
f_\phi^{induced}(2))^{0.5} (f_\phi^{direct}(2))^{0.5} &= (0.7)^{0.5} (0.6)^{0.5}=0.6481,
\end{align*}
\noindent so we can plug these terms into (1) to yield

\[ t(\alpha) = [0.3464 +0.6481 ]^{-1}. \]

\noindent Denoting the pooled prior in \(\phi\)-space as \(f_\phi^{pooled}(\phi)\), we have
\begin{align*}
f_\phi^{pooled}(1) &= t(a)(f_\phi^{induced}(1))^{0.5} (f_\phi^{direct}(1))^{0.5} \\
&= [0.3464 + 0.6481]^{-1} (0.3464)  = 0.3483\\
f_\phi^{pooled}(2)  &= t(a)(f_\phi^{induced}(2))^{0.5} (f_\phi^{direct}(2))^{0.5} \\
&= [0.3464 + 0.6481]^{-1}   (0.6481) 
=0.6517.
\end{align*}
We summarize these results and compare \(f_\phi^{induced}, f_\phi^{direct}\), and \(f_\phi^{pooled}\) in Figure \ref{fig:comp}, the specific values for each probability mass are in Table \ref{table:table-pooled} .
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-14-1} 

}

\caption{\label{fig:comp} Comparing the direct, induced, and pooled distributions on $\phi$. Each is a discrete distribution where $\phi$ can take on only values 1 or 2.  Note that the probability mass of a given value of $\phi$ for the pooled distribution falls in between the probability masses of the induced and direct distributions for that value of $\phi$.}\label{fig:unnamed-chunk-14}
\end{figure}
\begin{table}[H]

\caption{\label{tab:unnamed-chunk-15}\label{table:table-pooled} Comparing the probability mass values of the direct, induced, and pooled distributions on $\phi$.}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
$\phi$ & $f_\phi^{direct}(\phi)$ & $f_\phi^{induced}(\phi)$ & $f_\phi^{pooled}(\phi)$\\
\hline
1 & 0.4 & 0.3 & 0.3483\\
\hline
2 & 0.6 & 0.7 & 0.6517\\
\hline
\end{tabular}
\end{table}
However, we also want the pooled prior on the inputs \(\theta\), that is, \(f_\theta^{pooled}(\theta)\).

Poole et al.~reasoned as follows. Since \(M\) uniquely maps \(\theta=1\) to \(\phi =1\), the probability that \(\theta=1\) should be equal to the probability \(\phi = 1\). That is, we should have \(f_\theta^{pooled}(1) = f_\phi^{pooled}(1)\).

However, the relationship for \(\theta=2\) or \(\theta=3\) to \(\phi\) is not one to one. Since \(M(2)=2\) and \(M(3)=2\), the sum of the probabilities for \(\theta=1\) and \(\theta=2\) should be equal to that for \(\phi=2\), that is, \(f_\theta^{pooled}(2) + f_\theta^{pooled}(3) = f_\phi^{pooled}(2) = 0.6517\).

The challenge here is how we divide the probability for \(f_\phi^{pooled}(2)\), which is defined, among \(f_\theta^{pooled}(2)\) and \(f_\theta^{pooled}(3)\). The prior for \(\phi\) yields no information to assist in this choice, because knowing which value \(\phi\) takes on does not give us any information about whether \(\theta=2\) or \(\theta=3\). Thus, the information we have about \(\theta\) must be taken from \(f_\theta(\theta)\).

That is, we can assign a probability for \(f_\theta^{pooled}(2)\) by considering the probability that \(\theta = 2\) relative to the probability \(\theta =3\), computing

\[f_\theta^{pooled}(2) = f_\phi^{pooled}(2) \Big( \frac{f_\theta(2)}{f_\theta(2) + f_\theta(3)}\Big).\]

That is, if the probability \(\theta\) takes on the value \(2\) is lower in this case than the probability \(\theta=3\) which we know from the prior on \(\theta\), \(f_\theta(\theta)\), then the pooled prior on \(\theta\), \(f_\theta^{pooled}(2)\), should reflect this.

Using this reasoning, we have
\begin{align*} f_\theta^{pooled}(2) &= (0.7) \frac{0.2}{0.2+0.5} = 0.1862\\
f_\theta^{pooled}(3) &= (0.7) \frac{0.5}{0.2+0.5} = 0.4655.
\end{align*}
The result in this simple example, using \(f_\theta(\theta)\) to determine how to distribute the probability for values of \(\phi\) where multiple \(\theta\) map to \(\phi\), can be used to derive general formulas to compute \(f_\theta^{pooled}(\theta)\) for discrete and continuous distributions (Poole \& Raftery, 2000).

\hypertarget{general-solution-for-the-discrete-case}{%
\subsection{General Solution for the Discrete Case}\label{general-solution-for-the-discrete-case}}

Denote the possible values of \(\theta\) as \(A_1, A_2, \dots\), the possible values of \(\phi\) as \(B_1, B_2, \dots\), and a mapping \(m: \mathbb{N} \to \mathbb{N}\) such that \(M(A_i) = B_{m(i)}\) and \(C_j = M^{-1}(B_j) = \{A_i : M(A_i) = B_j\}\). Then

\[f_\theta^{pooled}(A_i) = f_\phi^{pooled}(B_{m(i)}) \left( \frac{f_\theta(A_i)}{f_\phi^{induced}(B_{m(i)})} \right).\]

\hypertarget{general-solution-for-the-continuous-case}{%
\subsection{General Solution for the Continuous Case}\label{general-solution-for-the-continuous-case}}

We denote \(B = M(A) = \{M(\theta) : \theta \in A \}\) and \(C = M^{-1}(B) = \{\theta: M(\theta) \in B \}\).

Then

\[
f_\phi^{pooled} (M(\theta)) =t({\alpha}) f_\theta(\theta) \left( \frac{f_\phi^{direct}(M(\theta))}{f_\phi^{induced}(M(\theta))} \right)^{1-\alpha} \tag{2}
\]
where \(t({\alpha})\) is a renormalizing constant for the choice of \(\alpha\).

\hypertarget{implementation-through-the-sampling-importance-resampling-algorithm}{%
\subsection{Implementation through the Sampling-Importance-Resampling Algorithm}\label{implementation-through-the-sampling-importance-resampling-algorithm}}

We can obtain the pooled distributions \(f^{pooled}_\theta\) and \(f^{pooled}_\phi\) by using the Sampling-Importance-Resampling Algorithm.

The steps are as follows.
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We draw \(m\) observations of \(\theta\) from its prior distribution \(f_\theta(\theta)\).
\item
  For every \(\theta_i\) we compute \(\phi_i = M(\theta_i)\) to obtain a sample from the induced distribution.
\item
  Since the density \(f_\phi^{induced}(\phi)\) is unlikely to have an analytical form, we can compute it via a density approximation such as kernel density estimation.
\item
  Construct weights proportional to the ratio of the prior on \(\phi\) evaluated at \(M(\theta_i)\) to the induced prior \(f_\phi^{induced}\) evaluated at \(M(\theta_i)\). If a likelihood \(L_1(\theta)\) for the inputs and \(L_2(\phi)\) is available, the weights are
  \[w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{1-\alpha}L_1(\theta_i) \; L_2(M(\theta_i)).\]
  However, in this work, no likelihood is available for the variables of interest, so the likelihood is left out of the weights, leaving us with
  \[w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{1-\alpha}.\]
\item
  Sample \(r\) observations from \(\theta\) and \(\phi\) from step (1) with probabilities proportional to the weights from (4).
\end{enumerate}
\newpage

\hypertarget{meld}{%
\section{Bayesian Melding Applied to COVID-19 Misclassification}\label{meld}}

~~~We can relate the inputs \(\theta = \{\Pr(S_1|\text{untested}), \alpha, \beta \}\) and \(\phi = \Pr(S_0|\text{test}_+,\text{untested})\) by the deterministic model\footnote{For the derivation of \(M\) using the laws of probability, see the section \protect\hyperlink{derivation}{Derivation of \(M\)}.} \(M: \theta \to \phi\) given by \[\Pr(S_0|\text{test}_+, \text{untested}) = \dfrac{\beta(1 - \Pr(S_1|\text{untested}))}{\beta(1-\Pr(S_1|\text{untested})) + \alpha \Pr(S_1|\text{untested})}.\]

Now, we have two distributions on \(\phi\). One is the distribution we can specify based on data on the asymptomatic rate of infection of COVID-19. We refer to this prior as the direct prior. We also have the distribution generated by taking \(M(\theta)\) where \(\theta\) represents the values from the defined distributions of \(\alpha,\beta,\) and \(\Pr(S_1|\text{untested})\). With Bayesian melding, we pool these distributions using logarithmic pooling, and then implement the sampling-importance-resampling algorithm to obtain constrained distributions of the inputs \(\theta\) that are in accordance with information about the asymptomatic rate of the virus.

Due to the uncertainty around our definitions of \(\alpha\) and \(\beta\), it is particularly useful to leverage the information we have about the asymptomatic rate of the virus \(\Pr(S_0|\text{test}_+,\text{untested})\) because a large collection of studies has been published in this area. In a meta-analysis pooling data from 95 studies, the pooled estimate among the confirmed population that was asymptomatic was 40.50\% {[}95\% CI, 33.50\%-47.50\%{]} (Ma et al., 2021b). Another meta-analysis including 350 studies estimated the asymptomatic percentage to be 36.9\% {[}95\% CI: 31.8 to 42.4\%{]}, and, when restricting to screening studies, 47.3\% (95\% CI: 34.0\% -61.0\%) (Sah et al., 2021b).

To summarize, we have two priors on the asymptomatic rate \(\phi\), that by taking \(M(\theta)\) for sampled values of \(\theta\), denoted \(f_\phi^{induced}\) in the previous section, and that based on data about the asymptomatic rate, \(f_\phi^{direct}\). By applying Bayesian melding, we can obtain the contrained distributions of \(\alpha,\beta,\) and \(\Pr(S_1|\text{untested})\) that are in accordance with information about the asymptomatic rate of infection.

In this section, we use the distributions of \(\alpha,\beta,\) and \(\Pr(S_1|\text{untested})\) as specified in in \protect\hyperlink{defpriors}{Chapter 4.}

\newpage

The first step of Bayesian melding in this context is obtaining a sample of size \(m\) from the specified distributions of \(\theta\), which are shown in Figure \ref{fig:theta}. This gives us \(\{ \theta_1, \theta_2, \dots, \theta_m\}\), where \(\theta_i = \Big(\alpha_i,\beta_i,\Pr(S_1|\text{untested})_i\Big)\). We sample each of \(\alpha,\beta,\) and \(\Pr(S_1|\text{untested})\) independently.

Of note, while \(\Pr(S_1|\text{untested})\) must be less than 1 since it is a probability, \(\alpha\) and \(\beta\) are ratios of probabilities, and as such they are not constrained to be less than 1. In light of this, we specify \(\alpha\) as a gamma distribution, since it is reasonable that
\(\alpha = \frac{\Pr(\text{test}_+|\text{untested}, S_1)}{\Pr(\text{test}_+|\text{tested})}\) could exceed 1. However, since \(\beta =\frac{\Pr(\text{test}_+|\text{untested}, S_0)}{\Pr(\text{test}_+|\text{tested})}\), we specified \(\beta\) to be a beta distribution based on the assumption that an untested individual with no symptoms would not have a higher probability of testing positive than someone who was tested (and may or may not have symptoms).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/theta-samp-chap2} 

}

\caption{\label{fig:theta}Sampling from the defined distributions of $\alpha$, $\beta$, and $\Pr(S_1|\text{untested})$. $\Pr(S_1|\text{untested})$ must be less than 1 since it is a probability, but $\alpha$ and $\beta$ are ratios of probabilities, so they do not have this constraint. Since it is plausible that $\alpha = \frac{Pr(\text{test}_+|\text{untested}, S_1)}{\Pr(\text{test}_+|\text{tested})}$ could take on values larger than 1, we specify $\alpha$ as a gamma distribution. The others are beta distributions.}\label{fig:create-theta}
\end{figure}
Then, by applying the function \(M\) to our sampled values of \(\theta\), we can compute the induced distribution \(f_\phi^{induced}(M(\theta))\) and compare it to our prior on \(\phi\) from meta-analyses on the asymptomatic rate, \(f_\phi^{direct}(\phi)\) (Figure \ref{fig:prior-induced}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/induced-direct} 

}

\caption{\label{fig:prior-induced}The induced prior is generated by computing $M(\theta)$ for sampled values of $\theta$ and estimating the density. The direct prior is informed based on meta-analyses on the asymptomatic rate of COVID-19, which generally support values between 0.3 and 0.6. We see here that there is substantial disagreement between the priors, where the induced prior places much more density near 1, while the mode of the direct prior is close to 0.35. This disagreement is a key part of the motivation for Bayesian melding -- we want to constrain our sampled values of $\alpha,\beta,$ and $\Pr(S_1|\text{untested})$ to be in accordance with information on the asymptomatic rate of COVID-19.}\label{fig:unnamed-chunk-18}
\end{figure}
\hypertarget{pooling}{%
\subsection{Pooling}\label{pooling}}

At this point, we want to obtain the logarithmically pooled distribution of the two priors we have on \(\phi\), denoted \(f_\phi^{pooled}\).

Now, as described in greater detail in the section on the \protect\hyperlink{logpooled}{Sampling-Importance-Resampling algorithm}, to obtain the logarithmically pooled prior with equal pooling weights on \(\phi\), we sample with the weights \[w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{0.5}. \tag{1}\]

We perform a kernel density estimation to approximate the density of \(f_\phi^{induced}(\phi)\) at the coordinates \(\phi_1, \dots, \phi_M\). The kernel desntiy estimation of \(f_\phi^{induced}(\phi)\) corresponds to the \emph{Induced Distribution} in Figure \ref{fig:melded}. To compute \(f_\phi^{direct}(\phi)\), we can use the density function \(f_\phi^{direct}\).

We will denote \(r\) as the posterior sample size. Once we calculate the weights from (1) using the density function for \(f^{direct}\) and the kernel denstiy estimates for \(f^{induced}\), we obtain a resample of \(r\) observations from \(\phi_1,\dots,\phi_m\) by sampling with these weights.

The result is approximately a sample from the target distribution \[t(\alpha) \Big( f^{induced}(M(\theta)) \Big)^{0.5} \Big( f^{direct} (M(\theta)) \Big)^{0.5},\] where \(t(\alpha)\) is the normalizing constant needed to make the pooled density valid.

Similarly, we obtain a resample of size \(r\) from the initially sampled \(\theta_1, \dots, \theta_m\) with this same set of weights to obtain the melded distributions for the inputs. In Figure \ref{fig:melded}, these resamples for \(\theta\) and \(\phi\) are denoted as \emph{After Melding} in Figure \ref{fig:melded}, while the samples before resampling are denoted as \emph{Before Melding}.

Looking at the distributions before and after melding, we see that melding shifts \(\beta\) to have density lower than the version we specified originally. Since \(\beta\) is defined as the ratio of the test positivity among the population with no symptoms or mild COVID-19-like symptoms, that is, \(\dfrac{\Pr(\text{test}_+|S_0,\text{untested})}{\Pr(\text{test}_+|\text{tested})}\), this indicates that values of this ratio that are smaller, between 0 and 0.025, are in greater concordance with information on the asymptomatic rate than the distribution that placed a substantial amount of density on values up until 0.5.

Meanwhile, melding shifts \(\Pr(S_1|\text{untested})\) to have more density at greater values. Of note here, simulations along with experimentation with a \href{https://q-w-a.shinyapps.io/bayesian_melding_priors/}{Shiny app} demonstrated that with larger values of \(\Pr(S_1|\text{untested})\), the induced prior is more similar to the direct prior.

Lastly, the change in \(\alpha\) is minimal. In general, \(M\) is much less sensitive to \(\alpha\) than to \(\beta\) or \(\Pr(S_1|\text{untested})\). There are a couple of reasons for this. For one, \(\alpha\) is multiplied by \(\Pr(S_1|\text{untested})\), which is generally less than 0.05, which means the change in \(\alpha\) would need to be very large to change \(M(\theta)\) in a meaningful way. Simulations where \(\Pr(S_1|\text{untested})\) was increased resulted in \(\alpha\) changing more with melding, but changes were still small. This leads to the other reason, which is due to the structure of the function itself -- \(\alpha\) is only a term in the denominator, whereas both \(\Pr(S_1|\text{untested})\) and \(\beta\) influence both the numerator and denominator of the fraction. In Supplementary Figure \ref{fig:alpha-little-change}, we see that \(M(\theta)\) changes very little over the full set of values of \(\alpha\), while \(M(\theta)\) is much more sensitive to \(\beta\).

Another note to as we consider this resampling process is how the pooled prior on \(\phi\) is defined on the intersection of the support of \(f_\phi^{direct}\) and \(f_\phi^{induced}\). Since we are resampling from a sample from \(f_\phi^{induced}\) with weights given by \(w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{0.5}\), if \(f^{direct}\) has zero density for some value, the weight will be zero, and, as such, it will have zero density in the pooled distribution as well.

The relevance of this restriction on the support is that issues can arise when using truncated priors, where the density drops abruptly to zero at the truncation bounds. Because truncation introduces another restriction on the support, depending on the specification of priors, the support of the pooled distribution may be in a vary narrow range. Then, when we resample, if there are only a small number of observations in our initial sample \(\phi_1,\dots,\phi_m\) that can be sampled with nonzero weights, the resulting distribution can be highly irregular, where single values are sampled many times.

As a result of this observation, we made two changes to the implementation of melding in Wu et al. (2020). First, we did not truncate the densities. This helps with the issue with irregularity, and also can reflect the genuine uncertainty about where these bounds might be. Secondly, we did not set the sample size and resample size to be identical and instead set the resample size to be 10 times smaller, as is reccomended in Rubin, Gelman, \& Meng (2004).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/meld-chap2} 

}

\caption{\label{fig:melded} Comparing the sampled distributions of $\alpha,\beta$, and $\Pr(S_1|\text{untested})$ to the distributions after melding, when we have resampled these distributions with weights such that when we apply $M$ to the resample, we obtain a distribution of $\phi$ that is the log pool of the direct and induced priors on $\phi$.}\label{fig:unnamed-chunk-19}
\end{figure}
The central idea here is that comparing the induced and direct priors on \(\Pr(S_0| \text{test}_+, \text{untested})\) demonstrated that although there is a region of shared support, many values from the induced distribution we acquire by using \(M\) to generate values of \(\phi\) from sampled values of \(\theta\) are very unlikely to be in accordance with the information we know about the prevalence of SARS-CoV-2 asymptomatic infection. Bayesian melding allows us to constrain the priors we specified related to incomplete testing (\(\alpha,\beta,\) and \(\Pr(S_1|\text{untested})\)) to be in accordance with published analyses on asymptomatic infection. We then use these constrained distributions as inputs in the probabilistic bias analysis.

\hypertarget{impact-of-applying-melding-on-corrected-estimates}{%
\subsection{Impact of Applying Melding on Corrected Estimates}\label{impact-of-applying-melding-on-corrected-estimates}}

We can see the impact of using melded priors in Suffolk county in Massachusetts in Figure \ref{fig:suffolkmelding}. Since using the priors without melding allows for asymptomatic rates \(\Pr(S_0|\text{test}_+,\text{untested})\) that are extremely high, the upper bound of the estimates will be substantially higher than predicted when using the melded priors, which do not include values where the inputs lead to values of asymptomatic rate that are unsupported by available data (e.g., meta-analyses) on the asymptomatic rate.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/suffolk_bayesian_melding} 

}

\caption{\label{fig:suffolkmelding}Comparing the final corrected estimates we obtain when using the melded distributions on $\alpha$, $\beta$, and $\Pr(S_1|\text{untested})$ to those we obtain when omitting the melding step. We see the upper bounds when we do not apply melding are substantially higher. Referring back to Figure \ref{fig:melded}, we can see how the melded distributions contribute to this difference. In particular, the melded density of $\beta$ has higher density at smaller values than the distribution pre-melding, which reduces the estimates of unobserved infections.}\label{fig:unnamed-chunk-21}
\end{figure}
\hypertarget{derivation}{%
\subsection{\texorpdfstring{Derivation of \(M\)}{Derivation of M}}\label{derivation}}

\indent We define \(\theta\) as the set of bias parameters \(\{\Pr(S_1|\text{untested}), \alpha, \beta \}\). The parameters \(\alpha\) and \(\beta\) relate the observed overall test positivity rate to the test positivity rate we would obtain if we tested the asymptomatic and symptomatic partitions of the untested population. We define:
\begin{itemize}
\tightlist
\item
  \(\alpha = \dfrac{\Pr(\text{test}_+|S_1,\text{untested})}{\Pr(\text{test}_+|\text{tested})}\)
\item
  \(\beta = \dfrac{\Pr(\text{test}_+|S_0,\text{untested})}{\Pr(\text{test}_+|\text{tested})}\).
\end{itemize}
The parameter \(\Pr(S_1|\text{untested})\) reflects the probability someone among the untested population has moderate to severe COVID-like symptoms.

We relate this set of parameters to the asymptomatic infection rate \(\phi = \Pr(S_0|\text{test}_+, \text{untested})\) by the function \(M: \theta \to \phi\):
\begin{tcolorbox}
\vspace{2 mm}
\begin{align*}   
 M(\theta)  = \dfrac{\beta (1- \Pr(S_1|\text{untested}))}{\beta(1- \Pr(S_1|\text{untested})) + \alpha(\Pr(S_1|\text{untested})} = \Pr(S_0|\text{test}_+, \text{untested}).\\
\end{align*}
\end{tcolorbox}
\noindent In what follows, we show this equality holds.

\noindent Since we have \(\alpha = \frac{\Pr(\text{test}_+|S_1, \text{untested})}{\Pr(\text{test}_+|\text{tested})}\) and \(\beta = \dfrac{\Pr(\text{test}_+|S_0, \text{untested})}{\Pr(\text{test}_+|\text{tested})}\), we can write
\begin{align*}  &= \dfrac{\dfrac{\Pr(\text{test}_+|S_0, \text{untested})}{\Pr(\text{test}_+|\text{tested})}(1 - \Pr(S_1|\text{untested}))}{\dfrac{\Pr(\text{test}_+|S_0, \text{untested})}{\Pr(\text{test}_+|\text{tested})}(1-\Pr(S_1|\text{untested})) + \dfrac{\Pr(\text{test}_+|S_1, \text{untested})}{\Pr(\text{test}_+|\text{tested})} \Pr(S_1|\text{untested})}
\end{align*}
and cancelling out the term \(\Pr(\text{test}_+|\text{tested})\) we have

\[ = \dfrac{{\Pr(\text{test}_+|S_0, \text{untested})}(1 - \Pr(S_1|\text{untested}))}{\Pr(\text{test}_+|S_0, \text{untested})(1-\Pr(S_1|\text{untested})) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})}.\]

\noindent Since \(\Pr(S_0|\text{untested}) = 1 - \Pr(S_1|\text{untested})\),
\begin{align*} 
&=  \dfrac{{\Pr(\text{test}_+|S_0, \text{untested})}\Pr(S_0|\text{untested})}{\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested}) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})}.
\end{align*}
Applying the definition of conditional probability to the term

\noindent \(\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested})\) in the numerator,
\begin{align*}
&=
    \dfrac{\Big( \dfrac{\Pr(\text{test}_+,S_0, \text{untested})}{\Pr(S_0, \text{untested})} \Big) \Big(\dfrac{\Pr(S_0, \text{untested})}{\Pr(\text{untested})}\Big)}{\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested}) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})}\\ 
    &=  \dfrac{\dfrac{\Pr(\text{test}_+,S_0, \text{untested})}{\Pr(\text{untested})}}{\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested}) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})}.\\
    \end{align*}
\text{Since $\dfrac{\Pr(\text{test}_+,S_0, \text{untested})}{\Pr(\text{untested})} = \Pr(\text{test}_+,S_0|\text{untested})$},
\begin{align*} 
    &=  \dfrac{{\Pr(\text{test}_+,S_0|\text{untested})}}
 {\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested}) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})} .
\end{align*}
Now, consider the first term in the denominator of the previous line, which we can write as \begin{align*} &= \left(\dfrac{\Pr(\text{test}_+,S_0,\text{untested})}{\Pr(S_0,\text{untested})} \right) \left( \dfrac{\Pr(S_0,\text{untested})}{\Pr(\text{untested})}\right)\\
&= \dfrac{\Pr(\text{test}_+,S_0,\text{untested})}{\Pr(\text{untested})} \\
&=  \Pr(\text{test}_+,S_0|\text{untested}).
\end{align*}
\noindent We can substitute this result in for the \(\Pr(\text{test}_+|S_0, \text{untested})\Pr(S_0|\text{untested})\) term in the denominator to yield
\begin{align*}
  &=  \dfrac{{\Pr(\text{test}_+,S_0|\text{untested})}}{\Pr(\text{test}_+,S_0|\text{untested}) + \Pr(\text{test}_+|S_1, \text{untested}) \Pr(S_1|\text{untested})} \hspace{ 20 mm }
\end{align*}
With same reasoning, we can simplify
\begin{align*}
\Pr(\text{test}_+|S_1, \text{untested})\Pr(S_1|\text{untested}) = \Pr(S_1, \text{test}_+|\text{untested}),
\end{align*} giving us
\begin{align*}
  &=  \dfrac{{\Pr(\text{test}_+,S_0|\text{untested})}}{\Pr(\text{test}_+,S_0|\text{untested}) +  \Pr(S_1, \text{test}_+|\text{untested})} \hspace{45 mm }\\ 
   &=  \dfrac{{\Pr(\text{test}_+,S_0|\text{untested})}}{\Pr(\text{test}_+|\text{untested}) } \\
   &= \dfrac{\dfrac{\Pr(S_0, \text{test}_+, \text{untested})}{\Pr(\text{untested})}}{ \dfrac{\Pr(\text{test}_+,\text{untested})}{\Pr(\text{untested})}} \\ 
  &=\dfrac{\Pr(S_0, \text{test}_+, \text{untested})}{\Pr(\text{test}_+,\text{untested})} \\
  &= \Pr(S_0 |\text{test}_+, \text{untested}).
\end{align*}
\noindent Hence, we have
\begin{align*}
\Pr(S_0 |\text{test}_+, \text{untested}) = \dfrac{\beta (1- \Pr(S_1|\text{untested}))}{\beta(1- \Pr(S_1|\text{untested})) + \alpha(\Pr(S_1|\text{untested})}
\end{align*}
\noindent as desired.
\qed

\newpage

\hypertarget{sampling}{%
\section{Sampling-Importance-Resampling Algorithm}\label{sampling}}

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

~~~The Sampling-Importance-Resampling Algorithm, introduced in Rubin (1987), is a non-iterative method for approximating a sample from a target probability density function \(f\) by sampling from a density \(g\). This algorithm is fundamental to the implementation of Bayesian melding.

In the context of Bayesian melding for incorporating information on the asymptomatic rate, \(g\) is the induced distribution of \(\phi\), and the target distribution \(f\) is the log pool of the induced and direct priors on \(\phi\).

The two main steps of the algorithm are the sampling step and importance resampling step. We have two (generally distinct) sample sizes, where \(m\) is the initial sample size and \(r\) is the resample size.

In the sampling step, we draw an independent and identically distributed sample of size \(m\) from \(g\), \(Y_1, Y_2, \dots, Y_m\). Then, we compute weights \(h(Y)\) such that \(g \cdot h \propto f\). That is, we set the weights

\[w_i = h(Y_i) = \dfrac{\frac{f(Y_i) } {g(Y_i)} }{\sum_{i=1}^m\frac{f(Y_i) } {g(Y_i)} }.\]

We resample with these defined weights to obtain a sample of size \(r\) from \(Y_1, Y_2, \dots, Y_m\). We denote this resample \(Z_1,\dots, Z_r\). With these weights, \(Z_1,\dots, Z_r\) is approximately a sample from \(f\).

The method is most efficient when \(g\) is a good approximation of \(f\). The relationship between the sample size \(m\) and resample size \(r\) also has implications for the quality of the approximation. The algorithm generates independent and identically distributed samples as \(m/r \to \infty\), but in most applications \(m/r\) between 10 and 20 is appropriate (Rubin et al., 2004). The practical implications of this choice are discussed \protect\hyperlink{presamp}{later in this section}.

To better understand the use of this algorithm, we provide a proof that formally relates the choice of \(g\), weights \(h\), and the target distribution \(f\). We then follow up with a couple concrete examples where there is a closed formed solution to visualize how the algorithm works in practice.

\hypertarget{proof}{%
\subsection{Proof that Algorithm Obtains Approximate Sample from Target Distribution}\label{proof}}

To gain further insight into how sampling with weights
\(w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{0.5}\)
approximates a sample from the target distribution the logarithmically pooled distribution \(f^{pooled}\), we first prove a more general result.
\begin{tcolorbox}[title=Sampling $g$ with weights defined by $h$]
Suppose we sample $Y_1, Y_2, \dots, Y_m$ independently and identically distributed with probability density function  $g$ and compute the weights
\[ w_i =\dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }\]
for some nonnegative function $h$ defined on the support of $Y$.

If  we sample $Z_1, \dots, Z_r$ from the discrete distribution $Y_1,\dots, Y_m$ such that 

\[ \Pr(Z = Y_i) = \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) } = w_i ,\]
then $Z_1, \dots, Z_r$ is approximately a sample with density proportional to $h \cdot g$.

\end{tcolorbox}
\vspace{5 mm}

Since \(Z\) is sampled from \(Y\), we have
\[ \Pr(Z \leq x ) = \sum_{z_i \leq x} \Pr(Z=z_i) = \sum_{Y_i \leq x} \Pr(Z=Y_i) .\]

We can take this sum to be over all possible values of \(Y\) by including the indicator function \(\mathbb{I} (Y_i \leq x)\), yielding
\[  = \sum_{i = 1}^m \Pr(Z=y_i)\;\;\mathbb{I} (Y_i \leq x).  \]
and since \(\Pr(Z=Y_i) = \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }\) by definition we have
\begin{align*} 
&= \sum_{i = 1}^m \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x)   \\
&=  \left( \dfrac{1}{ {\sum_{i=1}^mh(Y_i) }} \right) {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x)   \\
&=   \dfrac{ {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\sum_{i=1}^mh(Y_i) } \\
&=   \dfrac{ \frac 1m {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\frac 1m \sum_{i=1}^mh(Y_i) }. \\
\end{align*}
Now, we need the Weak Law of Large Numbers. That is, if we have a sequence of random variables \(X_1, X_2, \dots\) with finite variance, then,
\[ \lim_{n \to \infty} \left( \frac{1}{n} \sum_{i=1}^n X_i \right)  = E(X_i). \]

Applying this law to both the numerator and denominator, we obtain
\begin{align*}  \lim_{m \to \infty} \left( \dfrac{ \frac 1m {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\frac 1m \sum_{i=1}^mh(Y_i) } \right) &= \dfrac{ E_g[ h(Y) \;\; \mathbb I (Y \leq x) ]  }{ E_g[ h(Y) ]  }\\
&= \dfrac{\int_{-\infty}^\infty h(y) \;\; \mathbb I (y \leq x) \; g(y) \; dy}{\int_{-\infty}^\infty h(y) \, g(y) \;dy}\\
&= \dfrac{\int_{-\infty}^x h(y) \, g(y) dy}{\int_{-\infty}^\infty h(y) \, g(y) \;dy}\\
&\propto \int_{-\infty}^x h(y) \, g(y) dy. 
\end{align*}
It follows that the probability density function of \(Z\) is proportional to \(h \cdot g\).

\vspace{3 mm}

\qed

It is easiest to understand the Sampling-Importance-Resampling Algorithm when the resampled distribution has a closed form, which we can see in the following two examples.

\newpage

\hypertarget{example-1}{%
\subsubsection{Example 1:}\label{example-1}}

Suppose \(Y \sim Exp(\lambda)\), so we have the PDF \(f_Y(y) = \lambda e^{-\lambda y}\), and we sample \(Z_1,\dots,Z_r\) from \(Y_1, \dots, Y_m\) with weights direction proportional to \(X\), that is, \(h(Y) = Y\).

Then \(Z_1,\dots Z_r\) is approximately a sample from \(h(x) \cdot f_Y(y) = y \; \lambda e^{-\lambda y}\).

From the PDF of the gamma distribution, \(\dfrac{\beta^\alpha}{\Gamma(\alpha) }y^{\alpha - 1} e^{-\beta y}\) we can recognize that \(y \cdot e^{-\lambda y}\) corresponds to the gamma distribution with \(\alpha = 2\) and \(\beta = \lambda\).

We can see this result in Figure \ref{fig:ex12}, where we plot the distribution of \(Y\), the distribution of the resample of \(Y\) sampled with weights defined by \(h\), and compare it to the gamma distribution with \(\alpha = 2\) and \(\beta = \lambda\).

Then, we can see that the PDF of the the gamma distribution with \(\alpha = 2\) and \(\beta = \lambda\) corresponds to the post-sampling distribution as expected (Figure \ref{fig:ex12}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-23-1} 

}

\caption{\label{fig:ex12} The distribution in gold is the exponential random variable $Y \sim Exp(0.2)$. Then, we take a weighted sample of $Y$ with weights that are directly proportional to $Y$, that is, $h(y)=y$. The weighted sample is we obtain the gamma distribution Gamma(2, 0.2), which is the product of the probability density function of $Y$ and the function $h$.}\label{fig:unnamed-chunk-23}
\end{figure}
\newpage

\hypertarget{example-2}{%
\subsubsection{Example 2:}\label{example-2}}

Similarly, again suppose \(Y \sim Exp(\lambda)\), so \(f_Y(y) = \lambda e^{-\lambda y}\). However, now we sample with weights defined by \(h(y)= e^{-\lambda y}\).
Then our sample \(Z_1,\dots,Z_r\) is approximately a sample from
\begin{align*} 
h(y) \; f_Y(y) &=   e^{-\lambda y} \cdot \lambda e^{-\lambda y}\\
&= \ e^{-2 \lambda y},  
\end{align*}
which is proportional to the exponential distribution with parameter \(2\lambda\).

The distributions before and after resampling are shown in Figure \ref{fig:ex22}, along with the PDF of the exponential distribution with parameter \(2\lambda\).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-25-1} 

}

\caption{\label{fig:ex22}The distribution of an exponential random variable $Y \sim Exp(0.2)$ is in gold. After we obtain a weighted sample from $Y$ by sampling with weights defined by $h(y) e^{-\lambda y}$, we see the resulting distribution is proportional to the distribution $Exp(2\lambda)$, as we see by comparing the distribution of the resample to the theoretical density of  $Exp(2\lambda)$ is shown in red.}\label{fig:unnamed-chunk-25}
\end{figure}
\newpage

\hypertarget{logpooled}{%
\subsection{Obtaining Logarithmic Pooled Distribution with the Sampling-Importance-Resampling Algorithm}\label{logpooled}}

As outlined in Carvalho, Villela, Coelho, \& Bastos (2023), we can formally define logarithmic pooling as follows.

If we have a set of densities \(\{ f_1(\phi), f_2(\phi), \ldots, f_n(\phi)\}\) and corresponding pooling weights \(\boldsymbol{\alpha}=\{\alpha_1, \alpha_2, \ldots, \alpha_n\}\), then the pooled density is
\[f^{\text{pooled}}(\phi) = t(\boldsymbol{\alpha}) \prod_{i=0}^n f_i(\phi)^{\alpha_i},\]
where \(t(\boldsymbol{\alpha})\) is the normalizing constant \(t(\boldsymbol{\alpha}) = \dfrac{1}{ \int_{\Phi}\prod_{i=0}^n f_i(\phi)^{\alpha_i} d\phi}\) to ensure the pooled density is a valid probability density.

The case for this work is more simple: we only have two densities we wish to pool, \(f_\phi^{induced}\) and \(f_\phi^{direct}\), and we assign them equal weights by letting \(\boldsymbol{\alpha} = \{.5, .5\}\). This yields

\[f^{pooled}(\phi) = t(\boldsymbol \alpha) \left( f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi) \right)^{0.5}.\]

Since our target distribution is \(t(\boldsymbol \alpha) \left( f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi) \right)^{0.5}\), and we have a sample from \(f^{induced}\), we compute the weights such that
\begin{align*} w_i &\propto \dfrac{ \left( f^{induced} (\phi_i) \right)^{0.5} \left( f^{direct} (\phi_i) \right)^{0.5} } {f^{induced}(\phi_i)} \\
&=  \dfrac{ \left( f^{direct} (\phi_i) \right)^{0.5} } {\left( f^{induced} (\phi_i) \right)^{0.5} } \\
&=   \left( \dfrac{  f^{direct} (\phi_i) } { f^{induced} (\phi_i) }  \right)^{0.5}. \\
\end{align*}
Sampling from \(f^{induced}\) with these weights will yield a sample with approximately the target density \(t(\alpha) \left(f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi)\right)^{0.5}\) from the result in the \protect\hyperlink{proof}{previous section}.

\hypertarget{presamp}{%
\subsection{Implications of the Sample Size and Resample Size}\label{presamp}}

When we have an initial sample of size \(m\) from \(g\), denoted \(Y_1,\dots,Y_m\), and take a weighted sample of size \(r\), \(Z_1,\dots,Z_r\), the choices of \(m\) and \(r\) can have notable effects on the estimated distribution of the resample. In particular, when the sample size and resample size do not differ substantially, it becomes more likely that we will sample some element of \(Y_1,\dots,Y_m\) more than once. This can result in irregularities in the estimated distribution of \(Z_1,\dots,Z_r\). We see this in \ref{fig:prepostsamp} when the ratio of \(m/r\) is closer to 1, while the problem reduces as we increase the sample size \(m\) compared to the posterior (resample) size \(r\).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/pre-post-sampling-code-1} 

}

\caption{\label{fig:prepostsamp}}\label{fig:pre-post-sampling-code}
\end{figure}
When using the Sampling-Importance-Resampling algorithm to obtain the logarithmically pooled distribution, see the effect of this choice has a major impact when we are melding truncated distributions. The pooled distribution is only defined on the intersection of the supports of the distributions being pooled. Truncation, then, can limit the choices of \(Y_1,\dots, Y_m\) we take when resampling, which can lead to substantial irregularities in the resulting estimated pooled distribution (Figure \ref{fig:trunc}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/effect-truncation} 

}

\caption{\label{fig:trunc} Here, we see the effect of the initial sample size, when keeping the posterior sample size constant. The effect is compounded by the use of a truncated density for $P(S_0|\text{test}_+,\text{untested})$, which restricts the support of the pooled distribution. This restricted support can lead to sampling a single observation many times, resulting in the irregularities we see in the distributions, particularly in the first panel. Increasing the sample size, and, as such increasing the ratio of the sample size to the resample size, does help in reducing these irregularities.}\label{fig:unnamed-chunk-29}
\end{figure}
\newpage

\hypertarget{loess-smoothing}{%
\section{LOESS Smoothing}\label{loess-smoothing}}

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

Locally estimated scatterplot smoothing (LOESS) fits a collection of local regression models to obtain a smooth curve through the observed data (Figure \ref{fig:loess}). It is highly flexible in the sense that we do not have to specify the functional relationship between the predictor and response variable for the entire range of the predictor, which may be impossible in various settings. It is particularly useful when working with time series data with substantial noise.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-30-1} 

}

\caption{\label{fig:loess}LOESS curve fitted with  a span of 0.2. }\label{fig:unnamed-chunk-30}
\end{figure}
To perform LOESS smoothing, we estimate a set of local regressions (Chambers, 1997). To do this, we must specify the span; this smoothing parameter is the fraction of the data that is used for the local polynomial fit. With a smaller span, the resulting curve will fit the trends more closely, while a larger span reflects broader trends (Figure \ref{fig:smooth-spans}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-31-1} 

}

\caption{\label{fig:smooth-spans}}\label{fig:unnamed-chunk-31}
\end{figure}
For more detail on how we fit the LOESS curve, see the \protect\hyperlink{loess}{LOESS Smoothing} section in the appendix.

The way we use LOESS smoothing for the purposes of probabilistic bias analysis is for smoothing survey data from the COVID-19 Trends and Impact Survey (Reinhart et al., 2021). We use these survey estimates to inform the priors for \(\beta\) and \(\Pr(S_1|\text{untested})\).

One important choice in the use of LOESS smoothing for the survey data is selecting the smoothing parameter \(h\), which represents the fraction of the data that is used for the local polynomial fit; smoothing methods are very sensitive to the choice of this parameter.

Methods exist for picking the smoothing parameter \(h\) that minimizes the mean squared error between the predicted values from the estimated line and observed values of the dependent variable, for example, leave-one-out cross-validation or generalized cross-validation.

However, in the context of smoothing the COVID-19 Trends and Impact Survey data, we choose the smoothing parameter for each variable based on domain knowledge regarding the level of noise present for each variable of interest. For example, there is substantial noise in the screening test positivity data that reflect trends that do not represent meaningful differences in the screening test positivity. Some trends in the screening sensitivity may be due to scheduled workplace screenings happening at regular time intervals, and some of the variation may be due to the frequency of screening testing due to other variables, such as the access and cost of testing.

Since the ratio \(\frac{\text{screening test positivity}}{\text{overall test positivity}}\) is used to estimate \(\beta = \frac{P(\text{test}_+| S_0, \text{untested})}{P(\text{test}_+|tested)}\), the variability in the screening positivity creates substantial variability in our estimates of \(\beta\).

In light of this variability and the presence of other trends regarding the screening test positivity, we set the span to \(\frac{4}{12} = 0.33\) to fit the local regressions for 4-month intervals with the aim to capture the broader trends over time.

There was less variabiity in the smoothing span for the weighted percentage of COVID-like illness, the estimate of \(P(S_1|\text{untested})\). Hence, we set the smoothing parameter to \(0.2\) detect trends at a finer time scale.

\newpage

\hypertarget{kernel-density-estimation}{%
\section{Kernel Density Estimation}\label{kernel-density-estimation}}

\hypertarget{overview-2}{%
\subsection{Overview}\label{overview-2}}

When we have a random sample \(X_1,\dots X_n\) drawn from the density \(f_X\) and we want to estimate \(f_X\) at some set of points, we can use kernel density estimation. This is relevant in this work for estimating \(f^{induced}\).

We define a kernel function as follows (Wasserman, 2006).
\begin{tcolorbox}[title=Definition: Kernel Function]

A kernel function $K$ is a smooth nonnegative function such that 

$$\int K(x) \; dx = 1, \int x K(x) dx = 0, \sigma^2_k \equiv \int x^2 K(x) dx > 0.$$ 
\end{tcolorbox}
The Gaussian kernel \(K(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\) is commonly used in practice; the tricube kernel, as discussed in the LOESS smoothing section, is another valid kernel function.

The kernel density estimator is

\[\hat f_n(x) = \frac 1n \sum_{i=1}^n \frac 1h K\left(\dfrac{x-X_i}{h} \right)\]

where \(h\) is the smoothing parameter or bandwidth. In Figure \ref{fig:kernel}, we see the effect of increasing the bandwidth \(h\): larger values result in smoother curves, while smaller values result in curves that follow the histogram more closely.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-32-1} 

}

\caption{\label{fig:kernel}}\label{fig:unnamed-chunk-32}
\end{figure}
\hypertarget{bounded-density-estimation}{%
\subsection{Bounded Density Estimation}\label{bounded-density-estimation}}

A question warranting investigation is the choice of kernel given we are working with a bounded variable -- the density we seek to estimate, \(f^{induced}\) is the density of \(P(S_0|\text{untested}, \text{test}_+)\) and hence is bounded between 0 and 1.

One way to handle density estimation for a bounded variable \(X\) is by performing a transformation
\(X=g(Y)\) and then using the change of variables for a probability density to obtain \(f_X(x)\) (Aurelien Pelissier, 2022). However, the transformation approach can be problematic due to boundary bias, where, as the name suggests, density estimates are highly biased at the boundaries of the support. This is particularly problematic when there is substantial density near the boundaries. More details on this are provided in the section \protect\hyperlink{bounded-density-transform}{Transformation Approach to Bounded Density Estimation}.

An alternative to the transformation approach for density estimation of bounded variables by using beta kernel estimators, which resolves the issue of boundary bias (S. X. Chen, 1999).

Although we can see the advantages of using the beta kernel estimator in the section \protect\hyperlink{betakernel}{Beta Kernel Density Estimation}, in practice, available implementations (e.g., the \texttt{bde} package implementation) are much more computationally expensive than using Gaussian kernel density estimation, which make the beta kernel estimator unfeasible for this work. The exact density of the induced distribution is not going to change the estimated counts dramatically relative to the other sources of variability (e.g.~the specification of the prior distributions or sources of data to inform these priors), and, as we see in Figure \ref{fig:gaus}, Gaussian density estimation does perform reasonably well for a variety of bounded distribution shapes.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/compare-gaussian} 

}

\caption{\label{fig:gaus}Comparing the performance of Gaussian density estimation for beta distributions of various shapes. The Gaussian density estimate is shown in teal, while the theoretical density is in red. For the shapes included here, the Gaussian density estimate is a reasonable approximation of the true density.}\label{fig:unnamed-chunk-34}
\end{figure}
\hypertarget{defpriors}{%
\chapter{Definition of Prior Distributions for Bias Parameters}\label{defpriors}}

\hypertarget{background-on-the-beta-distribution}{%
\section{Background on the Beta Distribution}\label{background-on-the-beta-distribution}}

The priors of the bias parameters we are specifying are probabilities, with the exception of \(\alpha\) and \(\beta\), which represent ratios of probabilities. To define a prior for a parameter that takes on values in \([0,1]\), a particularly useful distribution is the beta distribution, which is only defined on the interval \([0,1]\). It is parameterized by two positive values \(a, b\)\footnote{In R, \(a\) = shape1 and \(b\) = shape2.}.

The beta distribution is a flexible distribution\footnote{The beta distribution is also useful in Bayesian statistics, because it is the conjugate prior distribution for the binomial distribution and negative binomial distributions. That is, if we have a binomial likelihood with parameter \(p\) and \(p\) is distributed according to the beta distribution, the resulting posterior follows a beta distribution.}; by altering the parameters \(a,b\), we can get an extensive array of shapes, as seen in Figure \ref{fig:beta-flexible}.
\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{thesis_files/figure-latex/unnamed-chunk-36-1} 

}

\caption{\label{fig:beta-flexible}By changing the values of parameters of the parameters $a$ and $b$ of the beta distribution, we see that a variety of shapes are possible.}\label{fig:unnamed-chunk-36}
\end{figure}
In defining a beta distribution to reflect knowledge about a parameter, we need to work backwards to find the parameters \(a\) and \(b\) that correspond to the desired mean and variance.

There are multiple parameterizations of the beta distribution, but R uses that where we define the probability density function as

\[
f(x|a,b) = \dfrac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a - 1} (1-x)^{b - 1}.
\]
or equivalently as
\[
f(x|a,b) = \dfrac{1}{B(a,b)} x^{a - 1} (1-x)^{b - 1},
\]
where the beta function \(B(a, b) = \dfrac{\Gamma(a)\Gamma(b) }{\Gamma ( a + b)}\).

The expected value of the beta distribution is then \(\mu= \dfrac{a}{a + b}\) and the variance is given by \(\sigma^2 = \dfrac{a b}{(a + b)^2(a +b + 1)}\); the derivation for both is given in the appendix. We can then solve for \(a\) and \(b\) to obtain \(a = \Big(\frac{1-\mu}{\sigma^2} - \frac{1}{\mu}\Big) \mu^2\) and \(b = a\Big(\frac{1}{\mu} - 1\Big)\). At this point, we can easily write a function in R that generates the parameters of the beta distribution with the desired mean and variance, as we see below.
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_beta\_params }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sd) \{}
\NormalTok{    var }\OtherTok{=}\NormalTok{ sd}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{    a }\OtherTok{\textless{}{-}}\NormalTok{ ((}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ mu) }\SpecialCharTok{/}\NormalTok{ var }\SpecialCharTok{{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ mu) }\SpecialCharTok{*}\NormalTok{ mu }\SpecialCharTok{\^{}} \DecValTok{2}
\NormalTok{    b }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ mu }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ a,}
                \AttributeTok{b =}\NormalTok{ b))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{background-on-the-gamma-distribution}{%
\section{Background on the Gamma Distribution}\label{background-on-the-gamma-distribution}}

The gamma distribution is another very flexible distribution. However, the support of the gamma distribution is \((0,\infty)\) rather than \([0,1]\). Because some of the bias parameters are not probabilities (\(\alpha\) and \(\beta\) are ratios of probabilities), we can instead use the gamma distribution to allow the random variable to take on values over 1.

As with the beta distribution, we see in Figure \ref{fig:gamma-shapes} a variety of shapes are possible with the gamma distribution.
\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{thesis_files/figure-latex/unnamed-chunk-38-1} 

}

\caption{\label{fig:gamma-shapes}Altering the parameters for the shape and scale of the gamma distribution yields a variety of possible shapes of densities defined on $(0,\infty)$.}\label{fig:unnamed-chunk-38}
\end{figure}
Let \(k=shape\) and \(\theta=scale\). The parameterization of the gamma distribution that R uses is

\[f(x|k,\theta) = \frac{1}{\Gamma(k) \;\theta^k}x^{k-1} e^{x/\theta},\]
where the mean \(\mu =k\theta\) and the variance \(\sigma^2 = k\theta^2\).

This allows us to obtain \(\dfrac{\mu^2}{\sigma^2} = \dfrac{k^2 \theta^2}{k\theta^2} = k\).

We can then substitute \(\dfrac{\mu^2}{\sigma^2}\) in for \(k\) to give
\begin{align*}
\sigma^2 = k \theta^2 = \dfrac{\mu^2}{\sigma^2} \theta^2\\
\frac{\sigma^4}{\mu^2}=\theta^2\\
\frac{\sigma^2}{\mu}=\theta
\end{align*}
This allows us to calculate the shape and scale parameters of a gamma distribution with the desired mean and variance.

\hypertarget{definition-of-prior-distributions-for-incomplete-testing-correction}{%
\section{Definition of Prior Distributions for Incomplete Testing Correction}\label{definition-of-prior-distributions-for-incomplete-testing-correction}}

\hypertarget{defining-ps_1textuntested}{%
\subsection{\texorpdfstring{Defining \(P(S_1|\text{untested})\)}{Defining P(S\_1\textbar\textbackslash text\{untested\})}}\label{defining-ps_1textuntested}}

We recall that \(S_1\) denotes the event that an individual has moderate to severe symptoms, so
\(P(S_1|\text{untested})\) is the probability of having moderate to severe symptoms among those who were not tested. We note that this would include people that have moderate to severe COVID-like symptoms that do indeed have COVID-19 as well as people that do not have COVID-19 and have some other respiratory illness.

Wu et al. (2020) defined this distribution with a truncated beta distribution such that \(\Pr(S_1|\text{untested}) \sim \text{TBeta}(\alpha = 1.18, \beta = 45.97)\), bounded between 0 and 15\%. When we define the distribution of \(P(S_1|\text{untested})\), we release the truncation (Figure \ref{fig:prior-s-untested}) . Furthermore, we actually remove the truncation on all prior distributions considered. This choice was based on the fact that we have a lack of data to support particular bounds for a truncated distribution, and such assumptions can lead to difficulties in the Bayesian melding component of the probabilistic bias analysis.
\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-41-1} 

}

\caption{\label{fig:prior-s-untested}The specified prior for $Pr(S_1|\text{untested})$, where  $Pr(S_1|\text{untested}) \sim Beta(1.7,55)$, with a mean at 0.03 and standard deviation 0.0225.}\label{fig:unnamed-chunk-41}
\end{figure}
However, to implement this approach over a more extended time interval, we need to allow this parameter to vary by time. Due to state-specific differences in symptom prevalence, it also makes more sense to allow this parameter to vary by state.

To do this, we can use the COVID-like illness indicator from the COVID-19 Trends and Impact Survey (Salomon et al., 2021a). The COVID-19 Trends and Impact Survey (CTIS) is a large scale internet-based survey that invites a sample of Facebook users to respond to questions on several topics of public health interest, including testing and symptom status. The survey effort selects participants using stratified random sampling by state, and responses are aggregated and made publicly available.

\hypertarget{defining-alpha}{%
\subsection{\texorpdfstring{Defining \(\alpha\)}{Defining \textbackslash alpha}}\label{defining-alpha}}

We define \(\alpha\) as the ratio \(\dfrac{\Pr(\text{test}_+|S_1, \text{untested})}{\Pr(test+|\text{tested})}\), applied to allow
\(P(\text{test}_+|S_1, \text{untested})\) to vary by state. \(\Pr(\text{test}_+|\text{tested})\) is the state-level empirical estimate, but we do not calculate \(\alpha\) itself using the state-level empirical estimate. Instead, we calculate \(\Pr(\text{test}_+|S_1, \text{untested})\) as \(P(\text{test}_+|S_1, \text{untested}) =\alpha P(\text{test}_+|\text{tested})\). We can think about \(\alpha\) as the adjustment to the test positivity rate as we estimate the probability of testing positive among symptomatic \emph{untested} individuals. We assume this ratio is close to 1, that is, that the probability of testing positive among symptomatic untested individuals would be near 90\% of the probability of testing positive among tested individuals (not all of whom would be symptomatic).

With the the expansion of testing resources in 2021, it is plausible that \(P(\text{test}_+|\text{untested},S_1)\) could exceed \(P(\text{test}_+|\text{tested})\), so we use a gamma distribution to allow \(\alpha\) to take on values greater than 1.
\begin{center}\includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-44-1} \end{center}

\hypertarget{defining-beta}{%
\subsection{\texorpdfstring{Defining \(\beta\)}{Defining \textbackslash beta}}\label{defining-beta}}

Similar to the way we defined \(\alpha\), we define \(\beta\) as the ratio of \(\dfrac{P(\text{test}_+ |S_0, \text{untested})}{P(\text{test}_+|\text{tested})}\), applied to allow \(P(\text{test}_+ |S_0, \text{untested})\) to vary by state. We use \(\beta\) to calculate \(P(test+|S_1, \text{untested})\) by the expression \(P(\text{test}_+|S_0, \text{untested}) =\beta P(\text{test}_+|\text{tested})\). We can think about \(\beta\) as the adjustment to the test positivity rate as we estimate the probability of testing positive among aymptomatic untested individuals (in contrast to \(\alpha\), which is symptomatic untested individuals). We assume the values of \(\beta\) are substantially lower than \(\alpha\), reflecting that we expect a much smaller proportion of asymptomatic untested individuals to test positive than symptomatic individuals.
\begin{center}\includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-45-1} \end{center}

From the COVID-19 Trends and Impact Survey, we have the screening test positivity, defined as the estimated test positivity rate of people tested for COVID-19 who were being screened with no symptoms, as well as the overall test positivity.

Because we define \(\beta\) as the ratio of \(\frac{\Pr(\text{test}_+ |S_0, \text{untested})}{\Pr(\text{test}_+|\text{tested})}\), we can estimate \(\beta\) empirically by taking the screening test positivity rate as an estimate of \(\Pr(test + |S_0, untested)\) and then dividing by the overall test positivity rate \(\Pr(\text{test}_+|\text{tested})\). The screening test positivity and overall test positivity are available at the state level, but there are not estimates of screening test positivity for all states.

When the survey data is available, informing the prior for \(\beta\) by centering the distribution at the ratio of the screening test positivity to the overall test positivity allows us to obtain a time and state-specific estimate of \(\beta\).

\hypertarget{defining-ps_0texttest_textuntested}{%
\subsection{\texorpdfstring{Defining \(P(S_0|\text{test}_+,\text{untested})\)}{Defining P(S\_0\textbar\textbackslash text\{test\}\_+,\textbackslash text\{untested\})}}\label{defining-ps_0texttest_textuntested}}

\(P(S_0|\text{test}_+,\text{tested})\) is the probability of not having symptoms among those who test positive, that is, the asymptomatic rate of COVID-19. Although we can never obtain a true estimate of \(P(S_0|\text{test}_+,\text{untested})\) given it is by nature unobserved, we can look to studies that estimate \(P(S_0|\text{test}_+,\text{tested})\).

One large meta-analysis found \(P(S_0|\text{test}_+,\text{tested})\) to be 40.50\% (95\% CI: 33.50\%-47.50\%), although it did not restrict to screening studies (Ma et al., 2021a). Another meta-analysis, when restricting to screening studies, found \(P(S_0|\text{test}_+,\text{tested})\) to be 47.3\% (95\% CI: 34.0\% -61.0\%) (Sah et al., 2021a).

Of note, we use the prior \(P(S_0|\text{test}_+,\text{untested})\) in the Bayesian melding step of the analysis, but this prior is not one that we use to directly calculate the corrected counts.

Given we are considering an extended time interval with changing viral dynamics due to the emergence of new variants that change the severity of the disease, we leave the standard deviation on this prior distribution fairly wide.
\begin{center}\includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-46-1} \end{center}

\hypertarget{definition-of-priors-for-test-inaccuracy-correction}{%
\section{Definition of Priors for Test Inaccuracy Correction}\label{definition-of-priors-for-test-inaccuracy-correction}}

Test accuracy is broken up into two components, specificity and sensitivity. We define a prior for each.

\hypertarget{defining-test-sensitivity-s_e}{%
\subsection{\texorpdfstring{Defining Test Sensitivity (\(S_e\))}{Defining Test Sensitivity (S\_e)}}\label{defining-test-sensitivity-s_e}}

We define the prior distributions of the test sensitivity and test specificity to follow the definitions in Wu et al. (2020).

The test sensitivity is the probability an individual tests positive given that they are truly positive. Denoting the event of truly being positive as \(+\), we can write \(S_e = \Pr(\text{test}_+|+)\).

The test sensitivity \(S_e\) is defined by a truncated beta distribution such that \(S_e \sim \text{TBeta}(a= 4.20, b= 1.05)\), bounded between 0.65 and 1 and with mean \(\dfrac{a}{a+b} = 0.80\).
\begin{center}\includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-47-1} \end{center}

There are several studies to look to for informing the prior for PCR test sensitivity. In a population-based retrospective study including both inpatients and outpatients, Kortela et al. (2021) estimate the clinical sensitivity of the PCR test is 89.9\% (95\% CI 88.2 -- 92.1\%); their approach considers repeat-tested patients who initially tested negative but later tested positive. However, as Kortela \emph{et al.} discuss, this approach is likely an overestimate of the true clinical sensitivity, because individuals will only be tested twice if there is high clinical suspicion that they do have COVID-19. To account for this, they produced an estimate of sensitivity including cases with high clinical suspicion in the denominator, which resulted in an estimate closer to 50\%, yet this is likely an underestimate due to the fact that even those with a classical COVID-19 symptom presentation may not have COVID-19. They conclude that due to these biases, the true value most likely falls between the overestimate near 90\% and the underestimate near 50\%.

Another analysis of repeat-tested patients using data from a large sample of patients tested at the provincial Public Health Laboratory in Canada estimated the clinical sensitivity to be 90.7\% (95\% CI 82.6--98.9\%) (Kanji et al., 2021). Green et al. (2020) found that the clinical sensitivity ranged from 58\% to 96\%: the estimate of 96\% was dependent on the assumption that negative results, repeated or not, were true negatives, while the estimate of 58\% assumed the rate of false negatives among the repeat-tested population would be the same as in the repeat-tested patients. In a meta-analysis of 51 studies, Mustafa Hellou et al. (2021) estimated the clinical sensitivity to be 0.96 (95\% CI 93\% - 98\%).

Because PCR tests are designed to target a highly conserved region of the viral genome, their sensitivity is expected to be relatively robust to the circulation of different variants of SARS-CoV-2. However, analytical sensitivity has shown some differences by genetic variants (Y. Chen et al., 2022). Viral shedding dynamics also have differed by genetic variant, but the variants dominant throughout most of the time period considered here, Delta and Omicron, have similar viral loads (Fall et al., 2022; Singanayagam et al., 2022).
\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./figure/variant_plot} 

}

\caption{Variant proportions in the United States from genomic surveillance data collected by the CDC. Data is not available for time periods earlier than May 8, 2021.}\label{fig:unnamed-chunk-49}
\end{figure}
Ultimately, although it is plausible that test sensitivity may vary by time due to differences in viral shedding dynamics over time as well as differences due to the mutations present in circulating variants, there is a lack of data to inform exactly how the sensitivity may vary over time. As a result, we do not vary the specification of the prior for test sensitivity over time.

\hypertarget{defining-test-specificity-s_p}{%
\section{\texorpdfstring{Defining Test Specificity (\(S_p\))}{Defining Test Specificity (S\_p)}}\label{defining-test-specificity-s_p}}

The test specificity is the probability an individual tests negative given that they are truly negative. Denoting the event of testing negative as \$\text{test} - \$ and being truly negative as \(=\), we test specificity is \(S_p = P(\text{test} - | -)\).

We define the distribution with a truncated beta distribution such that
\(P(\text{test}-| -) \sim \text{TBeta}(a = 4998.50, \beta = 0.25)\), where this distribution is bounded between 0.9998 and 1 and with mean \(\dfrac{a}{a + b} = 0.99995\). The high certainty for this parameter is based on the CDC 2019-nCoV Real-Time RT-PCR Diagnostic Panel (Lu et al., 2020).
\begin{center}\includegraphics[width=0.8\linewidth]{thesis_files/figure-latex/unnamed-chunk-50-1} \end{center}

\hypertarget{exploration-of-the-implications-of-changes-in-the-bias-parameters}{%
\section{Exploration of the Implications of Changes in the Bias Parameters}\label{exploration-of-the-implications-of-changes-in-the-bias-parameters}}

To explore the implications of changes in the bias parameter distributions both on the melding and the final corrected estimates, we developed the Shiny app hosted \href{https://q-w-a.shinyapps.io/bayesian_melding_priors/}{here}.\footnote{Code is available at the repository \href{https://github.com/q-w-a/probabilistic_bias_correction}{here}}.

\hypertarget{correction-for-incomplete-testing}{%
\section{Correction for Incomplete Testing}\label{correction-for-incomplete-testing}}

We denote \(N^*\) to be positive tests (not infections).

As discussed previously, once we have sampled values from the constrained distributions of \(P(S_1|\text{untested})\), \(\alpha\), \(\beta\), we estimate the test positivity among the symptomatic untested population as \(P(\text{test}_+|S_1,\text{untested}) = \alpha \; P(\text{test}_+|\text{tested})\), and we estimate the test positivity among the asymptomatic untested population as \(P(\text{test}_+|S_0,\text{untested}) = \beta \; P(\text{test}_+|\text{tested})\). Then, we compute the positives among the symptomatic and mild/asymptomatic parts of the population respectively as

\[N^*_{\text{untested},S_1} = N_{\text{untested}} \; P(S_1|\text{untested}) \cdot P(\text{test}_+ | S_1,\text{untested}) \;\;\;\text{ and }\]
\[N^*_{\text{untested},S_0} = N_{\text{untested}}(1-P(S_1|\text{untested}))P(test + | S_0,\text{untested}).\]
Then, we take the total positives among the untested population as

\[N^*_{\text{untested}} = N^*_{\text{untested},S_1} + N^*_{\text{untested},S_0}\]
and finally we add the number of observed positives, \(N^*_{tested}\) to obtain the estimate for the positives among the total population, as

\[N^* = N^*_{\text{tested}} +N^*_{\text{untested}}.\]

\hypertarget{correct-test-inaccuracy}{%
\section{Correction for Diagnostic Test Inaccuracy}\label{correct-test-inaccuracy}}

At this point, we have corrected for the incompleteness of testing. That is, we have an estimate of the total number who would test positive if we tested the entire population, denoted \(N^*\). However, we also need to correct for the imperfect test accuracy of the diagnostic test.

Then, given that we have the number \(N^*\) who tested positive (or, in the context of our work, would have tested positive), the specificity \(S_p\), the sensitivity \(S_e\), and the total population size \(N\), we can calculate the true positives with the formula
\[\text{Number Truly Positive} = \dfrac{N^* - (1-S_p) \times N}{S_e+S_p-1}\]
as defined in (Rothman et al., 2008).

It is possible that this formula will yield a negative value when \(N^* < (1-S_p) N\). In the context of our implementation, this only occurs if \(N^*\) is zero, which does happen in some small counties with low testing rates. To handle this, if the value was negative, we took it to be zero.

\hypertarget{derivation-of-formula-for-correction-for-diagnostic-test-inaccuracy}{%
\subsection{Derivation of Formula for Correction for Diagnostic Test Inaccuracy}\label{derivation-of-formula-for-correction-for-diagnostic-test-inaccuracy}}

We define test sensitivity and specificity as follows:
* \(S_e\) = test sensitivity = the probability an individual tests positive if they have COVID-19 (probability of a true positive), that is, \(P(\text{test}_+ | +)\).
* \(S_p\) = test specificity = probability an individual tests negative if they do not have COVID-19 (probability of a true negative), that is, \(P(\text{test}- |-)\).

As defined previously, both are defined as by truncated beta distributions.

To correct case counts for diagnostic test inaccuracy, we use the formula
\[\text{Number Truly Positive} = \dfrac{N^*- (1-S_p) \times N}{S_e+S_p-1}. \tag{1}\]

\vspace{5 mm}

To obtain this formula, we let:
\begin{itemize}
\tightlist
\item
  \(N\) denote the total population size
\item
  \(N^*\) denote the number \emph{classified} as positive
\item
  \(N^-\) denote the number \emph{classified} as negative
\item
  \(T^+\) denote the number that is \emph{truly} positive
\item
  \(T^-\) denote the number that is \emph{truly} positive.
\end{itemize}
With this notation, \(T^+\) is the left hand side of (1).

We also recall that
\[ \text{Sensitivity} = S_e = \Pr(\text{test}_+ | +) \]
\[ \text{Specificity} = S_p = \Pr(\text{test}- | - ) .\]

\noindent The quantity we want to estimate is the number of truly positive individuals when accounting for imperfect test accuracy, that is, \(T^+\).

\noindent The number classified as positive, \(N^*\) can be written as

\[ N^* = \Pr(\text{test}_+ | +) T^+ + \Pr(\text{test}_+  | -) T^-\]

\noindent where \(\Pr(\text{test}_+ | +) T^+\) is the number of true positives and \(\Pr(\text{test}_+ | -) N^-\) is the number of false positives. By the definitions of sensitivity \(S_e\) and specificity \(S_p\), we can write this more concisely as
\[ N^* =S_e T^+ + (1-S_p) T^-.\]

\noindent Meanwhile, the number classified as negative, \(N^-\) can be written as

\[ N^- = \Pr(\text{test} - | -) T^- + \Pr(\text{test} - | +) T^+\]

\noindent where \(\Pr(\text{test} - | -) T^-\) is the number of true negatives and \(\Pr(\text{test}- | +) T^+\) is the number of false negatives. Substituting in \(S_e\) and \(S_p\) we can express this as

\[ N^- = S_p T^- + (1-S_e) T^+.\]

\noindent At this point, we can solve the expression \(N^- = S_p T^- + (1-S_e) T^+\) for the number of people classified as positive for the number truly negative, \(T^-\). This yields

\[\dfrac{( N^- - (1-S_e) T^+) }{S_p}=  T^- .\]

\noindent Now, we can substitute this result into our expression for

\(N^+ =S_e T^+ + (1-S_p) T^-\) and solve for the desired value, the number of truly positive individuals, \(T^+\). This gives us
\begin{align*}
 N^* &=S_e T^+ + (1-S_p)  \left( \dfrac{( N^- - (1-S_e) T^+) }{S_p} \right)
\\
 S_p \; N^* &=S_pS_e T^+ + (1-S_p)  \left( {( N^- - (1-S_e) T^+) } \right)
\\
 S_p \; N^* &=S_pS_e T^+ + (1-S_p)  ( N^-)  - (1-S_p)(1-S_e) T^+
\\
 S_p \; N^* -   (1-S_p)  ( N^-) &=S_pS_e T^+  - (1-S_p)(1-S_e) T^+
\\
 S_p N^* -   (1-S_p)  ( N^-) &= (S_pS_e  - (1-S_p)(1-S_e)) T^+
\\
 S_p \; N^* -   (1-S_p)  ( N^-) &= (S_p + S_e - 1) T^+
\\
 T^+ &= \dfrac{ S_p \; N^* -   (1-S_p)  ( N^-)}{(S_p + S_e - 1)} .
\end{align*}
At this point, we can simplify the numerator as follows by using the fact
\(N =N^* + N^-\). This gives us
\begin{align*} &=S_p \;N^* -   (1-S_p)  N^-\\
&=  S_p   N^* + S_p  N^- - N^-\\
&=  S_p(N^* +  N^-) - N^- \\
&=  S_pN - (N-N^*) \\
&=  S_pN - (N-N^*) \\
&=  (S_p-1)N + N^* \\
&=  N^* - (1-S_p)N,\\
\end{align*}
so we have
\[
 T^+ = \dfrac{ N^*- (1-S_p)N}{(S_p + S_e - 1)}.
\]

That is, if we have the number of positive tests \(N^*\), the specificity, and the sensitivity, we can use this formula to estimate the number of true positives in the population.

In this context, from the correction for incomplete testing, \(N^*\) is the estimate of the total number who would test positive if the whole population were tested. Then, using the defined distributions for \(S_p\) and \(S_e\), we can apply this formula to adjust for inaccuracy of the diagnostic test.

\hypertarget{details-of-implementation}{%
\chapter{Details of Implementation}\label{details-of-implementation}}

To see the impact of informing prior parameters with data from the COVID-19 Trends and Impact Survey, we include four implementations, summarized in Table \ref{fig:priors-versions-table}.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/priors_implementations} \caption{\label{fig:priors-versions-table}Description of each implementation of probabilistic bias analysis. Version 1 does not inform priors based on the COVID-19 Trends and Impact Survey, while versions 2-4 do utilize the survey data.}\label{fig:unnamed-chunk-53}
\end{figure}
To make the implementation of each more concrete, we will illustrate the steps of the probabilistic bias analysis with the infections in Suffolk County, Massachusetts in from May 9, 2021 to May 23, 2021.

\newpage

\hypertarget{version-1-priors-do-not-vary-by-state-or-date}{%
\section{Version 1: Priors do Not Vary by State or Date}\label{version-1-priors-do-not-vary-by-state-or-date}}

As we did in the \protect\hyperlink{sampling}{Background} section, we denote the sample size as \(m\) and the posterior (resample) size as \(r\).

We begin by sampling \(m=10^6\) times from the defined prior distributions \(\theta = \Big\{ \alpha, \beta, \Pr(S_1| \text{untested})\Big\}\), as we see in Figure \ref{fig:sample-priors}.
\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-55-1} \caption{\label{fig:sample-priors}As the first step of the probabilistic bias analysis, we sample $m$ observations from the defined priors  $\theta = \{ \alpha, \beta, \Pr(S_1| \text{untested})\}$.}\label{fig:unnamed-chunk-55}
\end{figure}
At this point, we use Bayesian melding to obtain \(r=10^5\) observations from the constrained priors for \(\alpha, \; \beta, \;\Pr(S_1|\text{untested})\), and \(\Pr(S_0|\text{test}_+,\text{untested})\), which we see in the first two rows of Figure \ref{fig:melded-all-together}. The melded distributions of \(\alpha, \; \beta, \;\Pr(S_1|\text{untested})\), and \(\Pr(S_0|\text{test}_+,\text{untested})\) are the inputs to the probabilistic bias analysis.

For use in the test inaccuracy correction step, we also sample \(r=10^5\) values from the priors for sensitivity and specificity. However, these parameters are not involved in the Bayesian melding step.
\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-56-1} \caption{\label{fig:melded-all-together}The full set of priors used in the probabilistic bias analysis. The first row is our sampled values of $  heta$, before and after melding. The post-melding distributions are what we use as inputs to the probabilistic bias analysis. The second row is $\Pr(S_0|\text{test}_+, \text{untested})$, which is a part of the melding process that enables us to incorporate information on the asymptomatic rate to inform the definitions of $\theta$; however, we do not use the parameter $\Pr(S_0|\text{test}_+, \text{untested})$ directly in the bias correction; its only role is in the melding step.}\label{fig:unnamed-chunk-56}
\end{figure}
From here, we obtain the bias corrected estimates by sampling from the \(r\) observations of the bias parameters and computing the unobserved infections.

For example, suppose we sample a value from each of \(\beta,\alpha,\Pr(S_1|\text{untested}), S_p\) and \(S_e\).

We first compute the number of untested individuals as the population size \(N\) minus the number tested:

\[ N_{\text{untested}} =  N - N_{\text{tested}} .\]

We then use \(\alpha\) and \(\beta\) to estimate the test positivity rates of the symptomatic and asymptomatic partitions of the population, giving us the positivity rates
\[ \Pr( \text{test}_+ | S_1, \text{untested})  = \Pr( \text{test}_+ | \text{tested}) \cdot  \alpha  \]
and
\[ \Pr( \text{test}_+ | S_0, \text{untested})  = \Pr( \text{test}_+ | \text{tested}) \cdot  \beta. \]

We use these positivity rates to calculate the number of positive tests among the untested as
\begin{align*}
N^*_{\text{untested},S_1} &= \Pr( \text{test}_+ | S_1, \text{untested}) ( N_{S_1,\text{untested}}) \\
&= \Pr( \text{test}_+ | S_1, \text{untested}) (N_{\text{untested}} ) \Pr(S_1|\text{untested})\\
N^*_{\text{untested},S_0} &= \Pr( \text{test}_+ | S_0, \text{untested}) ( N_{S_0,\text{untested}})\\
&= \Pr( \text{test}_+ | S_1, \text{untested}) (N_{\text{untested}} ) (1- \Pr(S_1|\text{untested}))
\end{align*}
We use \(N^*\) to denote the number of people who would test positive, not the number who are infected. This distinction is important because we are not yet correcting for diagnostic test inaccuracy. With this notation, \(N^*_{\text{untested}}\) is the number of people among the untested population we expect to test positive if everyone were tested, while \(N^*_{\text{tested}}\) is simply the number of positive tests reported for this 2-week interval. This means we can write the total number of individuals who we expect to test positive as
\begin{align*}
 N^*_{\text{untested}}
 &= N^*_{\text{tested}} + N^*_{\text{untested}} \\
 &= N^*_{\text{tested}} + N^*_{\text{untested}, S_0} + N^*_{\text{untested}, S_1}.
\end{align*}
To estimate the infections \(N^+\), rather than positive tests \(N^*\), we must correct for diagnostic test inaccuracy, which we can do by applying the formula for correcting for diagnostic test inaccuracy discussed \protect\hyperlink{correct-test-inaccuracy}{previously}.

That is, we take the sampled value of sensitivity \(S_e\) and specificty \(S_p\) and compute

\[N^+ = \dfrac{(N^* - (1 - S_p) N) } { (S_e + S_p - 1) }.\]

Repeating this process with sampled values of the bias parameters gives us a distribution for the corrected counts, as we see in Figure \ref{fig:correctedsuffolk}. The 95\% simulation intervals presented in the \protect\hyperlink{res}{Results} section are the 2.5\% and 97.5\% percentiles for each geographic unit and 2-week time interval, as we see in Figure \{fig:corrected-suffolk-over-time\}.
\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-57-1} \caption{\label{fig:correctedsuffolk}Here, we consider Suffolk County in a specific two-week interval. By sampling from the bias parameters, melding, correcting for incomplete testing, and then correcting for test inaccuracy, we obtain a distribution of estimated infections for the two-week interval. We can summarize this distribution with a 95\% simulation interval.}\label{fig:unnamed-chunk-57}
\end{figure}
\begin{figure}
\includegraphics[width=1\linewidth]{../presentation/figure/suffolk_biweek_10} \caption{\label{fig:corrected-suffolk-over-time}Here, we show how we visualize the 95\% simulation intervals like the one shown in \ref{fig:correctedsuffolk}, which is highlighted in red. Presenting these intervals over time for a given location is often how we present the results of the probabilistic bias analysis.}\label{fig:unnamed-chunk-58}
\end{figure}
\hypertarget{version-2-4-allowing-some-prior-parameters-to-vary}{%
\section{Version 2-4: Allowing Some Prior Parameters to Vary}\label{version-2-4-allowing-some-prior-parameters-to-vary}}

The implementation for versions 2-4 when considering a single time and geographic unit are almost identical to version 1; the difference is in the source of the priors.

For one version, we allow \(\beta\) to vary by date and state by using the COVID-19 Trends and Impact Survey (Salomon et al., 2021b). Since \(\beta\) represents the ratio of \(\dfrac{\Pr(\text{test}_+|\text{untested},S_0)}{\Pr(\text{test}_+|\text{tested})}\), one way we can think about estimating this quantity empirically is as the screening test positivity over the overall test positivity. Since both these quantities are reported by the COVID-19 Trends and Impact Survey, we can center the distribution for \(\beta\) for a given location and time interval at this ratio, after performing LOESS smoothing. That is, we use the same standard deviation assumed in the first version, where we do not allow priors to vary by state or date, but we set the mean to be the empirical estimate of beta.

For another version, we allow \(\Pr(S_1|\text{untested})\) to vary by date and state by centering the distribution for \(\Pr(S_1|\text{untested})\) at the percentage of COVID-19-like illness reported by the COVID-19 Trends and Impact Survey, since this variable reflects the presents of symptoms related to COVID-19 among the general population.

Lastly, in one version we center both the distribution for \(\Pr(S_1|\text{untested})\) and \(\beta\) at their empirical values.

While we recognize taking values of the survey may in itself introduce biases, in the United States, there is little alternative to inform the priors for \(\beta\) or \(\Pr(S_1|\text{untested})\), particularly at a fine geographic scale. Some countries, for example, the United Kingdom, conducted PCR tests and recorded symptoms for a random sample of the population to estimate community prevalence of COVID-19 (Riley et al., 2021). A study like this would be able to estimate the test positivity rate among the untested population directly (and the asymptomatic or symptomatic positivity rates as well) rather than defining random variables \(\alpha\) and \(\beta\) to correct the observed test positivity rate.

In this sense, looking at the empirical estimates of \(\Pr(S_1|\text{untested})\) and \(\beta\) using the COVID-19 Trends and Impact Survey can provide a useful baseline for plausible values these parameters can take on, and allows us to see how the bias-corrected estimates differ depending on the data used to inform the priors.

\hypertarget{comparison-to-the-covidestim-model}{%
\chapter{Comparison to the Covidestim Model}\label{comparison-to-the-covidestim-model}}

\hypertarget{overview-3}{%
\section{Overview}\label{overview-3}}

One challenge in correcting for biases in general is that although we may have some information about the influence of possible biases, we do not have a ground truth for comparison. However, one approach to handle the fact that the true cases are unobserved is comparing our estimates to those from other approaches seeking to estimate a similar quantity. In particular, if other approaches make different assumptions and come to a similar result, this can give us more confidence in our estimates.

One notable project seeking to estimate the true infection burden at the county-level over time is the Covidestim project. In this work, Chitwood et al.~proposed a mechanistic model that includes states for asymptomatic/pre-symptomatic infection, symptomatic but mild infection, severe COVID-19 presentations, and death. This approach also enables the estimation of \(R_t\), the number of secondary infections a single infected individual causes at time \(t\). This is a useful quantity to estimate, but is sensitive to reporting delays and changes in testing practices (Pitzer et al., 2021).

\hypertarget{the-covidestim-model}{%
\section{The Covidestim Model}\label{the-covidestim-model}}

Chitwood \emph{et al.} propose a Bayesian evidence synthesis model to correct for reporting delays and time varying case ascertainment testing rate in the estimation of incident infections and \(R_t\).

To estimate the expected cases and deaths at a particular point in time, the model uses a convolution of the time series of observed cases and deaths and reporting delay distributions that are specific to the health state categories. This enables the model to account for the fact that reporting delay is different depending on the severity of an infection; for instance, someone with severe COVID-19 may be diagnosed more quickly than someone with a mild infection.

For any health state, for example, asymptomatic, the individual can either transition to the next health state (symptomatic) or recover. Thus, with each transition between a defined health state, for example, asymptomatic, there is a probability of transitioning to the next health state (in this case, asymptomatic to symptomatic); the complement of this probability is the probability of recovery.

Each of these transitions is defined by a delay distribution. For example, the distribution for moving from asymptomatic to symptomatic represents the probability an individual moves to the symptomatic state at a point in time. The probabilities asymptomatic to symptomatic and symptomatic to severe are modeled as not varying with time. Meanwhile, the probability of transitioning from severe to death was defined to be higher in 2020 due to higher case fatalities early in the pandemic. The infection fatality rates, adjusted to be specific to a given state or county based on age distributions and the prevalence of risk factors for COVID-19, are used to inform the probability of moving from the severe category to the death category.

The change in daily infections from the previous day (i.e., the new infections) is calculated as a function of the estimated effective reproductive number \(R_t\) and the mean serial interval, where serial interval is the time from the onset of infection of a primary case to the time of onset of infection in the secondary case. \(R_t\) is estimated using a log-transformed cubic spline, under the assumption individuals can only be infected once.

They also defined a distribution for the delay to diagnosis, which was distinct by health state category to reflect differences in diagnosis delays that occur depending on the disease severity.
The probability of diagnosis among different health states was allowed to vary by time to reflect changing testing rates throughout the pandemic.

A separate distribution models the reporting delay to correct the total number of diagnoses on a given day for the fact that these diagnoses correspond to past infections.

The observed cases and death data for each state to the model were fitted using negative binomial likelihood functions.

\hypertarget{assumptions}{%
\section{Assumptions}\label{assumptions}}

This approach relies on infection fatality ratios and death counts to estimate the true case counts. Thus, it is sensitive to estimates of infection fatality rate, with higher infection fatality ratio estimates resulting in lower estimated infections. The infection fatality ratio is defined as the proportion of COVID-19 infections that lead to death, which means there is uncertainty in estimating both the numerator and the denominator of the ratio. The true cumulative incidence depends on the same uncertainties in estimating the true case burden at any point in time. Estimating the infection fatality ratio itself is a challenging task.

The Covidestim model uses age-specific estimates of the infection fatality ratio (IFR)\footnote{The infection fatality ratio of an infectious disease is the proportion of those infected who die.} produced by O'Driscoll et al. (2021). They used national-level age-stratified, and when possible sex-stratified, COVID-19 death counts and cumulative infection estimates from seroprevalence studies. Of note, the estimates of infection fatality ratio are assumed to be constant over time, which may not be the case due to improving treatments (e.g., Paxlovid), different variants leading to less severe presentations, or changes in the demographics of individuals being infected. However, reliable estimates of infection fatality ratio that vary with time difficult to acquire; Covidestim assumed a higher case fatality in 2020 given the novelty of the virus and consequent lack of available treatments.

\hypertarget{comparison-to-serological-data}{%
\section{Comparison to Serological Data}\label{comparison-to-serological-data}}

There are known issues with seroprevalence estimates. For one, these samples are drawn from a convenience (i.e.~nonrandom) sample of individuals with blood specimens taken for purposes other than COVID-19 antibody detection ({``Commercial {Laboratory Seroprevalence Surveys} \textbar{} {Coronavirus} \textbar{} {COVID-19} \textbar{} {CDC},''} n.d.). Secondly, while a positive serological test is evidence for infection, a negative serological test is less clear to interpret. The person may have been infected but not yet have developed antibodies, or their immune system may not have produced antibodies at a detectable level (CDC, 2020).

Indeed, Chitwood et al.~found limited concordance between their estimates and seroprevalence data. However, there was a stronger correlation between estimates of cumulative infection and cumulative hospitalizations and cumulative deaths \footnote{The correlation employed here is the Spearman rank correlation, which measures the strength of the monotonic relationship rather than the strength of the linear relationship, in which case the Pearson correlation coefficient is the usual choice. The Spearman rank correlation is equivalent to the Pearson correlation of the rank values rather than the values themselves. }.

\hypertarget{lims}{%
\subsection{Limitations of this Comparison}\label{lims}}

At this point in the pandemic, there is no true gold standard to compare to. Covidestim is one model, among many, that makes key assumptions about aspects of the virus. Another note is that estimates from the Covidestim model are reported on the daily timescale for counties, while the probabilistic approach we implemented here is at the biweekly time scale.

To ensure the comparisons are on the same time scale, we sum the reported 95\% credible intervals for the days in each 2-week interval. These intervals do not represent a 95\% credible interval for the 2-week interval, and while such an interval would be ideal for the comparison, computation of a 95\% credible interval for the two-week interval is not feasible because of the model structure. Due to the correlation between observations for each day for a given location, summing the intervals yields an estimate that is likely to be more conservative than a true 95\% credible interval for the two-week interval would be. More detail on this assumption is in \protect\hyperlink{conservativeintervals}{the appendix}.

\hypertarget{res}{%
\chapter{Results}\label{res}}

\hypertarget{state-level-results}{%
\section{State-level Results}\label{state-level-results}}

There is much more comprehensive testing data at the state-level than at the county level, as John Hopkins has tracked state-level testing data throughout the pandemic.
As a result, we can apply probabilistic bias analysis across the United States at the state-level.

However, since versions 2-4 of the analysis utilize empirical estimates of \(P(S_1|\text{untested})\) or \(\beta\) from the COVID-19 Trends and Impact Survey, these versions are only possible when there is sufficient data for the state. As we see in Figure \ref{fig:statectis}, some states have very little data for the empirical estimate of beta. However in Figure \ref{fig:statectis-s-untested} we see that there is more consistent reporting of the prevalence of COVID-19-like symptoms, which we can use as an estimate of \(P(S_1|\text{untested})\).

For implementations 2-4 of the probabilistic bias analysis (refer to Table \ref{fig:priors-versions-table} for details on each), we excluded states where more than 40\% of observations for that state in the survey data were missing at the daily time step. For states retained, missing values for states with sufficient data were imputed before summarizing to the two-week level using a linear weighted moving average.

\vspace{5 cm}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ctis_beta_states} \caption{\label{fig:statectis}The screening test positivity over the overall test positivity from the COVID-19 Trends and Impact Survey, by state. For states where less than 40\% of observations were missing at the daily time step, the LOESS smoothed line is shown in red.}\label{fig:unnamed-chunk-60}
\end{figure}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ctis_s_untested_states} \caption{\label{fig:statectis-s-untested} The percent of the population experiencing COVID-19-like illness from the COVID-19 Trends and Impact Survey, by state. The LOESS smoothed line is shown in red. For the implementations of probabilistic bias analysis where we use this indicator to inform $P(S_1|\text{untested})$, we center the distribution of $P(S_1|\text{untested})$ at the smoothed value from the survey. We note that there is less missing data for this indicator than for $\beta$ (Figure \ref{fig:statectis}), for which we need both the screening positivity and overall test positivity.}\label{fig:unnamed-chunk-61}
\end{figure}
In Figures \ref{fig:state-results-1} and \ref{fig:state-results-2}, we compare the 95\% credible intervals of Covidestim summed to be on the biweek time scale to the probabilistic bias analysis intervals\footnote{As mentioned in the \protect\hyperlink{lims}{limitations} section, the two week intervals cannot be interpreted as 95\% credible intervals. However, the comparison is still useful to contextualize the results of the probabilistic bias analysis.}. In these figures, we only consider the states were there was enough survey data to do all four of the implementations. However, since testing data is availlable for all states, the first implementation, which does not rely on survey data, was possible for all states in the U.S. (Supplementary Figure \ref{fig:all-states-first-impl}).

We see correspondence is much higher before the time period spanning December 2021 through January 2022, where Covidestim intervals tend to be higher than the probabilistic bias intervals. From Figure \ref{fig:variant-prop}, we see that this period corresponds to the Omicron wave of the pandemic.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/state_comp_covidestim1} \caption{\label{fig:state-results-1}95\% simulation intervals for each state and two-week interval. Only states with all four versions are included; this figure represents the first half of states with all four versions, and \ref{fig:state-results-2} represents thee second half. Each column represents a distinct implementation of the probabilistic bias analysis. The first is where priors do not vary by state or location; that is, priors are specified once and these same priors are used for all states and time inteervals considered. The second implementation centers the distribution of $\beta$ at the ratio of screening test positivity to overall test positivity from the COVID-19 Trends and Impact Survey. The third implementation centers the distribution of $P(S_1|\text{untested})$ at the percent of the population experiencing COVID-19-like illness. The fourth implementation centers both $\beta$ and $P(S_1|\text{untested})$ at the survey values.}\label{fig:unnamed-chunk-62}
\end{figure}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/state_comp_covidestim2} \caption{\label{fig:state-results-2} As in \ref{fig:state-results-1}, this figure contains the probabilistic bias analysis intervals for states where all four implementations were possible for the second half of states (where \ref{fig:state-results-1} corresonds to the first half). More detail on each column is discussed in \ref{fig:state-results-1}.}\label{fig:unnamed-chunk-63}
\end{figure}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/variant_prop} \caption{\label{fig:variant-prop}Variant proportions for variants designated as Variant of Interest or Variant of Concern by the World Health Organization. We see Delta begins its rise in May 2021 and reaches an estimated proportion of 0.98 of sequenced infections by mid-August 2021. Omicron's rise occurs more quickly, beginning its increase throughout December and becoming almost entirely dominant by mid-January.
Data source: SARS-CoV-2 Variant Proportions from Data.CDC.gov.}\label{fig:unnamed-chunk-64}
\end{figure}
This period of disagreement between the approaches in December 2021 through January 2022 corresponds to the Omicron wave of the pandemic, which we can see in Figure \ref{fig:variant-prop}. This was a major shift in the pandemic: the Omicron variant is highly transmissible compared to previous variants, and it has immune invasion capacity, which means vaccines provides less protection against infection (Andrews et al., 2022). Omicron also is associated with a lower infection fatality ratio (Liu, Yu, Zhao, \& He, 2022), as well as milder infection overall, which may influence testing behavior. Omicron's rise was rapid too, presumably as a result of this enhanced transmissibility and immune escape; it became the dominant variant over the course of a single month.

Beyond the clear differences during the Omicron wave, we also see in several states that there is an additional rise in July with the probabilistic bias analysis implementations that include data from the COVID-19 Trends and Impact Survey (columns 2 through 4), while this peak is smaller in the Covidestim intervals. States exhibiting this trend include Arizona, Maryland, Michigan, Minnesota, Missouri, New Jersey, New York, and Wisconsin. Meanwhile, there are some states where there is a peak of similar size in the Covidestim intervals that lags the peak in the probabilistic bias analysis, for example, Colorado, Kentucky, Texas, and Virginia.

This peak beginning close to July corresponds to the Delta wave, where the more transmissible B.1.617.2 (Delta) variant of concern displaced the previously dominant B.1.351 (Alpha) variant (Del Rio, Malani, \& Omer, 2021).

To better understand the differences between the probabilistic bias analysis and Covidestim during the Delta wave in the summer of 2021, we consider two examples that represent different trends.

First, we can take a closer look at Michigan to see the trends in the probabilistic intervals in relation to changes in circulating variants (Figure \ref{fig:michigan-variant}). We see, as we observe across most states in Figure \ref{fig:state-results-1} and Figure \ref{fig:state-results-2}, that the times when the probabilistic bias intervals differ the most from Covidestim are in the increases around late July and in December, where we see that July corresponds to the Delta Wave and December corresponds to the Omicron wave.

\hypertarget{example-where-july-peak-is-larger-in-probablistic-bias-analysis-intervals-michigan}{%
\subsection{Example Where July Peak is Larger in Probablistic Bias Analysis Intervals: Michigan}\label{example-where-july-peak-is-larger-in-probablistic-bias-analysis-intervals-michigan}}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/michigan_variant} \caption{\label{fig:michigan-variant}Probabilistic intervals for Michigan along with the variant proportions for variants designated as Variant of Interest or Variant of Concern by the World Health Organization. These variant proportions are not specific to Michigan; the estimates are for the United States. The rise in Omicron corresponds to the dramatic increase in both the probabilistic bias intervals and Covidestim estimates in December of 2021 through January of 2022. We also see the increase in the Delta variant that precedes the Delta wave in the summer of 2021.}\label{fig:unnamed-chunk-65}
\end{figure}
As discussed in the Covidestim model change log, Chitwood \emph{et al.} made substantial changes to handle the rise of the Omicron variant. Because the variant causes much milder infections, the infection fatality for Omicron infections is lower than previous variants. To handle the changes in the infection fatality ratio and very low death counts, rather than fitting model with deaths, they switched to using hospitalizations. They also allowed for the possibility of reinfections, since although reinfections were more rare with previous variants, Omicron is associated with higher reinfection rates (Pulliam et al., 2022). The changes in the model may contribute to the differences we see between the probabilistic bias intervals and Covidestim intervals during the Omicron wave.

In July, meanwhile, the difference we see between the Covidestim estimates and probabilistic bias intervals is likely a result of the way these approaches treat incomplete testing. The focus of the probabilistic bias analysis is to correct for incomplete testing, and as such the method is sensitive to changes in the total number tested and the positivity rate. By contrast, while Covidestim models probabilities of diagnosis by symptom state to allow for variation in case ascertainment, the total number of tests is not an input into the model, so model estimates are not affected by changes in testing rate. This is particularly relevant in the time interval where we observe a larger peak in the probabilistic bias intervals. While the trend in observed tests follows a more steady upward trend like that we see in the Covidestim estimates, the probabilistic bias intervals are influenced by the relationship between positive tests and total tests in this time period.

In Figure \ref{fig:test-capacity}, we consider the number of positive and total tests reported in Michigan. In the period from July through September, the positive tests increase at a higher rate than the total tests. Because the number of infections among the untested population is calculated based on the observed test positivity rate, the higher test positivity rate (resulting from positive tests increasing more than total tests) results in the peak we see in this time interval in the probabilistic bias intervals. A similar trend appears across several states, where the positive tests increase more rapidly than the total tests during this time period (Supplementary Figure \ref{fig:test-capacity-all-states}).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/test_capacity} \caption{\label{fig:test-capacity} Percent change in positive tests and the percent change in total tests from March 2021 through the end of February 2022. We see in the period from July through September that the positive tests increase at a higher rate than the total tests, which relates to the increase we see beginning in July in the probabilistic bias intervals.}\label{fig:unnamed-chunk-66}
\end{figure}
\hypertarget{example-where-covidestim-intervals-lag-probabilistic-bias-intervals-texas}{%
\subsection{Example Where Covidestim Intervals Lag Probabilistic Bias Intervals: Texas}\label{example-where-covidestim-intervals-lag-probabilistic-bias-intervals-texas}}

Texas is one state where there the peak corresponding to the Delta wave is lagged in the Covidestim intervals compared to the probabilistic bias intervals. In Figure \ref{fig:texas}, we narrow in on two implementations of the probabilistic bias analysis: the one that does not incorporate survey data to inform the priors, and the implementation that centers \(\beta\) and \(\Pr(S_1|\text{untested})\) at the values from the survey data. The presence of the lag in both versions indicates that it is not the survey data that is causing this lag. From Figures \ref{fig:state-results-1} and \ref{fig:state-results-2}, this is often the case across the U.S.: when there is a lag, it is generally present in all implementations, but size of the increase may be different for each implementation.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/texas-lag} \caption{\label{fig:texas} Probabilistic bias analysis intervals for Texas across time. Each probabilistic bias interval is a 95\% simulation interval for the estimated infections for that entire 2-week period. In red are Covidestim estimates summed to be on the same biweekly time scale. Texas is one state where the Covidestim estimates appear to lag the probabilistic bias analysis for the Delta wave beginning in July. This is true for both the version that does not incorporate survey data from the COVID-19 Trends and Impact Survey, as well as the version that does, so the difference cannot be explained by the survey data alone.}\label{fig:unnamed-chunk-67}
\end{figure}
To consider the reasons we see this lag, we can first compare the data sources. For the probabilistic bias analysis we use the CDC's reports of positive and total tests by state, while the Covidestim model uses the Johns Hopkins Center for Systems Science and Engineering (CSSE) case counts.

We see that the CDC positive test counts are very similar to the Johns Hopkins CSSE case counts, so the difference in data sources does not appear to be driving the lag. However, the test positivity, calculated as the total number of positive tests over total tests for each 2-week interval, begins to increase in early June, before we see positive tests or cases increasing, due to how total tests are changing relative to positive tests (Supplementary Figure \ref{fig:test-capacity-all-states}).

Since the probabilistic bias analysis uses the observed test positivity to estimate the (hypothetical) test positivity in the untested population, increases in test positivity lead to the increases we see in the bias intervals.

This means that if positive tests are steady but total tests are decreasing (as is the case in this particular example in mid June), we will see an increase in the probabilistic bias estimates due how we use test positivity to estimate the number of unobserved infections.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/texas-data-sources} \caption{\label{fig:texas-source} Comparing the data sources for the probablistic bias analysis and the Covidestim model to look into the source of the lag observed in Texas, as shown in Figure \ref{fig:texas}. The two solid lines are the data sources: the data source for the probabilistic analysis is PCR tests from the CDC's Diagnostic Laboratory Testing Time series, while the data source for the Covidestim model is Johns Hopkins CSSE case counts. Each source is summed on the biweekly time scale. The test positivity is the positive tests from the CDC source over total tests for the two-week interval. The Johns Hopkins CSSE and  CDC positive tests are largely concordant, so differences in data source do not appear to explain the lag for the delta wave in Texas. The test positivity, however, does clarifies the lag. The test positivity begins increasing in early June, before we see positive tests or cases increasing, due to total tests at first decreasing and then increasing, but not at the same rate as the positive tests (Supplementary Figure \ref{fig:test-capacity-all-states}) . This increase in test positivity is reflected in the probabalistic bias intervals in Figure \ref{fig:texas-source}, where increases corresponding to the Delta wave start begin earlier, in early to mid June, compared to July, where the increase is at about a two-week lag.}\label{fig:unnamed-chunk-68}
\end{figure}
Although we focus on the example of Texas here, we see similar trends in other states where Covidestim lags the probabilistic bias intervals, where, in these states, there is a lag to the increase in positive tests relative to total tests at the beginning of the Delta wave (Supplementary Figure \ref{fig:jhu-cdc-all}).

\hypertarget{summarizing-concordance-with-covidestim}{%
\subsection{Summarizing Concordance with Covidestim}\label{summarizing-concordance-with-covidestim}}

To summarize the trends we see across states in Figure \ref{fig:state-results-1} and Figure \ref{fig:state-results-2}, we can consider the proportion of probabilistic bias intervals, including all states, where the Covidestim median fell within, above, or below the probabilistic bias interval (Figure \ref{fig:summarize-concordance-state}). We see in Figure \ref{fig:summarize-concordance-state} that the implementation of the priors where we center the prior for \(P(S_1|\text{untested})\) at the percentage of COVID-19-like illness from the COVID-19 Trends and Impact Survey is most concordant with Covidestim.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/covidestim_concordance_state} \caption{\label{fig:summarize-concordance-state} To summarize the concordance with Covidestim for each state, we consider the proportion of all probabilistic bias intervals that contain the Covidestim median for that time interval, and the proportions where the Covidestim median falls above or below the interval. We see the implementation that centers the prior for $P(S_1|\text{untested})$ at the percent of COVID-19-like illness in the population is the most concordant with Covidestim.}\label{fig:unnamed-chunk-69}
\end{figure}
\hypertarget{ratio-between-estimated-infections-and-observed-infections}{%
\section{Ratio Between Estimated Infections and Observed Infections}\label{ratio-between-estimated-infections-and-observed-infections}}

To get a better sense for number of true infections relative to those that were observed, we can compare the ratio of estimated infections to observed infections, where the number of observed infections is the number of positive PCR tests.

In Figure \ref{fig:ratio-est-observed}, we compare the ratios of estimated to observed infections across locations and two-week intervals. The states are ordered by median ratio of estimated infections to observed infections across the set of time intervals.

For example, we see that in Mississippi, the ratio of estimated infections to observed was often than 10 in several cases, indicating that for every observed positive PCR test in the state, there were 10 true infections. For Mississipi, the ratio exceeded 10 for a third of all two-week intervals considered.

Meanwhile, states with substantial testing tended to have lower ratios across the entire time period considered. For example, in Rhode Island, Massachusetts, and the District of Columbia, the ratio never exceeded 4.

In general, the ratio of estimated to observed infections was highest in the summer of 2021 in the time period spanning June 18, 2021 through July 16, 2021.

The ratio is distinct from considering general peaks -- for example, even though we see the ratio of estimated infections to observed is highest in the summer, the number of estimated infections is far higher during the Omicron wave.

The higher ratio in the summer is related to the drop in testing during this time period (Figure \ref{fig:test-low-summer}).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/heatmap_ratio_est_observed} \caption{\label{fig:ratio-est-observed} The ratio of estimated infections to observed infections for each 2-week interval and each state, only considering the version of the priors that does not vary by state or time. States are ordered from highest to lowest by median ratio of estimated infections to observed infections over all 2-week intervals considered. The two-week intervals with highest ratios are in June 18, 2021 through July 16, 2021.}\label{fig:unnamed-chunk-70}
\end{figure}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/testrate-low-summer} \caption{\label{fig:test-low-summer}Testing rate, calculated as the etotal number of tests for a 2-week interval in a state over the census population in that state, over time. The median across states for each two-week interval is shown in red. In relation to Figure \ref{fig:ratio-est-observed}, where we see the period where the ratio of estimated to observed infections is the highest in June 18, 2021 through July 16, 2021, we see here that testing rates across states are at a minimum during this time period.}\label{fig:unnamed-chunk-71}
\end{figure}
Since the Omicron wave is of interest given the aspects of this variant that were dramatically different to previous variants, we can also look specifically at the ratio of estimated infections to observed during the peak of this wave in December 31, 2021 through January 14, 2022 (Figure \ref{fig:ratio-peak-omicron}).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ratio-peak-omicron} \caption{\label{fig:ratio-peak-omicron}The ratio of estimated infections to observed infections for each state from December 31, 2021 through January 14, 2022; this time interval corresponds to the peak of the Omicron wave. States are ordered by the highest median ratio, which is shown in red.}\label{fig:unnamed-chunk-72}
\end{figure}
To get a sense for how states compare to each other across time in the number of infections going unobserved, we can compare how states rank in the ratio of estimated infections to observed across the two-week intervals.

In Figure \ref{fig:rank-ratio-over-time}, we look at the states that consistently had the largest ratios or the smallest ratios across time. More precisely, we consider states that were ranked in the top 10 or bottom 10 by the ratio of estimated to observed infections in at least 80\% of all time intervals considered.

The states that had the lowest ratios of estimated to observed infections were the District of Columbia, Vermont, Massachusetts, New York, Rhode Island, and Alaska, indicating that case ascertainment (the proportion of infections that were detected) was highest in these states.

Meanwhile, the states that had the highest ratios of estimated to observed infections, and as such the lowest case ascertainment rates, were Nebraska, Oklahoma, South Dakota, Mississippi, Tennessee, and Alabama.

Nebraska was one of the only states that did not have a mandatory stay-at-home order in 2020 or recommendations to stay at home; Mississippi and South Dakota never had a mandatory stay-at-home order but did have recommendations to stay at home (Moreland et al., 2020).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/rank-ratio-over-time} \caption{\label{fig:rank-ratio-over-time}For each two-week interval, we rank each state's ratio of estimated to observed infections, where 1 corresponds to the state with the \emph{lowest} ratio (indicating less infections are going unobserved), and 51 corresponds to the state with the \emph{highest} ratio (indicating more infections are going unobserved relative to those observed). Visualized here are the states that consistently ranked as among the lowest ratios or highest ratios over time: specifically, states were among the top 10 highest ratios or top 10 lowest ratios for at least 80\% of the time intervals are shown.}\label{fig:unnamed-chunk-73}
\end{figure}
\newpage

\hypertarget{relationship-between-the-ratio-of-estimated-to-observed-infections-compared-to-testing-rate}{%
\section{Relationship Between the Ratio of Estimated to Observed Infections Compared to Testing Rate}\label{relationship-between-the-ratio-of-estimated-to-observed-infections-compared-to-testing-rate}}

Another question to consider is the relationship between the testing rate and the ratio of estimated infections to observed infections, as we would expect higher testing rates would lead to less underestimation of the true number of infections.

As we see in Figure \ref{fig:testrate}, the nature of the relationship between the testing rate and the ratio of estimated infections to observed infections depends on whether we allow \(\beta\) and \(P(S_1|\text{untested})\) to vary by location and date. In particular, when we sample from the same priors for every correction (the first panel of Figure \ref{fig:testrate}), we see there is little variability in the relationship between the testing rate and median estimated infections, because the form of the correction is identical for each two-week interval and state considered. Allowing \(\beta\) and/or \(P(S_1|\text{untested})\) to vary by time and location introduces additional variability in the relationship between the ratio of estimated infections to observed and testing rate.

The nonlinearity of the relationship between the testing rate and the ratio of estimated infections to observed infections is more clear when we think back to the correction formulas.

Denoting \(N^*\) again to be the number who would test positive, on the \(x\)-axis, we have approximately\footnote{This isn't exactly the estimated infections, because for simplicity of notation we are not writing out the correction for test inaccuracy.}
\[\frac{N^*_{\text{tested}} + N^*_{\text{untested}}}{N^*_{\text{tested}}},\]

where we calculate \(N^*_{\text{untested}}\) using the other priors:
\[N^*_{\text{untested}} =\Pr(\text{test}_+|S_1,\text{untested}) (\Pr(S_1 | \text{untested})) ( N_{\text{untested}} ) + \]
\[\Pr(\text{test}_+|S_0,\text{untested}) (1-(\Pr(S_1 | \text{untested})) ( N_{\text{untested}} ).\]
On the \(x\)-axis we have the number tested over the population size, \(\dfrac{N_{\text{tested}}}{N}\). Thus, we see the trend in each panel where for small changes in testing rate when the testing rate is very low, the ratio of unobserved to unobserved is very high since \(N^*_{untested}\) will be large relative \(N^*_{tested}\). However, with higher testing rates, \(N^*_{untested}\) will be large relative \(N^*_{tested}\), and the ratio of estimated to observed infections nears one.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/testing_rate_ratio} \caption{\label{fig:testrate}The ratio of the median estimated infections to observed infections plotted against the testing rate, where the testing rate is calculated as the total number tested in a two-week interval over the population size. When the priors are the same for all time intervals and states, there is minimal variability relationship between the testing rate and the ratio of estimated to observed infections, since the correction for incomplete testing and diagnostic test inaccuracy is identical for each time-interval and location. However, when we allow $\beta$ or $P(S_1|\text{untested})$ to vary by state and time interval, there is much variability in the relationship. A horizontal line in red at 1 is included to reference; a ratio of exactly one would indicate no infections went unobserved.}\label{fig:unnamed-chunk-74}
\end{figure}
It is worth noting that in some cases, the ratio of total number of tests for a two-week interval over the population size is incredibly high. For example, there are two-week intervals in Vermont, Rhode Island, and DC where the total number of tests over the census population size exceeds 0.25. There are two major factors to consider when we interpret these really high testing rates.

For one, there is repeat testing. If people are part of a regular PCR screening program where they are tested multiple times in a 2-week time interval, this will go toward higher numbers of tests relative to the census population size. While some states also report the number of people who tested positive and people who tested total\footnote{A data dictionary on the John Hopkins COVID-19 testing repository provides more detail on these definitions.} (rather than total positive tests and total tests), this level of granularity was not universal.

Additionally, the census population is not exactly the tested population. This is in part because universities are a major source of PCR testing, and students who are out of state are not reflected in the census counts for the location where they are tested.

When we look at the testing rates (taken as the total number of tests over the census population size) in Figure \ref{fig:college}, we see that the states with the highest biweekly testing rates are Vermont, Massachusetts, Rhode Island, and the District of Columbia. These are states that haves substantial student populations, who also may be tested more than once in a single two-week interval.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/college_populations} \caption{\label{fig:college} Comparing the biweekly testing rate by state across all 2-week intervals considered,  where color is by the ratio of the student population to the census population. States are ordered by median testing rate. Data source for student population by state: the U.S. Department of Education, National Center for Education Statistics, Integrated Postsecondary Education Data System (IPEDS), 12-month Enrollment component provisional data for 2020 to 2021. Student population counts include Title IV postsecondary institutions.}\label{fig:unnamed-chunk-75}
\end{figure}
\hypertarget{county-level-results}{%
\section{County-level Results}\label{county-level-results}}

We performed county-level probabilistic bias analysis for Michigan and Massachusetts. Among states that report the needed data, Michigan is among the largest, and hence this gives us a large number of counties to consider with a variety of demographic characteristics. Massachusetts, meanwhile, is interesting in the sense that it is a state with a very high testing rate and has counties where there is a substantial amount of screening testing (e.g., Hampshire County).

This work can be expanded to consider other states as well where the needed data is available. In particular, we need both county-level positive PCR tests and county-level total PCR tests. Because the assumptions of the bias correction are related to test positivity, it does not make sense to apply the method to a positive cases count that includes positive PCR tests lumped together with probable cases. In some states, this is the only value reported.

While positive tests are more regularly reported, total tests are reported less frequently at the county level. This is particularly true for states that do less testing overall.

\hypertarget{massachusetts}{%
\subsection{Massachusetts}\label{massachusetts}}

\hypertarget{comparing-implementations-of-probabilistic-bias-analysis}{%
\subsubsection{Comparing Implementations of Probabilistic Bias Analysis}\label{comparing-implementations-of-probabilistic-bias-analysis}}

Figure \ref{fig:pb_counts_ma} shows the bias-corrected estimates for each implementation, as well as the observed infections. We note that the lower bounds of the bias-corrected estimates are always above the observed infections because adding (unobserved) infections among the untested population to the observed positives among the population never results in a decrease in the estimated infections. In theory such a decrease could be possible since we do correct for differences due to imperfect test accuracy, and if the false positive rate was high enough, we might estimate the lower bound of cases as lower than the observed infections. However, the false positive rate of the COVID-19 PCR test is so low that in practice we do not see lower bounds lower than the number of infections.\footnote{The false positive rate differs by platform and laboratory, but multiple analyses estimated that it is less than 0.10\% (Chandler, Bourassa, Mathias, \& Greninger, 2021).}

We can see although the trends are broadly similar between versions for each county, centering the distribution at the empirical value of \(\beta\) leads to peaks not present in the version where priors do not vary by county and date. However, only centering \(P(S_1|\text{untested})\) at the empirical value leads to a distribution that is highly similar to the version where priors do not vary by county and date.

These results make sense when we consider that this analysis is much more sensitive to the choice of \(\beta\) than \(P(S_1|\text{untested})\). This follows from the fact we calculate the number of positive infections among those who are untested and \emph{asymptomatic} as
\begin{align*}
N^+_{\text{untested},S_0} &= P(\text{test}_+| S_0,\text{untested}) (N_{S_0,\text{untested}})\\
&= \Big( \beta P(\text{test}_+ |\text{tested}) \Big) N_{\text{untested}} (1-P(S_1|\text{untested}))
\end{align*}
and the number of positive infections among those who are untested and \emph{symptomatic} as\\
\begin{align*} N^+_{\text{untested},S_1}& = P(\text{test}_+| S_1,\text{untested}) (N_{S_1,\text{untested}})\\
&= \Big( \alpha P(\text{test}_+ |\text{tested}) \Big) N_{\text{untested}} (P(S_1|\text{untested})).
\end{align*}
Since \(N_{S_1, \text{untested}}\) is so much larger than \(N_{S_1, \text{untested}}\) for any of the specified values of \(P(S_1|\text{untested})\) (since the bulk of this distribution is less than 5\%), \(\beta\) has a larger impact on the number of estimated infections.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ma_pb_compared_to_observed} \caption{\label{fig:pb_counts_ma}Probabilistic bias intervals for counties in Massachusetts. Each probabilistic bias interval corresponds to a 95\% simulation interval for the total number of estimated infections for that county in that two-week time interval. The columns represent different implementations of the probabilistic bias analysis. The first column corresponds to the implementation where we specify priors without using data from the COVID-19 Trends and Impact Survey. For the second column, we center the distribution of $\beta$ at the ratio of the screening test positivity to the oerall test positivity from the survey. For the third column, we center the distribution of $P(S_1|\text{untested})$ at the percentage of the population experiencing COVID-19-like illness from the survey. The fourth column centers both  $\beta$ and $P(S_1|\text{untested})$ at the aforementioned values.}\label{fig:unnamed-chunk-76}
\end{figure}
To better see the overlap between versions, in Figure \ref{fig:pb_versions_ma} we can look at the versions together. This allows us to see more clearly how the version with both \(P(S_1|\text{untested})\) and \(\beta\) centered at their empirical values is consistently the highest. Meanwhile, the version with only \(P(S_1|\text{untested})\) centered at its empirical value corresponds so closely to the version that does not vary by date or location that there is no part of the intervals for the version not varying by date or location that does not overlap with the version only centering \(P(S_1|\text{untested})\) at the survey value.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ma_pb_compare_versions} \caption{\label{fig:pb_versions_ma} Presenting the same implementations from \ref{fig:pb_counts_ma}, but not where all implementations for a county are considered in the same plot to better see the overlap between the implementations. The implementation that centers both $P(S_1|\text{untested})$ and $\beta$ at their empirical values is consistently the highest among the implementations, and the version only centering $P(S_1|\text{untested})$ at the survey value is highly concordant with the version that does not use survey data to inform the priors.}\label{fig:unnamed-chunk-77}
\end{figure}
Beyond comparing the versions to each other, we can also compare the extent to which these intervals agree with the Covidestim estimates.

To compare the probabilistic bias intervals to the Covidestim estimates, we can again look at the proportion of time intervals where the probabilistic bias intervals contained the Covidestim median infections. We can summarize concordance for each implementation of the bias analysis by calculating, across all counties and time intervals, the fraction of bias intervals for a particular version that contained the Covidestim median (Figure \ref{fig:summarize-concordance-mass-counties}). From Figure \ref{fig:summarize-concordance-mass-counties}, we see that for counties in Massachusetts, the implementation of probabilistic bias analysis that centers the prior for \(P(S_1|\text{untested})\) at the percentage experiencing COVID-19-like illness is most concordant with the Covidestim estimates.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/summarize-concordance-mass-counties} \caption{\label{fig:summarize-concordance-mass-counties} To summarize the concordance with Covidestim for each county in Massachusetts, we consider the proportion of all probabilistic bias intervals (each corresponding to a two-week interval) that contain the Covidestim median for that time interval, or where the Covidestim median falls above or below the interval.}\label{fig:unnamed-chunk-78}
\end{figure}
To see how concordance differs by county, we can consider a specific implementation of the priors -- here, we compare the concordance with the version with \(P(S_1|\text{untested})\), since we see this implementation is the most concordant from Figure \ref{fig:summarize-concordance-mass-counties}. Then, for that implementation, we can see what proportion of probabilistic bias intervals for a particular county contain the Covidestim median (Figure \ref{fig:above-below-by-county-ma}).

In Figure \ref{fig:above-below-by-county-ma}, we see that for most counties, more than 3/4 of the probabilistic bias intervals contain the Covidestim estimates.

However, Suffolk County and Hampshire County are two notable exceptions, where for a substantial proportion of time-intervals, the Covidestim median falls above the probabilistic bias interval. For example, across the entire time period considered, the Covidestim median fell above the probabilistic bias intervals for Hampshire County in 16 out of 25 two-week intervals (64\%). We can see where these differences occur across time in Supplementary Figure \ref{fig:ma-comp-covidestim}.

These exceptions are likely driven by the high amount of testing, particularly screening testing, in counties that contain universities or colleges. For example, in Hampshire County, there is a high amount of asymptomatic testing done among the five colleges. Smith College alone, with a student body of about 2,500 and mandatory PCR testing twice a week, would contribute a substantial amount of tests to the test count in Hampshire County. Given the low infection burden, the vast majority of these tests were negative throughout most of 2021, reducing the positivity rate (Figure \ref{fig:hamp}). The extensive amount of asymptomatic testing done here would mean that
means that our correction factor \(\beta\) in probabilistic analysis\footnote{Recall \(P(\text{test}_+ | S_0, \text{untested}) = \beta \; P(\text{test}_+|\text{tested})\).}, which we use to estimate the positivity rate among the untested asymptomatic population, would be too low. This is because the observed test positivity rate when so much asymptomatic testing is conducted would be closer to what expect the positivity rate among the asymptomatic test positivity rate to be among the untested population. Because the assumptions of the probabilistic bias analysis here did not assume such extensive asymptomatic testing was contributing to the test positivity rate, we are likely underestimating the true number of unobserved infections if there is a substantial amount of screening testing. It would be simple, however, to adjust \(\beta\) to account for cases where there is increased asymptomatic testing.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/covidestim_above_below_by_county_ma} \caption{\label{fig:above-below-by-county-ma} To summarize the concordance with Covidestim for each county in Massachusetts, we consider the proportion of all probabilistic bias intervals (each corresponding to a two-week interval) that contain the Covidestim median for that time interval, or where the Covidestim median falls above or below the interval. The Covidestim median falls above the probabilistic bias intervals for Suffolk and Hampshire counties in more two-week intervals than in other counties.}\label{fig:unnamed-chunk-79}
\end{figure}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/hampshire} \caption{\label{fig:hamp}Hampshire County has among the lowest test positivity rates among counties in Massachusetts in the time period considered.}\label{fig:unnamed-chunk-80}
\end{figure}
It is worth emphasizing again that when we compare to the Covidestim estimates, we must recognize that Covidestim is a model, with a particular set of assumptions, and is not a ground truth. Concordance of the probabilistic bias intervals with Covidestim, then, should not be treated as a measure of accuracy. However, when there is disagreement between the approaches, this can allow us to think more deeply about the distinct assumptions of these approaches that underscore the differences we observe.

\hypertarget{comparing-counties}{%
\subsubsection{Comparing Counties}\label{comparing-counties}}

As we did at the state level, we can see how counties differ across time in the ratio of estimated infections to observed infections. In Figure \ref{fig:ma-heatmap}, we look at the ratios of estimated infections to observed infections across time.

Counties with the lowest ratios include Suffolk County and Hampshire County, both of which contain multiple colleges and universities. Students testing more frequently than the general population may contribute to the lower ratios we see in these counties.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/ma_county_heatmap_ratio_est_observed} \caption{\label{fig:ma-heatmap} The ratio of estimated to observed infections across time for counties in Massachusetts. Counties are ordered by the median ratio across time intervals, from the highest ratio (Barnstable) to the lowest (Hampshire). Similar to what we saw at the state level in \ref{fig:ratio-est-observed}, the highest ratios were during the summer of 2021 during the Delta wave -- a period of decreased testing. The span of time with the highest ratio of estimated to observed infections was July 2, 2021 through July 30, 2021.}\label{fig:unnamed-chunk-81}
\end{figure}
To see how counties compare to each other with regard to the ratio of estimated to observed infections across the full time span considered, we can rank the counties for each two-week interval, and see which counties consistently have the highest ratios and which have the lowest.

In Figure \ref{fig:rank-ratio-over-time-ma-county}, we see that Hampshire County, Suffolk County, and Middlesex County were consistently the three counties with the lowest ratios of estimated infections to observed infections. Hampshire County's rank declines during periods where school was not in session (June until the beginning of September) and in December; this may be related to the drops in asymptomatic testing during these periods.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/rank-ratio-over-time-ma-county} \caption{\label{fig:rank-ratio-over-time-ma-county}The rank of the ratio of estimated infections to observed infections over time. For each two-week interval, we rank the counties by the ratio of estimated infections to observed, where a rank of 13 would indicate the county had the *highest* ratio of estimated infections to observed, and a rank of 1   would indicate the county had the *lowest* ratio of estimated infections to observed. Only counties that were consistently at the extremes, that is, that were among the highest 3 or lowest 3 for at least 80\% of the time intervals considered, are included.}\label{fig:unnamed-chunk-82}
\end{figure}
\hypertarget{michigan}{%
\subsection{Michigan}\label{michigan}}

For reference, the figures including probabilistic bias analysis intervals for each of the 83 counties in Michigan, where each implementation is presented separately, are included in Supplementary Figures \ref{fig:pb_versions_mi-1}, \ref{fig:pb_versions_mi-2}, and \ref{fig:pb_versions_mi-3}.

We can compare to Covidestim again by considering the proportion of probabilistic bias intervals that contain the Covidestim median across all counties for each implementation (Figure \ref{fig:covidestim_above_below_by_county_mi}).

As we saw at the state level across the United States (Figure \ref{fig:summarize-concordance-state}) and in
Massachusetts at the county level (Figure \ref{fig:summarize-concordance-mass-counties})), we see the version with the highest concordance with Covidestim is that where the prior for \(P(S_1|\text{untested})\) is centered at the percent of the population experiencing COVID-19-like illness from the COVID-19 Trends and Impact Survey. This is true when summarizing across all counties, and it is also true for each county individually; the version centering \(P(S_1|\text{untested})\) at the survey value tends to be the most concordant for each county (Supplementary Figure \ref{fig:mi-county-concordance}).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/summarize-concordance-mich-counties} \caption{\label{fig:covidestim_above_below_by_county_mi} For each implementation, we calculate the proportion of probabilistic bias intervals over all counties and where the Covidestim median falls within, above, or below the interval. We interpret a higher proportion *in interval*, where the Covidestim falls within the probabilistic analysis interval, to indicate higher concordance. As we saw in Massachusetts (Figure \ref{fig:summarize-concordance-mass-counties}), the implementation with the highest concordance with Covidestim is the version that centers the prior for $P(S_1|\text{untested})$ at the survey value.}\label{fig:unnamed-chunk-83}
\end{figure}
When we compare the ratio of estimated infections to observed infections over time, we see that the ratios are highest across counties in late May into the summer, through July (near the Delta wave) and in late January through mid February (following the peak of the Omicron wave). This is similar to what we saw in Massachusetts, where, although the number of infections increased dramatically during the Omicron wave, there was only a small increase in the ratio of estimated total infections to observed in this time period.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/mi_county_heatmap_ratio_est_observed} \caption{\label{fig:mi-county-heatmap}Each row on the heatmap corresponds to a county, while each column is the two-week interval. The color of the tile for a two-week interval in a particular county is the ratio of total estimated infections to the observed infections for that two-week interval. The impelmentation of probabilistic bias analysis used for the total estimated infections is the implementation centering $P(S_1|\text{untesteed})$ at the survey value. Darker colors indicate higher ratios, where more infections were going unobserved relative to those that were observed. Intervals where the number of observed infections was zero (which is the case in some very small counties where the total number of tests, was, for instance, only 100) are shown in white, since the ratio of estimated to observed is undefined in these cases. Counties are ordered from highest to lowest by median ratio of estimated infections to observed infections. That is, counties at the top, for example, Schoolcraft and Alder, have the highest median ratios of estimated infections to observed, while counties at the bottom, for example, Washtenaw, have the lowest ratios. Across counties, we see notable increases in the summer of 2021 in June through July, and in late January through February.}\label{fig:unnamed-chunk-84}
\end{figure}
When we compare counties' ratios of total estimated infections to observed infections over time, we see that 2 counties had among the lowest 15 ratios across all time intervals considered. Washtenaw County has the lowest ratio across almost all time periods considered, while Kent County has among the lowest ratios until the Omicron wave. Both counties are among the top 10 largest counties in Michigan, and both include universities: University of Michigan is in Washtenaw County, and several liberal arts colleges, for instance, Aquinas College and Calvin University, are in Kent County.

Among the counties that had the highest ratios, Schoolcraft is one of the smallest states in Michigan (population close to 8,000), and Leelanau is also small (population about 21,800); both Clinton and Cass have more than 50,000. All have low population densities. Of note, when considering the ratios of all counties, the ratio of estimated infections to observed infections was not markedly different between population sizes, but there were more outliers in the ratios for smaller counties (Supplementary Figure \ref{fig:mi-ratio-pop}).
\begin{figure}
\includegraphics[width=1\linewidth]{figure/rank-ratio-over-time-mi-county} \caption{\label{fig:rank-ratio-over-time-mi-county}For two-week interval, we rank counties by the ratio of estimated total infections to observed infections, where the highest ranking of 83 indicates the highest ratio, while the lowest possible ranking of 1 indicates the lowest ratio. Counties that fell in the top 15 or bottom 15 for at least 80\% of time intervals are included here. Lines are dotted when there is a missing value due to reporting of zero observed cases, in which case the ratio of estimated to observed infections is undefined.}\label{fig:unnamed-chunk-85}
\end{figure}
\newpage

\hypertarget{cross-correlation-comparison}{%
\section{Cross Correlation Comparison}\label{cross-correlation-comparison}}

\hypertarget{background-1}{%
\subsection{Background}\label{background-1}}

An ongoing challenge for assessing the quality of the probabilistic bias intervals is that there is no ground truth to compare to. To broaden our comparison beyond Covidestim, we can look at wastewater data.

Wastewater data is a source of data that has been of rising interest throughout the pandemic, in part due to its cost effectiveness in assessing community-level burden, but also due to the fact it represents a much less sample than COVID-19 PCR testing does.

That said, there are challenges in relating wastewater concentrations to the true number of infections, in part because of the same issue we face here of the lack of ground truth for the true number of infections in any location. The choice of normalization of the viral RNA concentrations of SARS-CoV-2 is important for understanding how these concentrations scale to the number of infections, since the concentration of virus (in genome copies per liter) in a sample will be influenced by various factor unrelated to the true prevalence COVID-19, such as processing differences between treatment plants or trends in water usage. One common choice is to normalize against the concentration of a virus that has a relatively stable population in wastewater, such as Pepper Mild Mottle Virus (PMMoV) (Zhan et al., 2022).

Wastewater testing has become increasingly widespread throughout the pandemic as the technology and analysis approaches have evolved, as well as the demand for a source of data on the presence of COVID-19 that is less reliant on access to tests (or symptoms strong enough to warrant a test, which differ by the variants circulating). A comprehensive source of wastewater data across the United States is provided by Biobot Analytics, which is the institution partnering with the CDC for the National Wastewater Surveillance System (NWSS) (Duvallet et al., 2022). Biobot Analytics provide wastewater concentrations aggregated at the county scale by using a weighted average of the concentrations at sampling locations within the county, weighted by the size of the corresponding sewershed populations. This data is publicly available on a public github repository\footnote{As of May 11, 2023, this data is available at the github repository \href{https://github.com/biobotanalytics/covid19-wastewater-data}{here}.}.

Several counties in Massachusetts have reported wastewater data for a substantial period throughout 2021 to 2022. This allows us to compare the bias-corrected estimates -- as well as the Covidestim estimates -- to the wastewater concentrations.

Wastewater concentrations are typically a leading indicator of observed infections, though there may be some variability in the lead time during different waves of the pandemic (Hopkins et al., 2023). In particular, the lead time was strongest in the earliest waves of the pandemic, and has since declined (Xiao et al., 2022). Various factors can create the changes we see in lead time over the course of the pandemic; for example, the lead time can be impacted by differences in viral shedding, diagnostic testing turnaround times, and testing capacity and behavior (Olesen, Imakaev, \& Duvallet, 2021).

Since the correlation between the time series as well as the lag at which the maximum correlation occurs are both of interest, we assessed the cross correlation between the series.

First, we define autocorrelation since the definition of cross correlation is very similar. The definition here uses the notation of Shumway \& Stoffer (2011).

The auto-correlation function \(\rho_{XX}(\tau)\) measures the linear dependence between \(X_{1+\tau}, \dots X_n\) and \(X_1, \dots, X_{n-\tau}\), that is, the difference between the original time series and the time series shifted forward by \(\tau\) time units.
\begin{tcolorbox}[title=Definition: Autocorrelation]

Denote the set of time points of a time series $T$. For any time series $(x_t)_{t\in T}$, we define the auto-correlation function (ACF)  as 

$$\rho_{XX}(\tau) = \dfrac{E[(X_{t + \tau} - \mu_{X_{t+\tau}}) (X_t - \mu_{X_t})]}{sd(X_{t+\tau}) sd(X_t)}.$$
\end{tcolorbox}
Assuming second order stationarity\footnote{Second order stationarity is also referred to as weak stationarity, and implies that the mean and variance are constant over time and the autocovariance function depends only on the difference between time points.}, we have \(\mu_{X_{t+\tau}}=\mu_{X_{t}}\)
and similarly \(\text{Var}({X_{t+\tau}})=\text{Var}({X_{t}})\), so we can simplify the expression for \(\rho_{XX} (\tau)\) to yield\\
\[\rho_{XX} (\tau)=\dfrac{E[(X_{t + \tau} - \mu_{X}) (X_t - \mu_{X})]}{Var(X)}.\]

We can extend this definition to quantify the linear relationship between distinct lagged time series \(X_1, X_2, \dots, X_t\) and \(Y_1, Y_2, \dots, Y_t\) by defining the cross correlation function. The function is only defined on two time series that are over the same time interval and sampled at the same frequency.
\begin{tcolorbox}[title=Definition: Cross-Correlation]

We compute the cross-correlation function (CCF) as

$$\rho_{XY}(s,t) = \dfrac{E[(X_s - \mu_{X_s})(Y_t-\mu_{Y_t})]}{\sqrt{\text{Var}(X_s) \text{Var}(Y_t)}}.$$
Again assuming the series satisfy second-order stationarity, we have

$$\rho_{XY}(s,t) = \dfrac{E[(X_s - \mu_{X})(Y_t-\mu_{Y})]}{\sqrt{\text{Var}(X) \text{Var}(Y)}}.$$

\end{tcolorbox}
The implementation of the cross correlation in base R (\texttt{stats::ccf}) assumes second order stationarity (Venables \& Ripley, 2002).

Looking at cross correlation can be useful in the sense that we can both consider the strength of correlation and the lag at which the correlation is maximized. Before presenting the cross correlation results of the county level time series, we can consider a more concrete example, where the lag is known.

In Figure \ref{fig:compdiff}, we consider simulated data where \((Z_t)\) is \((Y_t)\) lagged by 3 time units with noise added. We can see that \(Z_t\) and \(Y_t\) are not second-order stationary since the mean clearly is not constant over time. However, to stabilize the mean, we can apply first order differencing, where we take the differences between consecutive observations.
\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-86-1} \caption{\label{fig:compdiff}In the first panel, where the time series in not differenced, the violation of second order stationarity is clear in the fact the mean is not constant over time. When we apply differencing, that is, subtracting the lagged value from each value in the series, we obtain the time series in the second panel, where the mean is constant over time. This underscores why differencing is important when interpreting the cross correlation, which assumes, in the `stats::ccf` implementation, second order stationarity.}\label{fig:unnamed-chunk-86}
\end{figure}
We can see the effect of applying differencing to the time series when we compute the cross correlations of \((Z_t)\) and \((Y_t)\), as shown in Figure \ref{fig:corzy}. The true lag of \(-3\) time units was recovered when considering the differenced time series, but not when we considered the original time series. In what follows, because the time series we are considering are not stationary, we consider the cross correlation between the differenced time series.
\vspace{5 cm}
\begin{figure}
\includegraphics[width=0.5\linewidth]{thesis_files/figure-latex/unnamed-chunk-87-1} \includegraphics[width=0.5\linewidth]{thesis_files/figure-latex/unnamed-chunk-87-2} \caption{\label{fig:corzy}In the first panel, we consider the time series that has not been differenced, and see that the lag at which the maximum cross correlation occurs is not the true lag. When we apply differencing, we see in the second panel that the maximum correlation occurs at the true lag.}\label{fig:unnamed-chunk-87}
\end{figure}
\hypertarget{cross-correlation-results-comparing-bias-corrected-counts-covidestim-estimates-and-wastewater-concentrations}{%
\subsection{Cross Correlation Results Comparing Bias-Corrected Counts, Covidestim Estimates, and Wastewater Concentrations}\label{cross-correlation-results-comparing-bias-corrected-counts-covidestim-estimates-and-wastewater-concentrations}}

Because wastewater data is reported at the weekly time scale while the bias-corrected estimates are at the 2-week time scale, we take a mean of the effective concentration for each 2 week interval, such that the time series are sampled at the same frequency.\footnote{We cannot interpret the cross correlation if the time steps are different.}

Since the effective concentration of SARS-CoV-2 in wastewater samples reported by Biobot is in genome copies per liter and is not directly comparable to estimates of infections, we place the wastewater concentration on a separate scale.

Looking at the counties in Figure \ref{fig:wastewater_ma_by_county}, we see that, with the exception of Barnstable, MA, the wastewater trends are highly similar to trends captured by the bias-corrected infection counts. We also see that the trends are similar both with regard to shape but also with regard to time, with little visible lag between the series. This is expected because although wastewater cases do in general lead cases, lead times generally are not on the order of 2 weeks. This means that since we are summarizing to 2-week intervals we would expect the lag to be very small, if present at all.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/wastewater_ma_by_county} \caption{\label{fig:wastewater_ma_by_county}}\label{fig:unnamed-chunk-88}
\end{figure}
\hypertarget{comparison-between-implementations-of-probabilistic-bias-analysis}{%
\subsubsection{Comparison Between Implementations of Probabilistic Bias Analysis}\label{comparison-between-implementations-of-probabilistic-bias-analysis}}

In Figure \ref{fig:correlation_observed_pb}, we see that, in general, infections were highly correlated with the wastewater effective concentrations, which was true across all implementations of probabilistic bias analysis. In most cases, the implementation where priors did not vary by state or date were the most highly correlated with the wastewater concentrations. Exceptions to this were Barnstable County (25001), where the implementation with the prior for \(\beta\) centered at the empirical value was the most highly correlated, and Worcester County (25027), where the implementation with the prior for \(P(S_1|\text{untested})\) centered at empirical value was the most highly correlated. In all counties except for Barnstable, the lag at which the maximum correlation was obtained was 0 units, while for Barnstable it was -1, indicating that wastewater concentrations led infections by one two-week interval.

Given the small size of Barnstable relative to other counties and high variability in its early estimates in 2021 (as seen in Figure \ref{fig:wastewater_ma_by_county}), it is possible that there were still aspects of the SARS-CoV-2 detection process that took time to refine. Another possibility is that the way Biobot aggregated wastewater concentrations by county failed to capture the infection dynamics in this county, since wastewater catchments are not contained within county lines. This is a central challenge in relating cases to wastewater concentrations, since these values are recorded for distinct geographic units.

Comparing the maximum correlations obtained for the observed infections, only in Hampshire County were the observed infections more correlated with the wastewater concentrations than all implementations of probabilistic bias analysis. We also see again that in most cases the maximum correlation is obtained at zero lag in observed cases; however, for Barnstable, the correlation is highest when wastewater concentrations lead infections by two two-week intervals, and for Hampshire the correlation is highest when wastewater concentrations lead infections by 1 two-week interval.
\begin{figure}
\includegraphics[width=1\linewidth]{figure/correlation_observed_pb} \caption{\label{fig:correlation_observed_pb}For each implementation of the priors, we consider the maximum cross correlation between the estimated infections for the two week interval and the mean effective concentration of SARS-CoV-2 in the wastewater for that two-week interval. We also include the cross correlation between the observed infections and effective wastewater concentration to see how the correlations between the bias-corrected estimates with wastewater concentrations compare to the correlations between the observed infections and wastewater concentrations. The shape of the point (circle, square, or triangle) indicates the lag at which the maximum correlation was obtained. For instance, a lag of -1 would indicate the maximum correlation between the time series and wastewater concentrations is when wastewater concentrations lead infections by one two-week interval. In most cases, the lag was zero.}\label{fig:unnamed-chunk-89}
\end{figure}
\vspace{8 cm}

\hypertarget{comparison-between-covidestim-observed-cases-and-bias-corrected-counts}{%
\subsubsection{Comparison Between Covidestim, Observed Cases, and Bias-Corrected Counts}\label{comparison-between-covidestim-observed-cases-and-bias-corrected-counts}}

In Figure \ref{fig:correlation_observed_pb_covidestim}, we also compare the Covidestim estimates to the wastewater concentrations. In general, both Covidestim and bias-corrected counts are more correlated with wastewater concentrations than observed infections. Of note, Nantucket County (25019) is not included here because Covidestim does not report estimates are not reported for Nantucket.\footnote{In reporting of COVID-19 data, Nantucket values are grouped with Dukes County (Martha's Vineyard), which may be why Covidestim does not try to estimate the grouped counts.}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/correlation_observed_pb_covidestim} \caption{\label{fig:correlation_observed_pb_covidestim}}\label{fig:unnamed-chunk-90}
\end{figure}
\hypertarget{takeaways}{%
\subsubsection{Takeaways}\label{takeaways}}

The aim of the cross correlation analysis was to add another source of comparison for the county-level counts from an entirely different source of data -- in particular, a source of data that is less impacted by access to testing or test behavior. We see that in most counties considered here, there is high agreement between the time series. An avenue for future exploration would be to consider this analysis among a broader set of counties to see which time series tends to be most highly correlated with wastewater concentrations, a question that we cannot confidently address here when looking only at counties in Massachusetts.

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

\hypertarget{limitations-and-future-work}{%
\section{Limitations and Future Work}\label{limitations-and-future-work}}

\hypertarget{population-estimates}{%
\subsection{Population Estimates}\label{population-estimates}}

One limitation of our approach is that we treat the total number of tests as \(N_{\text{tested}}\), that is, the total number of people tested. If there is a substantial amount of repeat testing in a location, then the number of tests will be larger than \(N_{\text{tested}}\), and then \(N_{\text{untested}}\) will be too small, since \(N_{\text{tested}} = N- N_{\text{tested}}\). This may lead to underestimation of the true number of infections. At the state-level, one way to mitigate this issue is to use the ``people viral positive'' and ``people viral total'' values from the Johns Hopkins govex repository\footnote{As of May 2023, this repository is accessible \href{https://github.com/govex/COVID-19}{here}.} on Github, which some states report. However, there are many more data quality issues with this data.

Another limitation is our use of Census population estimates as the total population for a location.
In particular, college students, one of the most frequently tested populations -- are supposed to be counted ``where they live and sleep most of the time'' ({``College {Towns Depend} on {Accurate Count} of {Students Living} in {Area},''} n.d.). However, there have been concerns on institutional counts (e.g., for dorms) for recent Census estimates (Henderson, 2022). Additionally, students may leave the state for extended periods in the summer or during winter break, resulting in a change in the total population during those times.

\hypertarget{correlation-between-alpha-and-beta}{%
\subsection{\texorpdfstring{Correlation Between \(\alpha\) and \(\beta\)}{Correlation Between \textbackslash alpha and \textbackslash beta}}\label{correlation-between-alpha-and-beta}}

When we sample \(\alpha\), \(\beta\), and \(P(S_1|\text{untested})\), we sample them independently (before melding, which does introduce dependence). However, \(\alpha=\frac{P(\text{test}_+|S_1, \text{untested})}{P(\text{test}_+| \text{tested})}\) and \(\beta=\frac{P(\text{test}_+|S_0, \text{untested})}{P(\text{test}_+| \text{tested})}\) are not truly independent; they depend on the same set of conditional probabilities. That said, their dependence depends on probabilities we do not know, and we would need additional assumptions to specify the correlation between them. Simulations exploring implications of different correlations between \(\alpha\) and \(\beta\) would be useful to explore the extent to which correlation between these parameters changes the corrected estimates.

\hypertarget{using-results-for-other-sensitivity-analyses}{%
\subsection{Using Results for Other Sensitivity Analyses}\label{using-results-for-other-sensitivity-analyses}}

Applying probabilistic analysis gives us a distribution for the estimated infections for each two-week interval in a given location. These distributions are interesting in themselves, but another way we could use these results is to see how estimates of various models differ under various scenarios of incomplete testing. For example, a model assessing the relationship between various socioeconomic variables and cumulative infections may be impacted by incomplete testing in that the same disadvantages that may contribute to higher COVID-19 burden (inadequate health care resources, for instance) may contribute to a lack of accessible testing. Fitting a model like that J. T. Chen \& Krieger (2021b) or Karmakar, Lantz, \& Tipirneni (2021b), both of which study the relationship between COVID-19 infections and socioeconomic variables, and seeing how parameter estimates change when using sampled bias-corrected estimates of infection burden, rather than solely positive tests, would be informative for thinking how sensitive these parameter estimates are to the extent of incomplete testing. This process of fitting a model by sampling from the bias-corrected values is a common application of probabilistic bias analysis (Lash, Fox, \& Fink, 2009b).

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

The aim of this work is to consider possible scenarios for the extent of unobserved infections over an extended time during the COVID-19 pandemic, and to explore how we can present the uncertainty in the number of incident infections. Throughout the pandemic we often see line charts of observed cases or the test positivity rate. Advice has changed as has testing behavior, with warnings to not consider case counts in isolation, but rather to also look at trends in the test positivity rate (among other indicators). However, presentation of infections as intervals, their widths defined as a direct consequence of assumptions we make about the bias parameters, reflects genuine uncertainty about the number of true infections that may exist in a given area over time.

Various models exist to try to get at this quantity of the number of true infections, incorporating a range of sources of data, including COVID-19 deaths, hospitalizations, seroprevalence data, and viral concentrations in wastewater, as well as estimates such as the infection fatality ratio.
The strength of applying probabilistic bias analysis to consider possible values of true COVID-19 infections lays in its relative simplicity and transparency of assumptions, in addition to the ease of exploration of possible testing scenarios of the extent to which infections are going undetected. Although there is no ground truth to rigorously assess the accuracy of an estimate of the true number of COVID-19 infections, comparing approaches and understanding where and when they are non concordant provides useful insight into quantifying the range of true infections.

\appendix

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{loess}{%
\section{LOESS Smoothing}\label{loess}}

\hypertarget{fitting-the-loess-curve}{%
\subsection{Fitting the LOESS Curve}\label{fitting-the-loess-curve}}

To introduce some notation for the model at hand, we have a dependent variable \(\mathbf y\) and independent variable \(\mathbf x\), where \(\mathbf y\) and \(\mathbf x\) are related by some unknown function \(g\), that is, \(y = g(x) + \boldsymbol \epsilon\)\footnote{Recall we use bold type for vectors, e.g., \(\mathbf x \in \mathbb R^n\) is a vector with observations \(x_i \in \mathbb R\).}. When we want to use LOESS smoothing to estimate \(g\), often this function is complex, so we break up the problem into estimating a set of local regressions.

To obtain a predicted value \(\hat g(x^*)\) for a particular value of the independent variable \(x^*\), we fit a polynomial with greatest weight placed on points in the neighborhood of \(x^*\), where the width of this neighborhood is defined by the choice of smoothing span. Let \(\alpha \in (0,1]\) denote the chosen smoothing span.

For a particular value of \(x^*\), we estimate the predicted value \(\hat g(x^*)\) by fitting a local regression. We first compute the weights by computing the vector of distances from this point \(x^*\), that is,

\[\Delta (x^*) = |\mathbf x -x^* | \]

We define \(q = \text{floor}(\alpha n)\), and take \(\Delta_q(x^*) \in \mathbb R\) to be the \(q^{th}\) smallest distance of \(\Delta (x^*)\).

The vector of weights is then
\[T(\Delta(x^*), \Delta_q(x^*))\]

where \(T\) is the tricube weight function given by

\[
T(x) = \begin{cases} (1-(x)^3)^3 \hspace{9 mm}  \text{ for } |x| < 1\\
0  \hspace{25 mm} \text{ for $|x|$ } \geq 1 \end{cases}.
\]
Essentially, this process gives weight to points in the neighborhood of \(x^*\). When we consider \(x^* = 500\) and \(\text{smoothing span} = \alpha = .2\), the the weights we obtain are given in Figure \ref{fig:weights}.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/weights} 

}

\caption{\label{fig:weights} The only values with nonzero weights are those within the interval $(500 - \alpha (n), 500 - \alpha (n))$. That is, the proportion $\alpha$ of the data points closest to $x^*$ will have nonzero weights.}\label{fig:unnamed-chunk-93}
\end{figure}
\newpage

We fit a linear regression with polynomial terms, typically with degree up to 2, with these weights. For example, fitting the model for this same \(x^*=500\), we obtain the polynomial in Figure \ref{fig:ex-poly}.
\begin{flushleft}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-94-1} 

}

\caption{\label{fig:ex-poly}}\label{fig:unnamed-chunk-94}
\end{figure}
\end{flushleft}
By fitting the model for every point in \(\mathbf x\), we obtain the smoothed line shown in red in Figure \ref{fig:loess-all}.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/loess} 

}

\caption{\label{fig:loess-all}}\label{fig:unnamed-chunk-96}
\end{figure}
\hypertarget{bounded-density-transform}{%
\section{Transformation Approach to Bounded Density Estimation}\label{bounded-density-transform}}

Since \(P(S_0|\text{untested}, \text{test}_+)\) is a probability hence is bounded between 0 and 1, we will focus on thinking about density estimation when the support is the unit interval.

Let \(X \in [0,1]\). Since we want to transform it to the range \((-\infty,\infty)\), we can let \(Y = \text{logit}(X) = \log \left( \frac{X}{1-X} \right)\).

We know if we have \(X = g(Y)\), then we can acquire the distribution of \(X\) from that of \(Y\) by considering the change of variables of the probability density functions \(f_X\) and \(f_y\) given by
\[f_X(x) = f_Y(g^{-1}(X)) \;\; \left| \frac{d}{dx} g^{-1}(X) \right|. \tag{1}\]

Thus, in this case, we have \(Y = \text{logit}(X)\), so \(g^{-1}\) is the logit function. By definition of the change of variables formula (1), we have
\[f_X(x) = f_Y(\text{logit}(X)) \;\; \left| \frac{d}{dx} \text{logit}(X) \right|.\]
Computing the derivative and simplifying, we have
\begin{align*} &= f_Y(logit(X)) \;\; \left| \frac{d}{dx}log(\frac{x}{1-x}) \right|\\
&= f_Y(logit(X)) \;\; \left| \left(\frac{1-x}{x} \right) (x(1-x)^{-1})' \right|\\ 
&= f_Y(logit(X)) \;\; \left| \left(\frac{1-x}{x} \right) ((1-x)^{-1} + x(1-x)^{-2} ) \right|\\
&= f_Y(logit(X)) \;\; \left| \left(\frac{1-x}{x} \right) \left(\frac{(1-x) + x }{ (1-x)^{2} }\right) \right|\\
&= f_Y(logit(X)) \;\; \left| \left(\frac{1-x}{x} \right) \left(\frac{1 }{ (1-x)^{2} }\right) \right|\\
&= f_Y(logit(X)) \;\; \left|  \frac{1 }{ x (1-x) } \right|.
\end{align*}
This means that we compute \(Y = logit(X)\) and then estimate the density of the unbounded variable \(Y\), and then we can recover the density \(f_X\) by multiplying by \(\frac{1 }{ x (1-x) }\).

In some cases, this approach works well. In Figure \ref{fig:trans}, we simulate a variable \(X \sim Beta(3,2)\) and estimate the density with the transformation approach.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-100-1} 

}

\caption{\label{fig:trans}}\label{fig:unnamed-chunk-100}
\end{figure}
We see the difference between using the transformation approach versus estimating the density of \(X\) without first transforming it to be unbounded in Figure \ref{fig:original}.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/create-fig-original-1} 

}

\caption{\label{fig:original}}\label{fig:create-fig-original}
\end{figure}
However, when we simulate densities that have greater mass toward the boundaries 0 or 1, we see that boundary bias becomes problematic (Figure \ref{fig:compare-beta-params}). This is evident in panels B, C, D, and G of Figure \ref{fig:compare-beta-params}, where the estimated density near the boundaries is a poor estimate of the true density.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-102-1} 

}

\caption{\label{fig:compare-beta-params}}\label{fig:unnamed-chunk-102}
\end{figure}
\newpage

\hypertarget{betakernel}{%
\section{Beta Kernel Density Estimation}\label{betakernel}}

As defined in S. X. Chen (1999), the most simple beta kernel estimator would be
\[\hat f_1(x) = \dfrac{\sum_{i=1}^n K_{x/b + 1, \; (1-x)/b + 1} (X_i)}{n}\]

where \(K_{\text{shape1}, \text{shape2}}\) is the density function \(Beta(shape1, \; shape2)\).

However, S. X. Chen (1999) show that the modified beta kernel estimator \(\hat f_2(x)\) has lower variance and bias than \(\hat f_1\), where we define \(\hat f_2\) as follows:

\[
\hat f_2(x)  = \dfrac{\sum_{i=1}^n K_{x,b}^*(X_i)}{n},\]

\[K^*_{x,b} = \begin{cases}K_{x/b, \; (1-x)/b }(t)  & \text{if }x \in [2b,1-2b] \\
K_{\rho(x), \; (1-x)/b } (t)  & \text{if } x \in [0,2b) \\
K_{x/b, \; \rho(1-x)}(t) & \text{if } x\in(1-2b,1]
\end{cases},
\]
\[\rho(x,b) = 2b^2 + 2.5 - \sqrt{b^2 + 6b^2 +2.25-x^2 -x/b}.\]

Notably, for beta kernel estimators, the shape of the kernel depends on \(x\) (Figure \ref{fig:depends-on-x}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-104-1} 

}

\caption{\label{fig:depends-on-x}}\label{fig:unnamed-chunk-104}
\end{figure}
As we did in Figure \ref{fig:compare-beta-params}, we can compare the performance of the beta kernel \(\hat f_2\) for estimating the density of samples from different beta distributions (Figure \ref{fig:comp-beta}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-106-1} 

}

\caption{\label{fig:comp-beta}}\label{fig:unnamed-chunk-106}
\end{figure}
\hypertarget{total-tests-and-positive-tests-across-all-states}{%
\section{Total Tests and Positive Tests Across All States}\label{total-tests-and-positive-tests-across-all-states}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/test_capacity_all_states} 

}

\caption{\label{fig:test-capacity-all-states} Percent change in positive tests and the percent change in total tests from March 2021 through the end of February 2022. We see in the period from July through September that the positive tests consistently increase at a higher rate than the total tests; this is related to the peaks we observe in the probabilistic bias intervals.}\label{fig:unnamed-chunk-107}
\end{figure}
\hypertarget{comparing-johns-hopkins-csse-cases-to-cdc-positive-tests}{%
\section{Comparing Johns Hopkins CSSE Cases to CDC Positive Tests}\label{comparing-johns-hopkins-csse-cases-to-cdc-positive-tests}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/jhu_cdc_all_states_1} 

}

\caption{\label{fig:jhu-cdc-all}Comparing the data sources for the probablistic bias analysis and the Covidestim model. The two solid lines are the data sources: the data source for the probabilistic analysis is PCR tests from the CDC's Diagnostic Laboratory Testing Time series, while the data source for the Covidestim model is Johns Hopkins CSSE case counts. Each is summed across 2-week intervals time scale. The test positivity is the positive tests from the CDC source over total tests for the two-week interval. Looking at trends in the test positivity are informative for better understanding when Covidestim estimates lag the probabilistic bias intervals. This figure contains half of the states; the next half are in the following figure.}\label{fig:unnamed-chunk-108}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/jhu_cdc_all_states_2} 

}

\caption{\label{fig:jhu-cdc-all2} This figure provides the same information as \ref{fig:jhu-cdc-all} for the second half of the states.}\label{fig:unnamed-chunk-109}
\end{figure}
\hypertarget{first-implementation-of-probabilistic-bias-analysis-including-all-states}{%
\section{First Implementation of Probabilistic Bias Analysis (Including All States)}\label{first-implementation-of-probabilistic-bias-analysis-including-all-states}}

Some states have insufficient data from the COVID-19 Trends and Impact Survey to run the implementations that center the distributions of \(\beta\) and/or \(P(S_1|\text{untested})\) at the survey values (versions 2 through 4). However, because testing data is available for all states, the implementation that does not inform priors based on survey data is possible for all states in the U.S. (Figure \ref{fig:all-states-first-impl}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/all-states-first-impl} 

}

\caption{\label{fig:all-states-first-impl}  Probabilistic bias analysis 95\% simulation intervals for each two-week interval and state for the implementation where priors do not vary by location or time-interval.}\label{fig:unnamed-chunk-110}
\end{figure}
\hypertarget{county-level-comparison-to-covidestim-in-massachusetts}{%
\section{County Level Comparison to Covidestim in Massachusetts}\label{county-level-comparison-to-covidestim-in-massachusetts}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/ma_pb_compared_to_covidestim} 

}

\caption{\label{fig:ma-comp-covidestim}Probabilistic bias analysis intervals for each county in Massachusetts. Each probabilistic bias interval is a 95\% simulation interval for the estimated infections in that particular two-week interval. Each column represents a distinct implementation of the probabilistic bias analysis, where columns 2 through 4 incorporate survey data from the COVID-19 Trends and Impact Survey for informing the prior distributions. The Covidestim medians are presented in blue to compare concordance between each implementation of the bias analysis to Covidestim estimates.}\label{fig:unnamed-chunk-111}
\end{figure}
\hypertarget{full-set-of-results-at-the-county-level-for-michigan}{%
\section{Full Set of Results at the County Level for Michigan}\label{full-set-of-results-at-the-county-level-for-michigan}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mi1_pb_compared_to_observed} 

}

\caption{\label{fig:pb_versions_mi-1} Probabilistic bias intervals for each implementation of probabilistic bias analysis, for each county in Michigan. Each interval is a 95\% simulation interval for the total number of infections in that county in that two-week interval. The counties in Michigan are divided into thirds; this figure shows the first third, and the following two figures show the remaining counties.}\label{fig:unnamed-chunk-112}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mi2_pb_compared_to_observed} 

}

\caption{\label{fig:pb_versions_mi-2} This figure is identical to \ref{fig:pb_versions_mi-1} but shows the second third of counties in Michigan.}\label{fig:unnamed-chunk-113}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mi3_pb_compared_to_observed} 

}

\caption{\label{fig:pb_versions_mi-3} This figure is identical to \ref{fig:pb_versions_mi-1} but shows the last third of counties in Michigan.}\label{fig:unnamed-chunk-114}
\end{figure}
\hypertarget{concordance-with-covidestim-by-version-and-county-in-michigan}{%
\section{Concordance with Covidestim by Version and County in Michigan}\label{concordance-with-covidestim-by-version-and-county-in-michigan}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mi_pb_compared_to_covidestim_proportions} 

}

\caption{\label{fig:mi-county-concordance} Considering the proportion of probabilistic bias intervals that contained the Covidestim median for each implementation, for each county in Michigan. For most counties, we see that the implementation centering $P(S_1|\text{untested})$ at the survey value is most concordant with Covidestim.}\label{fig:unnamed-chunk-115}
\end{figure}
\hypertarget{ratio-of-estimated-infections-to-observed-in-michigan-counties-by-population-size}{%
\section{Ratio of Estimated Infections to Observed in Michigan Counties, by Population Size}\label{ratio-of-estimated-infections-to-observed-in-michigan-counties-by-population-size}}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mi-ratio-pop} 

}

\caption{\label{fig:mi-ratio-pop}Comparing the ratio of estimated infections to observed infections over time for counties of different population sizes in Michigan. Counties are split into four groups based on quartiles of the population size. Each point represents the ratio of the estimated infections to the observed infections for a county. Although the group with the smallest population size does have more outliers with very high ratios, the median ratios are highly similar across population sizes.}\label{fig:unnamed-chunk-116}
\end{figure}
\newpage

\hypertarget{alpha-melding}{%
\section{\texorpdfstring{Lack of Change in \(\alpha\) with Melding}{Lack of Change in \textbackslash alpha with Melding}}\label{alpha-melding}}

We see in any figure that shows the pre and post melding distributions that \(\alpha\) undergoes little change.

Recalling the function \(M\) we use to relate \(\alpha,\beta,\Pr(S_1|\text{untested})\) to the asymptomatic rate,

\[\Pr(S_0|\text{test}_+, \text{untested}) = M(\theta)  = \dfrac{\beta (1- \Pr(S_1|\text{untested}))}{\beta(1- \Pr(S_1|\text{untested})) + \alpha(\Pr(S_1|\text{untested})},\]

we note that there are two reasons that this function is not as sensitive to changes in \(\alpha\). For one, most of the density of \(\Pr(S_1|\text{untested})\) is at very small values (less than 0.05), which means that even if \(\alpha\) changes substantially, it will be multiplied by a very small value and thus will have a relatively small impact on \(M(\theta)\). Simulations revealed that changing the prior for \(\Pr(S_1|\text{untested})\) to be shifted higher increased the extent to which \(\alpha\) would change after melding.

The other reason is simply the form of the function itself, where \(\beta\) is in both the numerator and denominator, while \(\alpha\) is only a term in the denominator.

Below, we see that within the range of values of \(\alpha\) and \(\beta\) that will be sampled with the specification of the priors (considering \(\beta\) in the range \((0.01,0.4)\) and \(\alpha\) in the range \((0.7,1.3)\), \(M(\theta)\) is much more sensitive to changes in \(\beta\) across this range than changes in \(\alpha\). We hold \(\Pr(S_1|\text{untested})\) constant for the purposes of this figure.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../presentation/figure/alpha_little_change} 

}

\caption{\label{fig:alpha-little-change}Holding $\Pr(S_1|\text{untested})$ constant and considering $\alpha \in (0.7,1.3)$ and $\beta \in (0.01,0.4)$, where these ranges contain all values where the prior distribution has nontrivial density, we see that $M(\theta)$ is much more sensitive to changes in $\beta$ than to changes in $\alpha$. A consequence of this is the lack of change we see in $\alpha$ in the post-melding distributions.}\label{fig:unnamed-chunk-117}
\end{figure}
\hypertarget{conservativeintervals}{%
\section{\texorpdfstring{Relationship Between \((X+Y)_\alpha\) and \(X_{\alpha}\) +\(Y_{\alpha}\) for Dependent Variables \(X,Y\)}{Relationship Between (X+Y)\_\textbackslash alpha and X\_\{\textbackslash alpha\} +Y\_\{\textbackslash alpha\} for Dependent Variables X,Y}}\label{conservativeintervals}}

\hypertarget{simulation-bivariate-normal}{%
\subsection{Simulation: Bivariate Normal}\label{simulation-bivariate-normal}}

We can see this in a concrete example. Let \((X,Y)\) be bivariate normal with \(\boldsymbol \mu = \begin{pmatrix} 0\\0\end{pmatrix}\) and correlation matrix \(\boldsymbol \Sigma = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}\), and hence where \(X, Y\) are marginally standard normal random variables.

We let the subscript \(\alpha\) denote the \(\alpha^{th}\) and the subscript \(1-\alpha\) denote the \((1-\alpha)^{th}\) quantile of the distribution.

In Figure \ref{fig:simmvn}, in each panel, we increase the correlation \(\rho\) between \(X\) and \(Y\) by 0.25 units and plot the sum \(X +Y\) against \(X\). The vertical lines represent quantiles \(X_{0.025}\) and \(X_{0.975}\), and the horizontal lines represent the quantiles \((X+Y)_{0.025}\) and \((X+Y)_{0.975}\).

We see in Figure \ref{fig:simmvn} that when we increase the correlation between \(X\) and \(Y\), the width of the interval \(\Big((X+Y)_\alpha, (X+Y)_{1-\alpha}\Big)\) increases.
\begin{figure}

{\centering \includegraphics[width=0.96\linewidth]{thesis_files/figure-latex/unnamed-chunk-118-1} \includegraphics[width=0.96\linewidth]{thesis_files/figure-latex/unnamed-chunk-118-2} \includegraphics[width=0.96\linewidth]{thesis_files/figure-latex/unnamed-chunk-118-3} \includegraphics[width=0.96\linewidth]{thesis_files/figure-latex/unnamed-chunk-118-4} \includegraphics[width=0.96\linewidth]{thesis_files/figure-latex/unnamed-chunk-118-5} 

}

\caption{\label{fig:simmvn}}\label{fig:unnamed-chunk-118}
\end{figure}
In Figure \ref{fig:comp-intervals}, we compare the intervals defined by taking the quantiles of the sum, \(\Big((X+Y)_\alpha, (X+Y)_{1-\alpha}\Big)\), to the intervals taken by summing the quantiles individually, \(\Big(X_\alpha +Y_\alpha, \; X_{1-\alpha} +Y_{1-\alpha}\Big)\). We notice that, as we saw in Figure \ref{fig:simmvn}, increasing the correlation increases the width of the interval \(\Big((X+Y)_\alpha, (X+Y)_{1-\alpha}\Big)\), while the interval \(\Big(X_\alpha +Y_\alpha, \; X_{1-\alpha} +Y_{1-\alpha}\Big)\) is constant since changing the correlation does not change the marginal quantiles \(X_\alpha, X_{1-\alpha}\).,
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-119-1} 

}

\caption{\label{fig:comp-intervals}}\label{fig:unnamed-chunk-119}
\end{figure}
As we see in Figure \ref{fig:comp-intervals}, the intervals are identical when \(X,Y\) are perfectly correlated. This result is not dependent on the choice of distribution, as we can show by considering CDFs and quantile functions of a general distribution.
\begin{tcolorbox}[title = Quantiles of the Sum of Perfectly Correlated Random Variables]
When two random variables $X$ and $Y$ are perfectly correlated,
$$X_\alpha + Y_\alpha = (X+Y)_\alpha.$$
\end{tcolorbox}
When \(X\) and \(Y\) are perfectly correlated, \(Y\) must be a linear combination of \(X\), so we can write \(X+Y= X+bX=(1+b)X\).

Then, let the \(\alpha^{th}\) quantile of \((1+b)X\) be \(x_\alpha\). By definition of the quantile function, we have

\[F^{-1}_{(1+b) X } (\alpha) = x_\alpha \implies P((1+b) X \leq x_\alpha) = \alpha.\]
Since \((1+b)\) is just a constant, we can divide to yield

\[P\Big( X \leq x_\alpha/(1+b) \Big) = \alpha.\]
To obtain the quantile for \(bX\), we can multiply each side by \(b\) to yield
\[P\Big( bX \leq bx_\alpha/(1+b) \Big) = \alpha.\]
Putting these results together, we have
\begin{align*}
F^{-1}_X+F^{-1}_Y = F^{-1}_{bX} (\alpha) + F^{-1}_{X} (\alpha) \\
&= \frac{bx_\alpha } { 1+b} + \frac{x_\alpha}{1+b}
&= x_\alpha \\
&= F^{-1}_{(1+b)X}(\alpha)\\
&=F^{-1}_{X+Y}(\alpha) \end{align*}

\hypertarget{derivation-of-the-distribution-of-xy-for-bivariate-normal}{%
\subsection{Derivation of the Distribution of X+Y for Bivariate Normal}\label{derivation-of-the-distribution-of-xy-for-bivariate-normal}}

We can see why we observe this relationship between intervals in \ref{fig:comp-intervals}, where the 95\% intervals \(X_\alpha +Y_\alpha\) are wider than \((X+Y)_\alpha\) based on the the sum of the \(\alpha^{th}\) quantiles of the individual distributions, \(X_\alpha + Y_\alpha\), and the intervals based on the \(\alpha^{th}\) quantile of the distribution of \(X+Y\) by considering the definition of the quantile function of the normal distribution.

Defining \(Z=g(X,Y) = X+Y\), we can obtain the density function by a change of variables. Notice if \(g(X,Y) = X+Y\), \(g^{-1}(X,Z) = Z-X\), so we have
\begin{align*} f_{X,Z}(x,z) &= f_{X,Y}(x,g^{-1}(x,z)) \left|\frac{\partial g^{-1}(x,z)}{\partial z}\right| \\
f_{X,Z}(x,z) &= f_{X,Y}(x,z-x) \left|\frac{\partial (x-z)}{\partial z}\right|\\
f_{X,Z}(x,z) &= f_{X,Y}(x,z-x) \left|1\right|\\
f_{X,Z}(x,z) &= f_{X,Y}(x,z-x) \\
\end{align*}
Then, we can marginalize out \(X\) to get the PDF of \(f_Z\) by taking

\[f_Z(z) = \int_{\infty}^\infty f(x,z-x) \; dx.\]

Since \((X,Y)\) is bivariate normal with correlation \(\rho\), the PDF is given by

\[f(x,y) = \dfrac{exp\left[\dfrac{-1}{2(1-\rho^2)} \left( \dfrac{(x-\bar x)^2}{\sigma_x^2}+\dfrac{(y-\bar y)^2}{\sigma_x^2} - \dfrac{2 \rho (x-\bar x)(y-\bar y)}{\sigma_x\sigma_y} \right)\right]}{2\pi \sigma_x \sigma_y \sqrt{1- \rho^2}}\]

Integrating with respect to \(x\)\footnote{This integration is extremely long and technical, so we do not include it here.}, we have

\[f_Z(z)  = \int_{-\infty}^\infty \dfrac{\exp\left[\dfrac{-1}{2(1-\rho^2)} \left( \dfrac{(x-\bar x)^2}{\sigma_x^2}+\dfrac{(y-\bar y)^2}{\sigma_x^2} - \dfrac{2 \rho (x-\bar x)(z-x-\bar y)}{\sigma_x\sigma_y} \right)\right]}{2\pi \sigma_x \sigma_y \sqrt{1- \rho^2}} dx \]
\[=\dfrac{\exp\left[-\dfrac{(z-(\bar x + \bar y ))^2}{2(\sigma^2_x+\sigma^2_y + 2\rho \sigma_x \sigma_y)}\right]}{\sqrt{2\pi(\sigma_x^2 + \sigma_y^2 + 2\rho \sigma_x \sigma_y)}}.\]
It follows that \(Z\) is a normal random variable with mean \(\bar x + \bar y\) and standard deviation \(\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y }\).

In Figure \ref{fig:ex-sim-normal}, we plot the density estimate of the distribution of \(X+Y\) for \((X,Y) \sim MVN\left( \begin{pmatrix} 0\\0 \end{pmatrix}, \begin{pmatrix} 1 & 0.2 \\0.2 & 1 \end{pmatrix}\right)\) and plot the density of the random variable \(X+Y = Z \sim N\left(\bar x + \bar y,\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y }\right)\) and see they are in close alignment, as expected.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-122-1} 

}

\caption{\label{fig:ex-sim-normal} The theoretical density of $N\left(\bar x + \bar y,\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y }\right)$ is plotted in red over the kernel density estimate of the observed distribution of $X+Y$.}\label{fig:unnamed-chunk-122}
\end{figure}
Since we now know \(Z \sim N\left(\bar x + \bar y,\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y }\right)\), we can consider the quantile function of the normal distribution, which is defined as

\[F_Z^{-1}(\alpha)=\mu +\sigma_Z \; \text{erf}^{-1}(2\alpha - 1).\]
and since \(\sigma_Z=\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y }\) we have
\[F_Z^{-1}(\alpha)=\mu + \left(\sqrt{\sigma_x^2 +\sigma_y^2 + 2 \rho \sigma_x \sigma_y } \right) \; \text{erf}^{-1}(2\alpha - 1).\]
Now, we note the inverse error function \(\text{erf}^{-1}\) is increasing (Figure \ref{fig:erf}).

This means if \(\alpha > 0.5\), \(F_Z^{-1}\) is increasing with increasing values of \(\rho\), and if \(\alpha < 0.5\), \(F_Z^{-1}\) is decreasing with increasing values of \(\rho\).

This means that if we have a pair of correlated random variables \((X_1,Y_1)\) and \((X_2,Y_2)\) and \(\rho_{X_1,Y_1} > \rho_{X_2,Y_2}\) and consider \(\alpha < 0.5\),

\[(X_1+Y_1)_\alpha <(X_2+Y_2)_\alpha\]

and

\[(X_1+Y_1)_{1-\alpha} > (X_2+Y_2)_{1-\alpha}.\]
This is exactly what we observed in Figure \ref{fig:comp-intervals}: when we increased the correlation, we saw the bounds of the 95\% interval increase.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-123-1} 

}

\caption{\label{fig:erf}}\label{fig:unnamed-chunk-123}
\end{figure}
\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\noindent

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-andrews2022}{}}%
Andrews, N., Stowe, J., Kirsebom, F., Toffa, S., Rickeard, T., Gallagher, E., \ldots{} Lopez Bernal, J. (2022). Covid-19 {Vaccine Effectiveness} against the {Omicron} ({B}.1.1.529) {Variant}. \emph{New England Journal of Medicine}, \emph{386}(16), 1532--1546. http://doi.org/\href{https://doi.org/10.1056/NEJMoa2119451}{10.1056/NEJMoa2119451}

\leavevmode\vadjust pre{\hypertarget{ref-aurelienpelissier2022}{}}%
Aurelien Pelissier. (2022, February 4). Density {Estimation} for {Bounded Variables}. Retrieved March 19, 2023, from \url{https://medium.com/mlearning-ai/density-estimation-for-bounded-variables-7d68f633e772}

\leavevmode\vadjust pre{\hypertarget{ref-blitzsteinIntroductionProbability2019}{}}%
Blitzstein, J. K., \& Hwang, J. (2019). \emph{Introduction to probability} (Second edition). {Boca Raton}: {CRC Press}.

\leavevmode\vadjust pre{\hypertarget{ref-californiadepartmentofpublichealth2021}{}}%
California Department of Public Health. (2021, June 15). Blueprint for a {Safer Economy}. Retrieved December 16, 2022, from \url{https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/COVID-19/COVID19CountyMonitoringOverview.aspx}

\leavevmode\vadjust pre{\hypertarget{ref-carvalho2023}{}}%
Carvalho, L. M., Villela, D. A. M., Coelho, F. C., \& Bastos, L. S. (2023). Bayesian {Inference} for the {Weights} in {Logarithmic Pooling}. \emph{Bayesian Analysis}, \emph{18}(1). http://doi.org/\href{https://doi.org/10.1214/22-BA1311}{10.1214/22-BA1311}

\leavevmode\vadjust pre{\hypertarget{ref-cdc2020}{}}%
CDC. (2020, February 11). Cases, data, and surveillance to track and analyze {COVID-19}. Retrieved April 12, 2023, from \url{https://www.cdc.gov/coronavirus/2019-ncov/covid-data/serology-surveillance/index.html}

\leavevmode\vadjust pre{\hypertarget{ref-chambers1997}{}}%
Chambers, J. M. (Ed.). (1997). \emph{Statistical models in {S}} (Reprint). {London}: {Chapman \& Hall}.

\leavevmode\vadjust pre{\hypertarget{ref-chandler2021}{}}%
Chandler, C. M., Bourassa, L., Mathias, P. C., \& Greninger, A. L. (2021). Estimating the {False-Positive Rate} of {Highly Automated SARS-CoV-2 Nucleic Acid Amplification Testing}. \emph{Journal of Clinical Microbiology}, \emph{59}(8), e01080--21. http://doi.org/\href{https://doi.org/10.1128/JCM.01080-21}{10.1128/JCM.01080-21}

\leavevmode\vadjust pre{\hypertarget{ref-charlesd.baker2021}{}}%
Charles D. Baker. (2021). {COVID-19 Order No}. 65. Retrieved from \url{https://www.mass.gov/doc/covid-19-order-65/download}

\leavevmode\vadjust pre{\hypertarget{ref-chen2021a}{}}%
Chen, J. T., \& Krieger, N. (2021a). Revealing the {Unequal Burden} of {COVID-19} by {Income}, {Race}/{Ethnicity}, and {Household Crowding}: {US County Versus Zip Code Analyses}. \emph{Journal of Public Health Management and Practice}, \emph{27}, S43--S56. http://doi.org/\href{https://doi.org/10.1097/PHH.0000000000001263}{10.1097/PHH.0000000000001263}

\leavevmode\vadjust pre{\hypertarget{ref-chen2021b}{}}%
Chen, J. T., \& Krieger, N. (2021b). Revealing the {Unequal Burden} of {COVID-19} by {Income}, {Race}/{Ethnicity}, and {Household Crowding}: {US County Versus Zip Code Analyses}. \emph{Journal of Public Health Management and Practice}, \emph{27}, S43--S56. http://doi.org/\href{https://doi.org/10.1097/PHH.0000000000001263}{10.1097/PHH.0000000000001263}

\leavevmode\vadjust pre{\hypertarget{ref-chen1999}{}}%
Chen, S. X. (1999). Beta kernel estimators for density functions. \emph{Computational Statistics \& Data Analysis}, \emph{31}(2), 131--145. http://doi.org/\href{https://doi.org/10.1016/S0167-9473(99)00010-9}{10.1016/S0167-9473(99)00010-9}

\leavevmode\vadjust pre{\hypertarget{ref-chen2022}{}}%
Chen, Y., Han, Y., Yang, J., Ma, Y., Li, J., \& Zhang, R. (2022). Impact of {SARS-CoV-2 Variants} on the {Analytical Sensitivity} of {rRT-PCR Assays}. \emph{Journal of Clinical Microbiology}, \emph{60}(4), e02374--21. http://doi.org/\href{https://doi.org/10.1128/jcm.02374-21}{10.1128/jcm.02374-21}

\leavevmode\vadjust pre{\hypertarget{ref-chitwood2022}{}}%
Chitwood, M. H., Russi, M., Gunasekera, K., Havumaki, J., Klaassen, F., Pitzer, V. E., \ldots{} Menzies, N. A. (2022). Reconstructing the course of the {COVID-19} epidemic over 2020 for {US} states and counties: {Results} of a {Bayesian} evidence synthesis model. \emph{PLOS Computational Biology}, \emph{18}(8), e1010465. http://doi.org/\href{https://doi.org/10.1371/journal.pcbi.1010465}{10.1371/journal.pcbi.1010465}

\leavevmode\vadjust pre{\hypertarget{ref-zotero-1057}{}}%
College {Towns Depend} on {Accurate Count} of {Students Living} in {Area}. (n.d.). Retrieved May 12, 2023, from \url{https://www.census.gov/library/stories/2020/01/student-housing-off-campus-with-parents-college-students-count-2020-census.html}

\leavevmode\vadjust pre{\hypertarget{ref-zotero-1003}{}}%
Commercial {Laboratory Seroprevalence Surveys} \textbar{} {Coronavirus} \textbar{} {COVID-19} \textbar{} {CDC}. (n.d.). Retrieved April 12, 2023, from \url{https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/commercial-lab-surveys.html}

\leavevmode\vadjust pre{\hypertarget{ref-cuadros2022c}{}}%
Cuadros, D. F., Moreno, C. M., Musuka, G., Miller, F. D., Coule, P., \& MacKinnon, N. J. (2022). Association {Between Vaccination Coverage Disparity} and the {Dynamics} of the {COVID-19 Delta} and {Omicron Waves} in the {US}. \emph{Frontiers in Medicine}, \emph{9}, 898101. http://doi.org/\href{https://doi.org/10.3389/fmed.2022.898101}{10.3389/fmed.2022.898101}

\leavevmode\vadjust pre{\hypertarget{ref-delrio2021}{}}%
Del Rio, C., Malani, P. N., \& Omer, S. B. (2021). Confronting the {Delta Variant} of {SARS-CoV-2}, {Summer} 2021. \emph{JAMA}, \emph{326}(11), 1001. http://doi.org/\href{https://doi.org/10.1001/jama.2021.14811}{10.1001/jama.2021.14811}

\leavevmode\vadjust pre{\hypertarget{ref-dirican2022}{}}%
Dirican, E., \& Bal, T. (2022). {COVID-19} disease severity to predict persistent symptoms: A systematic review and meta-analysis. \emph{Primary Health Care Research \& Development}, \emph{23}, e69. http://doi.org/\href{https://doi.org/10.1017/S1463423622000585}{10.1017/S1463423622000585}

\leavevmode\vadjust pre{\hypertarget{ref-centersfordiseasecontrolandprevention2020}{}}%
Disease Control and Prevention, C. for. (2020, March 28). {COVID Data Tracker}. Retrieved December 16, 2022, from \url{https://covid.cdc.gov/covid-data-tracker}

\leavevmode\vadjust pre{\hypertarget{ref-dong2020}{}}%
Dong, E., Du, H., \& Gardner, L. (2020). An interactive web-based dashboard to track {COVID-19} in real time. \emph{The Lancet Infectious Diseases}, \emph{20}(5), 533--534. http://doi.org/\href{https://doi.org/10.1016/S1473-3099(20)30120-1}{10.1016/S1473-3099(20)30120-1}

\leavevmode\vadjust pre{\hypertarget{ref-duvallet2022}{}}%
Duvallet, C., Wu, F., McElroy, K. A., Imakaev, M., Endo, N., Xiao, A., \ldots{} Matus, M. (2022). Nationwide {Trends} in {COVID-19 Cases} and {SARS-CoV-2 RNA Wastewater Concentrations} in the {United States}. \emph{ACS ES\&T Water}, \emph{2}(11), 1899--1909. http://doi.org/\href{https://doi.org/10.1021/acsestwater.1c00434}{10.1021/acsestwater.1c00434}

\leavevmode\vadjust pre{\hypertarget{ref-fall2022}{}}%
Fall, A., Eldesouki, R. E., Sachithanandham, J., Morris, C. P., Norton, J. M., Gaston, D. C., \ldots{} Mostafa, H. H. (2022). The displacement of the {SARS-CoV-2} variant {Delta} with {Omicron}: {An} investigation of hospital admissions and upper respiratory viral loads. \emph{eBioMedicine}, \emph{79}, 104008. http://doi.org/\href{https://doi.org/10.1016/j.ebiom.2022.104008}{10.1016/j.ebiom.2022.104008}

\leavevmode\vadjust pre{\hypertarget{ref-genest1986}{}}%
Genest, C., McConway, K. J., \& Schervish, M. J. (1986). Characterization of {Externally Bayesian Pooling Operators}. \emph{The Annals of Statistics}, \emph{14}(2), 487--501. Retrieved from \url{https://www.jstor.org/stable/2241231}

\leavevmode\vadjust pre{\hypertarget{ref-green2020}{}}%
Green, D. A., Zucker, J., Westblade, L. F., Whittier, S., Rennert, H., Velu, P., \ldots{} Sepulveda, J. L. (2020). Clinical {Performance} of {SARS-CoV-2 Molecular Tests}. \emph{Journal of Clinical Microbiology}, \emph{58}(8), e00995--20. http://doi.org/\href{https://doi.org/10.1128/JCM.00995-20}{10.1128/JCM.00995-20}

\leavevmode\vadjust pre{\hypertarget{ref-greenland2016}{}}%
Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., \& Altman, D. G. (2016). Statistical tests, {P} values, confidence intervals, and power: A guide to misinterpretations. \emph{European Journal of Epidemiology}, \emph{31}(4), 337--350. http://doi.org/\href{https://doi.org/10.1007/s10654-016-0149-3}{10.1007/s10654-016-0149-3}

\leavevmode\vadjust pre{\hypertarget{ref-harris2022a}{}}%
Harris, J. E. (2022). {COVID-19 Incidence} and hospitalization during the delta surge were inversely related to vaccination coverage among the most populous {U}.{S}. {Counties}. \emph{Health Policy and Technology}, \emph{11}(2), 100583. http://doi.org/\href{https://doi.org/10.1016/j.hlpt.2021.100583}{10.1016/j.hlpt.2021.100583}

\leavevmode\vadjust pre{\hypertarget{ref-henderson2022}{}}%
Henderson, T. (2022, June 1). Census {Bureau} to {Review Potential Miscounts} of {Institutions}. Retrieved May 12, 2023, from \url{https://stateline.org/2022/06/01/census-bureau-to-review-potential-miscounts-of-institutions/}

\leavevmode\vadjust pre{\hypertarget{ref-hopkins2023}{}}%
Hopkins, L., Persse, D., Caton, K., Ensor, K., Schneider, R., McCall, C., \& Stadler, L. B. (2023). Citywide wastewater {SARS-CoV-2} levels strongly correlated with multiple disease surveillance indicators and outcomes over three {COVID-19} waves. \emph{Science of The Total Environment}, \emph{855}, 158967. http://doi.org/\href{https://doi.org/10.1016/j.scitotenv.2022.158967}{10.1016/j.scitotenv.2022.158967}

\leavevmode\vadjust pre{\hypertarget{ref-jiang2022a}{}}%
Jiang, D. H., Roy, D. J., Pollock, B. D., Shah, N. D., \& McCoy, R. G. (2022). Association of stay-at-home orders and {COVID-19} incidence and mortality in rural and urban {United States}: A population-based study. \emph{BMJ Open}, \emph{12}(4), e055791. http://doi.org/\href{https://doi.org/10.1136/bmjopen-2021-055791}{10.1136/bmjopen-2021-055791}

\leavevmode\vadjust pre{\hypertarget{ref-kanji2021}{}}%
Kanji, J. N., Zelyas, N., MacDonald, C., Pabbaraju, K., Khan, M. N., Prasad, A., \ldots{} Tipples, G. (2021). False negative rate of {COVID-19 PCR} testing: A discordant testing analysis. \emph{Virology Journal}, \emph{18}(1), 13. http://doi.org/\href{https://doi.org/10.1186/s12985-021-01489-0}{10.1186/s12985-021-01489-0}

\leavevmode\vadjust pre{\hypertarget{ref-kao2023}{}}%
Kao, S.-Y. Z., Sharpe, J. D., Lane, R. I., Njai, R., McCord, R. F., Ajiboye, A. S., \ldots{} Ekwueme, D. U. (2023). Duration of {Behavioral Policy Interventions} and {Incidence} of {COVID-19} by {Social Vulnerability} of {US Counties}, {April}--{December} 2020. \emph{Public Health Reports}, \emph{138}(1), 190--199. http://doi.org/\href{https://doi.org/10.1177/00333549221125202}{10.1177/00333549221125202}

\leavevmode\vadjust pre{\hypertarget{ref-karmakar2021a}{}}%
Karmakar, M., Lantz, P. M., \& Tipirneni, R. (2021a). Association of {Social} and {Demographic Factors With COVID-19 Incidence} and {Death Rates} in the {US}. \emph{JAMA Network Open}, \emph{4}(1), e2036462. http://doi.org/\href{https://doi.org/10.1001/jamanetworkopen.2020.36462}{10.1001/jamanetworkopen.2020.36462}

\leavevmode\vadjust pre{\hypertarget{ref-karmakar2021b}{}}%
Karmakar, M., Lantz, P. M., \& Tipirneni, R. (2021b). Association of {Social} and {Demographic Factors With COVID-19 Incidence} and {Death Rates} in the {US}. \emph{JAMA Network Open}, \emph{4}(1), e2036462. http://doi.org/\href{https://doi.org/10.1001/jamanetworkopen.2020.36462}{10.1001/jamanetworkopen.2020.36462}

\leavevmode\vadjust pre{\hypertarget{ref-kaufman2021}{}}%
Kaufman, B. G., Whitaker, R., Mahendraratnam, N., Hurewitz, S., Yi, J., Smith, V. A., \& McClellan, M. (2021). State variation in effects of state social distancing policies on {COVID-19} cases. \emph{BMC Public Health}, \emph{21}(1), 1239. http://doi.org/\href{https://doi.org/10.1186/s12889-021-11236-3}{10.1186/s12889-021-11236-3}

\leavevmode\vadjust pre{\hypertarget{ref-kojima2022a}{}}%
Kojima, N., Roshani, A., \& Klausner, J. D. (2022). Duration of {COVID-19 PCR} positivity for {Omicron} vs earlier variants. \emph{Journal of Clinical Virology Plus}, \emph{2}(3), 100085. http://doi.org/\href{https://doi.org/10.1016/j.jcvp.2022.100085}{10.1016/j.jcvp.2022.100085}

\leavevmode\vadjust pre{\hypertarget{ref-kortela2021a}{}}%
Kortela, E., Kirjavainen, V., Ahava, M. J., Jokiranta, S. T., But, A., Lindahl, A., \ldots{} Kekäläinen, E. (2021). Real-life clinical sensitivity of {SARS-CoV-2 RT-PCR} test in symptomatic patients. \emph{PLOS ONE}, \emph{16}(5), e0251661. http://doi.org/\href{https://doi.org/10.1371/journal.pone.0251661}{10.1371/journal.pone.0251661}

\leavevmode\vadjust pre{\hypertarget{ref-lash2009}{}}%
Lash, T. L., Fox, M. P., \& Fink, A. K. (2009a). \emph{Applying {Quantitative Bias Analysis} to {Epidemiologic Data}}. {New York, NY}: {Springer New York}. http://doi.org/\href{https://doi.org/10.1007/978-0-387-87959-8}{10.1007/978-0-387-87959-8}

\leavevmode\vadjust pre{\hypertarget{ref-lash2009b}{}}%
Lash, T. L., Fox, M. P., \& Fink, A. K. (2009b). \emph{Applying quantitative bias analysis to epidemiologic data}. {Dordrecht ; New York}: {Springer}.

\leavevmode\vadjust pre{\hypertarget{ref-liu2022}{}}%
Liu, Y., Yu, Y., Zhao, Y., \& He, D. (2022). Reduction in the infection fatality rate of {Omicron} variant compared with previous variants in {South Africa}. \emph{International Journal of Infectious Diseases}, \emph{120}, 146--149. http://doi.org/\href{https://doi.org/10.1016/j.ijid.2022.04.029}{10.1016/j.ijid.2022.04.029}

\leavevmode\vadjust pre{\hypertarget{ref-lu2020}{}}%
Lu, X., Wang, L., Sakthivel, S. K., Whitaker, B., Murray, J., Kamili, S., \ldots{} Lindstrom, S. (2020). {US CDC Real-Time Reverse Transcription PCR Panel} for {Detection} of {Severe Acute Respiratory Syndrome Coronavirus} 2. \emph{Emerging Infectious Diseases}, \emph{26}(8), 1654--1665. http://doi.org/\href{https://doi.org/10.3201/eid2608.201246}{10.3201/eid2608.201246}

\leavevmode\vadjust pre{\hypertarget{ref-ma2021}{}}%
Ma, Q., Liu, J., Liu, Q., Kang, L., Liu, R., Jing, W., \ldots{} Liu, M. (2021b). Global {Percentage} of {Asymptomatic SARS-CoV-2 Infections Among} the {Tested Population} and {Individuals With Confirmed COVID-19 Diagnosis}: {A Systematic Review} and {Meta-analysis}. \emph{JAMA Network Open}, \emph{4}(12), e2137257. http://doi.org/\href{https://doi.org/10.1001/jamanetworkopen.2021.37257}{10.1001/jamanetworkopen.2021.37257}

\leavevmode\vadjust pre{\hypertarget{ref-ma2021a}{}}%
Ma, Q., Liu, J., Liu, Q., Kang, L., Liu, R., Jing, W., \ldots{} Liu, M. (2021a). Global {Percentage} of {Asymptomatic SARS-CoV-2 Infections Among} the {Tested Population} and {Individuals With Confirmed COVID-19 Diagnosis}: {A Systematic Review} and {Meta-analysis}. \emph{JAMA Network Open}, \emph{4}(12), e2137257. http://doi.org/\href{https://doi.org/10.1001/jamanetworkopen.2021.37257}{10.1001/jamanetworkopen.2021.37257}

\leavevmode\vadjust pre{\hypertarget{ref-mallett2020a}{}}%
Mallett, S., Allen, A. J., Graziadio, S., Taylor, S. A., Sakai, N. S., Green, K., \ldots{} Halligan, S. (2020). At what times during infection is {SARS-CoV-2} detectable and no longer detectable using {RT-PCR-based} tests? {A} systematic review of individual participant data. \emph{BMC Medicine}, \emph{18}(1), 346. http://doi.org/\href{https://doi.org/10.1186/s12916-020-01810-8}{10.1186/s12916-020-01810-8}

\leavevmode\vadjust pre{\hypertarget{ref-mclaughlin2022a}{}}%
McLaughlin, J. M., Wiemken, T. L., Khan, F., \& Jodar, L. (2022). {US County-Level COVID-19 Vaccine Uptake} and {Rates} of {Omicron Cases} and {Deaths}. \emph{Open Forum Infectious Diseases}, \emph{9}(7), ofac299. http://doi.org/\href{https://doi.org/10.1093/ofid/ofac299}{10.1093/ofid/ofac299}

\leavevmode\vadjust pre{\hypertarget{ref-moreland2020}{}}%
Moreland, A., Herlihy, C., Tynan, M. A., Sunshine, G., McCord, R. F., Hilton, C., \ldots{} Popoola, A. (2020). Timing of {State} and {Territorial COVID-19 Stay-at-Home Orders} and {Changes} in {Population Movement} --- {United States}, {March} 1--{May} 31, 2020. \emph{MMWR. Morbidity and Mortality Weekly Report}, \emph{69}(35), 1198--1203. http://doi.org/\href{https://doi.org/10.15585/mmwr.mm6935a2}{10.15585/mmwr.mm6935a2}

\leavevmode\vadjust pre{\hypertarget{ref-mustafahellou2021}{}}%
Mustafa Hellou, M., Górska, A., Mazzaferri, F., Cremonini, E., Gentilotti, E., De Nardo, P., \ldots{} Paul, M. (2021). Nucleic acid amplification tests on respiratory samples for the diagnosis of coronavirus infections: A systematic review and meta-analysis. \emph{Clinical Microbiology and Infection}, \emph{27}(3), 341--351. http://doi.org/\href{https://doi.org/10.1016/j.cmi.2020.11.002}{10.1016/j.cmi.2020.11.002}

\leavevmode\vadjust pre{\hypertarget{ref-neyman1937}{}}%
Neyman, J. (1937). Outline of a {Theory} of {Statistical Estimation Based} on the {Classical Theory} of {Probability}. \emph{Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences}, \emph{236}(767), 333--380. http://doi.org/\href{https://doi.org/10.1098/rsta.1937.0005}{10.1098/rsta.1937.0005}

\leavevmode\vadjust pre{\hypertarget{ref-odriscoll2021}{}}%
O'Driscoll, M., Ribeiro Dos Santos, G., Wang, L., Cummings, D. A. T., Azman, A. S., Paireau, J., \ldots{} Salje, H. (2021). Age-specific mortality and immunity patterns of {SARS-CoV-2}. \emph{Nature}, \emph{590}(7844), 140--145. http://doi.org/\href{https://doi.org/10.1038/s41586-020-2918-0}{10.1038/s41586-020-2918-0}

\leavevmode\vadjust pre{\hypertarget{ref-olesen2021}{}}%
Olesen, S. W., Imakaev, M., \& Duvallet, C. (2021). Making waves: {Defining} the lead time of wastewater-based epidemiology for {COVID-19}. \emph{Water Research}, \emph{202}, 117433. http://doi.org/\href{https://doi.org/10.1016/j.watres.2021.117433}{10.1016/j.watres.2021.117433}

\leavevmode\vadjust pre{\hypertarget{ref-petersen2021}{}}%
Petersen, J. M., Ranker, L. R., Barnard-Mayers, R., MacLehose, R. F., \& Fox, M. P. (2021). A systematic review of quantitative bias analysis applied to epidemiological research. \emph{International Journal of Epidemiology}, \emph{50}(5), 1708--1730. http://doi.org/\href{https://doi.org/10.1093/ije/dyab061}{10.1093/ije/dyab061}

\leavevmode\vadjust pre{\hypertarget{ref-pitzer2021a}{}}%
Pitzer, V. E., Chitwood, M., Havumaki, J., Menzies, N. A., Perniciaro, S., Warren, J. L., \ldots{} Cohen, T. (2021). The {Impact} of {Changes} in {Diagnostic Testing Practices} on {Estimates} of {COVID-19 Transmission} in the {United States}. \emph{American Journal of Epidemiology}, \emph{190}(9), 1908--1917. http://doi.org/\href{https://doi.org/10.1093/aje/kwab089}{10.1093/aje/kwab089}

\leavevmode\vadjust pre{\hypertarget{ref-poole2000}{}}%
Poole, D., \& Raftery, A. E. (2000). Inference for {Deterministic Simulation Models}: {The Bayesian Melding Approach}. \emph{Journal of the American Statistical Association}, \emph{95}(452), 1244--1255. http://doi.org/\href{https://doi.org/10.1080/01621459.2000.10474324}{10.1080/01621459.2000.10474324}

\leavevmode\vadjust pre{\hypertarget{ref-powers2011}{}}%
Powers, K. A., Ghani, A. C., Miller, W. C., Hoffman, I. F., Pettifor, A. E., Kamanga, G., \ldots{} Cohen, M. S. (2011). The role of acute and early {HIV} infection in the spread of {HIV} and implications for transmission prevention strategies in {Lilongwe}, {Malawi}: A modelling study. \emph{The Lancet}, \emph{378}(9787), 256--268. http://doi.org/\href{https://doi.org/10.1016/S0140-6736(11)60842-8}{10.1016/S0140-6736(11)60842-8}

\leavevmode\vadjust pre{\hypertarget{ref-pulliam2022}{}}%
Pulliam, J. R. C., Van Schalkwyk, C., Govender, N., Von Gottberg, A., Cohen, C., Groome, M. J., \ldots{} Moultrie, H. (2022). Increased risk of {SARS-CoV-2} reinfection associated with emergence of {Omicron} in {South Africa}. \emph{Science}, \emph{376}(6593), eabn4947. http://doi.org/\href{https://doi.org/10.1126/science.abn4947}{10.1126/science.abn4947}

\leavevmode\vadjust pre{\hypertarget{ref-rader2022}{}}%
Rader, B., Gertz, A., Iuliano, A. D., Gilmer, M., Wronski, L., Astley, C. M., \ldots{} Brownstein, J. S. (2022). Use of {At-Home COVID-19 Tests} --- {United States}, {August} 23, 2021--{March} 12, 2022. \emph{MMWR. Morbidity and Mortality Weekly Report}, \emph{71}(13), 489--494. http://doi.org/\href{https://doi.org/10.15585/mmwr.mm7113e1}{10.15585/mmwr.mm7113e1}

\leavevmode\vadjust pre{\hypertarget{ref-reinhart2021}{}}%
Reinhart, A., Brooks, L., Jahja, M., Rumack, A., Tang, J., Agrawal, S., \ldots{} Tibshirani, R. J. (2021). An open repository of real-time {COVID-19} indicators. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(51), e2111452118. http://doi.org/\href{https://doi.org/10.1073/pnas.2111452118}{10.1073/pnas.2111452118}

\leavevmode\vadjust pre{\hypertarget{ref-riley2021}{}}%
Riley, S., Atchison, C., Ashby, D., Donnelly, C. A., Barclay, W., Cooke, G. S., \ldots{} REACT study group. (2021). {REal-time Assessment} of {Community Transmission} ({REACT}) of {SARS-CoV-2} virus: {Study} protocol. \emph{Wellcome Open Research}, \emph{5}, 200. http://doi.org/\href{https://doi.org/10.12688/wellcomeopenres.16228.2}{10.12688/wellcomeopenres.16228.2}

\leavevmode\vadjust pre{\hypertarget{ref-robson2014}{}}%
Robson, B. J. (2014). When do aquatic systems models provide useful predictions, what is changing, and what is next? \emph{Environmental Modelling \& Software}, \emph{61}, 287--296. http://doi.org/\href{https://doi.org/10.1016/j.envsoft.2014.01.009}{10.1016/j.envsoft.2014.01.009}

\leavevmode\vadjust pre{\hypertarget{ref-rothman2008}{}}%
Rothman, K. J., Greenland, S., \& Lash, T. L. (2008). \emph{Modern epidemiology} (Third edition). {Philadelphia Baltimore New York}: {Wolters Kluwer Health, Lippincott Williams \& Wilkins}.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1987}{}}%
Rubin, D. B. (1987). The {Calculation} of {Posterior Distributions} by {Data Augmentation}: {Comment}: {A Noniterative Sampling}/{Importance Resampling Alternative} to the {Data Augmentation Algorithm} for {Creating} a {Few Imputations When Fractions} of {Missing Information Are Modest}: {The SIR Algorithm}. \emph{Journal of the American Statistical Association}, \emph{82}(398), 543. http://doi.org/\href{https://doi.org/10.2307/2289460}{10.2307/2289460}

\leavevmode\vadjust pre{\hypertarget{ref-rubin2004}{}}%
Rubin, D. B., Gelman, A., \& Meng, X.-L. (Eds.). (2004). \emph{Applied {Bayesian} modeling and causal inference from incomplete-data perspectives: An essential journey with {Donald Rubin}'s statistical family}. {Chichester, West Sussex, England ; Hoboken, NJ}: {Wiley}.

\leavevmode\vadjust pre{\hypertarget{ref-sah2021}{}}%
Sah, P., Fitzpatrick, M. C., Zimmer, C. F., Abdollahi, E., Juden-Kelly, L., Moghadas, S. M., \ldots{} Galvani, A. P. (2021b). Asymptomatic {SARS-CoV-2} infection: {A} systematic review and meta-analysis. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(34), e2109229118. http://doi.org/\href{https://doi.org/10.1073/pnas.2109229118}{10.1073/pnas.2109229118}

\leavevmode\vadjust pre{\hypertarget{ref-sah2021a}{}}%
Sah, P., Fitzpatrick, M. C., Zimmer, C. F., Abdollahi, E., Juden-Kelly, L., Moghadas, S. M., \ldots{} Galvani, A. P. (2021a). Asymptomatic {SARS-CoV-2} infection: {A} systematic review and meta-analysis. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(34), e2109229118. http://doi.org/\href{https://doi.org/10.1073/pnas.2109229118}{10.1073/pnas.2109229118}

\leavevmode\vadjust pre{\hypertarget{ref-salomon2021}{}}%
Salomon, J. A., Reinhart, A., Bilinski, A., Chua, E. J., La Motte-Kerr, W., Rönn, M. M., \ldots{} Tibshirani, R. J. (2021a). The {US COVID-19 Trends} and {Impact Survey}: {Continuous} real-time measurement of {COVID-19} symptoms, risks, protective behaviors, testing, and vaccination. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(51), e2111454118. http://doi.org/\href{https://doi.org/10.1073/pnas.2111454118}{10.1073/pnas.2111454118}

\leavevmode\vadjust pre{\hypertarget{ref-salomon2021a}{}}%
Salomon, J. A., Reinhart, A., Bilinski, A., Chua, E. J., La Motte-Kerr, W., Rönn, M. M., \ldots{} Tibshirani, R. J. (2021b). The {US COVID-19 Trends} and {Impact Survey}: {Continuous} real-time measurement of {COVID-19} symptoms, risks, protective behaviors, testing, and vaccination. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(51), e2111454118. http://doi.org/\href{https://doi.org/10.1073/pnas.2111454118}{10.1073/pnas.2111454118}

\leavevmode\vadjust pre{\hypertarget{ref-sevcikova2007}{}}%
Ševčíková, H., Raftery, A. E., \& Waddell, P. A. (2007). Assessing uncertainty in urban simulations using {Bayesian} melding. \emph{Transportation Research Part B: Methodological}, \emph{41}(6), 652--669. http://doi.org/\href{https://doi.org/10.1016/j.trb.2006.11.001}{10.1016/j.trb.2006.11.001}

\leavevmode\vadjust pre{\hypertarget{ref-shumway2011}{}}%
Shumway, R. H., \& Stoffer, D. S. (2011). \emph{Time series analysis and its applications: With {R} examples} (3rd ed). {New York}: {Springer}.

\leavevmode\vadjust pre{\hypertarget{ref-singanayagam2022}{}}%
Singanayagam, A., Hakki, S., Dunning, J., Madon, K. J., Crone, M. A., Koycheva, A., \ldots{} Lackenby, A. (2022). Community transmission and viral load kinetics of the {SARS-CoV-2} delta ({B}.1.617.2) variant in vaccinated and unvaccinated individuals in the {UK}: A prospective, longitudinal, cohort study. \emph{The Lancet Infectious Diseases}, \emph{22}(2), 183--195. http://doi.org/\href{https://doi.org/10.1016/S1473-3099(21)00648-4}{10.1016/S1473-3099(21)00648-4}

\leavevmode\vadjust pre{\hypertarget{ref-thenewyorktimes2022}{}}%
The New York Times. (2022, December 16). Coronavirus in the {U}.{S}.: {Latest Map} and {Case Count}. Retrieved December 16, 2022, from \url{https://www.nytimes.com/interactive/2021/us/covid-cases.html}

\leavevmode\vadjust pre{\hypertarget{ref-tomwolf2020}{}}%
Tom Wolf. (2020, November 19). Process to {Reopen Pennsylvania}. Retrieved December 16, 2022, from \url{https://www.governor.pa.gov/process-to-reopen-pennsylvania/}

\leavevmode\vadjust pre{\hypertarget{ref-venables2002}{}}%
Venables, W. N., \& Ripley, B. D. (2002). \emph{Modern {Applied Statistics} with {S}}. {New York, NY}: {Springer New York}. http://doi.org/\href{https://doi.org/10.1007/978-0-387-21706-2}{10.1007/978-0-387-21706-2}

\leavevmode\vadjust pre{\hypertarget{ref-wasserman2006}{}}%
Wasserman, L. (2006). \emph{All of nonparametric statistics}. {New York}: {Springer}.

\leavevmode\vadjust pre{\hypertarget{ref-wu2020}{}}%
Wu, S. L., Mertens, A. N., Crider, Y. S., Nguyen, A., Pokpongkiat, N. N., Djajadi, S., \ldots{} Benjamin-Chung, J. (2020). Substantial underestimation of {SARS-CoV-2} infection in the {United States}. \emph{Nature Communications}, \emph{11}(1), 4507. http://doi.org/\href{https://doi.org/10.1038/s41467-020-18272-4}{10.1038/s41467-020-18272-4}

\leavevmode\vadjust pre{\hypertarget{ref-xiao2022}{}}%
Xiao, A., Wu, F., Bushman, M., Zhang, J., Imakaev, M., Chai, P. R., \ldots{} Alm, E. J. (2022). Metrics to relate {COVID-19} wastewater data to clinical testing dynamics. \emph{Water Research}, \emph{212}, 118070. http://doi.org/\href{https://doi.org/10.1016/j.watres.2022.118070}{10.1016/j.watres.2022.118070}

\leavevmode\vadjust pre{\hypertarget{ref-zhan2022}{}}%
Zhan, Q., Babler, K. M., Sharkey, M. E., Amirali, A., Beaver, C. C., Boone, M. M., \ldots{} Solo-Gabriele, H. M. (2022). Relationships between {SARS-CoV-2} in {Wastewater} and {COVID-19 Clinical Cases} and {Hospitalizations}, with and without {Normalization} against {Indicators} of {Human Waste}. \emph{ACS ES\&T Water}, \emph{2}(11), 1992--2003. http://doi.org/\href{https://doi.org/10.1021/acsestwater.2c00045}{10.1021/acsestwater.2c00045}

\end{CSLReferences}

% Index?

\end{document}
