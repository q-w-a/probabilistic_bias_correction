
```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      fig.align ='center')
```


```{r,eval=FALSE, include = FALSE}
rbbt::bbt_update_bib(path_rmd = "./02-chap2.Rmd", path_bib = "./bib/references.bib")
rbbt::bbt_insert()
```



# Background 

```{r, echo = FALSE }

library(tidyverse)

library(latex2exp)

theme_c <- function(...){ 
   # font <- "Helvetica"   #assign font family up front
  #  font <- "Arial"
    theme_bw() %+replace%    #replace elements we want to change
    
    theme(
      
      
      #text elements
      plot.title = element_text(             #title
                 #  family = font,            #set font family
                   size = 14,                #set font size
                   face = 'bold',            #bold typeface
                   hjust = .5,
                   vjust = 3),               
      
      plot.subtitle = element_text(          #subtitle
                #   family = font,            #font family
                   size = 14,
                   hjust = .5,
                   face = 'italic',
                   vjust = 3),               #font size
      
      axis.title = element_text(             #axis titles
                #   family = font,            #font family
                   size = 12),               #font size
      
      axis.text = element_text(              #axis text
                #   family = font,            #axis famuly
                   size = 9),
      # t, r, b, l
      plot.margin = unit(c(1,.5,.5,.5), "cm")
      ) %+replace%
      theme(...)
   
}
```



## Probabalistic Bias Analysis 

Often the focus of quantifying error about an effect estimate focuses on random error rather than the systematic error. For example, typical frequentist confidence intervals are frequent in medical and epidemiological literature, although they have faced rising criticism [@greenland2016]. These confidence intervals quantify the fraction of the times we expect the true value to fall in this interval under the assumption that our model is correct. That is, if we ran an experiment 100 times and computed the effect size each time, we would expect the 95% confidence interval to contain the true value to 95 of those times, on average. Neyman stressed this in his original publication formalizing the concept of a confidence interval in 1937 [@neyman1937]. The nuance that the confidence interval is not the probability that the true value falls within this interval, however, is often lost in the discussion of results, in part because the true meaning of a confidence interval is less intuitive.

The aim of quantitative bias analysis is to estimate systematic error to give a range of possible values for the true quantity of interest. In this sense, it is a type of sensitivity analysis. It can be used to estimate various kinds of biases, from misclassification, as is implemented in this work, as well as selection bias and unmeasured confounding [@petersen2021]. Often, the goal of performing such an analysis is to see how these sources of bias affect our estimates; in particular, under what situations of bias the observed effect would be null. 

There are multiple different forms of bias analysis [@lash2009]. The most simple case, simple bias analysis, is correcting a point estimate for a single source of error. Multidimensional bias analysis extends this to consider sets of bias parameters, but still provides a corrected point estimate rather than a range of plausible estimates. Probabilistic bias analysis, meanwhile, defines probability distributions for bias parameters to generate a distribution of corrected estimates by repeatedly correcting estimates for bias under different combinations of the parameter values. Then, via Monte Carlo we obtain a distribution of corrected estimates that reflect the corrected values under different scenarios of bias, that is, under different combinations of the bias parameters. This can give us a better idea for the extent of uncertainty about the corrected estimates, although this uncertainty does depend on the specification of the bias parameter distributions. Inherent in bias analysis is the dependence of our results on the specification of bias parameters, which reflect what is known from available data, literature, or theory on the extent of bias that may occur. There is uncertainty about how we define these distributions or values; otherwise, if the precise values of the bias parameters were known, we could simply correct the estimates and probabilistic bias analysis would not be useful.

Although some forms of probabilistic bias analysis can be applied to summarized data, for example, frequencies in a contingency table, the methods are most often implemented with unsummarized data in its original form, as implemented here. 

In choosing specific distributions for the bias parameters, different specifications may yield density functions where most of the density is within a similar interval, which means the choice of the specific distribution will not be sensitive to the particular choice of density. 



##  Background for the Approach 

The Bayesian melding approach was proposed by Poole et al. [@poole2000].

This approach enables us to account for both uncertainty from inputs and outputs of a deterministic model. The initial motivation for the approach was to study the population dynamics of whales in the presence of substantial uncertainty around model inputs for population growth [@poole2000]. However, the framework provided by Poole et al. can applied in any circumstance where we have uncertainty around some quantities $\theta$ and $\phi$ where there is a deterministic function $M:\theta \to\phi$. Due the utility of Bayesian melding in various contexts, since this deterministic model $M$ could take on a wide range of forms, the approach has since been applied in various fields, including urban simulations [@sevcikova2007], ecology [@robson2014], and infectious disease [@powers2011].  

Let $M: \theta \to \phi$ be the deterministic model defined by the function relating a vector of input parameters $\theta$ to an output vector $\phi$, and suppose we have a prior on $\theta$ denoted $f_\theta(\theta)$ and  a prior on $\phi$ denoted $f_\phi^{direct}(\phi)$.

However, note that we actually have two distinct priors on $\phi$. There is the prior formed by the distribution induced on $\phi$ by the prior for $\theta$ and the function $M$, where we denote this induced prior $f_\phi^{induced}(\phi)$. Generally, these priors are based on different sources of information.


If $M^{-1}$ exists, we can write this induced prior $f_\phi^{induced}(\phi) = f_\theta(M^{-1}(\phi)) |J(\phi)|$. This result follows from the fact $M(\theta) = \phi$, so we apply a change of variables to obtain the distribution of $\phi$ from the distribution of $M(\theta)$. 

In practice,  $M^{-1}$ rarely exists exists since $\theta$ is often of higher dimensionality then $\phi$, in which cases $M$ is not invertible. This means we generally approximate $q^*_1(\phi)$ without acquiring its analytical form. 


 In addition to this induced prior, we have the prior $f_\phi^{direct}(\phi)$, which does not involve $M$ nor the inputs $\theta$. Since these priors are based on different sources of information and may reflect different uncertainties, often it useful to use both sources of information to inform our estimates. To do so, we need to combine the distributions for $q^*_1(\phi)$ and $f_\phi^{direct}(\phi)$ to create a pooled distribution.

Multiple pooling strategies exist for distinct distributions, but one requirement for a Bayesian analysis is that the distribution should be independent of the order in which the prior is updated and the combining of the prior distribution. That is, updating the prior distributions using Bayesâ€™ theorem and then combining distributions should yield the same result as combining distributions and then updating this combined distribution; pooling methods that have this property are deemed externally Bayesian. Logarithmic pooling has been shown to be externally Bayesian under some conditions, which are likely to hold in most settings. Furthermore, logarithmic pooling has actually been shown to be the only pooling method where this holds [@genest1986]. For this reason, Poole *et al.* recommend proceeding with logarithmic pooling for Bayesian melding.

The logarithmically pooled prior for $\phi$ by pooling $q^*_1(\phi)$ and $f_\phi^{direct}(\phi)$ is proportional to 

$$(f_\phi^{induced}(\phi))^{\alpha} (f_\phi^{direct}(\phi))^{1-\alpha}$$

where $\alpha \in [0,1]$ is a pooling weight. Commonly, a choice of $\alpha = 0.5$ is used to give the priors equal weight. In this case, logarithmic pooling may be referred to as geometric pooling since it is equivalent to taking a geometric mean.

If $M$ is invertible, we can obtain the contrained distributions for the model inputs by simply inverting $M$. However, $M$ is rarely invertible, so we have to think about how to proceed in the noninvertible case.

To get intuition for a valid strategy Poole et al. recommend, we consider a mapping $M: \theta \to \phi$ for $\theta \in \mathbb{R}$ and $\phi \in \mathbb{R}$ 
defined as follows. Note the choice of $f_\theta,f_\phi^{direct}$ does not matter here as long as they are valid densities.



```{r, echo = FALSE}


library(kableExtra)
df <- tibble::tibble(
             `$\\theta$` = c(1,2,3),
             `$f_\\theta(\\theta)$` = c(.3,.2,.5),
             `$\\phi$` = c(1,2,2),
             `$f_\\phi^{direct}(\\phi)$` = c(0.4,0.6,0.6))

kbl(df, escape=FALSE) %>%
  kable_classic()%>%
  kable_styling(latex_options = "HOLD_position")


```



We see that $M$ is not invertible since $\theta=1$ and $\theta = 2$ both map to $\phi=2$, which implies the inverse $M^{-1}$ would not be well defined.

We can generate a sample from the density $f_\phi^{induced}(\phi)$ using our function $M$ and taking $f_\phi^{induced}(\phi) = f_\theta(M^{-1}(\phi))$; in the continuous case we need to multiply by $|J(\phi)|$, but not in the discrete case [@blitzsteinIntroductionProbability2019].


So we have $f_\phi^{induced}(1) = f_\theta(1) = 0.3$ since $M(1)=1$, and $f_\phi^{induced}(2) = f_\theta(2) + f_\theta(3) = 0.2 + 0.5=0.7$ since $M(2) = 2$ and $M(3)=2$.

Then, we can compute the logarithmically pooled pooled prior with $\alpha=0.5$ by taking $f_\phi^{induced}(\phi)^{\alpha} f_\phi^{direct}(\phi)^{1-\alpha}$.

For $\phi = 1$, we have $f_\phi^{induced}(\phi)^{\alpha} f_\phi^{direct}(\phi)^{1-\alpha} = (0.3)^{0.5}(0.4)^{0.5} = 0.3464$.
For $\phi = 2$, we have $f_\phi^{induced}(\phi)^{\alpha} f_\phi^{direct}(\phi)^{1-\alpha} = (0.6)^{0.5}(0.7)^{0.5} = 0.6481$.

To make this a valid density, however, these probabilities must sum to 1, so we renormalize by dividing by (0.3464 + 0.6481). Denoting the pooled prior in phi-space as $f_\phi^{pooled}(\phi)$, this gives us
$0.3464 / (0.3464 + 0.6481) = 0.3483$ for $\phi =1$ and $0.6481 / (0.3464 + 0.6481) =0.6517$ for $\phi=2$. 

Summarizing these results, we have 

```{r, echo = FALSE}
df <- tibble::tibble(
             # `$\\theta$` = c(1,2,3),
             # `$f_\theta(\\theta)$` = c(.3,.2,.5),
             `$\\phi$` = c(1,2),
             `$f_\\phi^{direct}(\\phi)$` = c(0.4,0.6),
            `$f_\\phi^{induced}(\\phi)$` = c(0.3, 0.7),
            `$q^{\\sim[\\phi]}(\\phi)$` = c(0.3483,0.6517 ))

kbl(df, escape=FALSE) %>%
  kable_classic()%>%
  kable_styling(latex_options = "HOLD_position")
```


However, we want the pooled prior on the inputs $\theta$, that is, $f_\theta^{pooled}(\theta)$.

Poole et al. reasoned as follows. Since $M$ uniquely maps $\theta=1$ to $\phi =1$, the probability that $\theta=1$ should be equal to the probability $\phi = 1$. That is, we should have $f_\theta^{pooled}(1) = f_\phi^{pooled}(1)$.

However, the relationship for $\theta=2$ or $\theta=3$ to $\phi$ is not one to one. Since $M(2)=2$ and $M(3)=2$, the sum of the probabilities for $\theta=1$ and $\theta=2$ should be equal to that for $\phi=2$, that is, $f_\theta^{pooled}(2) + f_\theta^{pooled}(3) = f_\phi^{pooled}(2) = 0.6517$. 

The challenge here is how we divide the probability for $f_\phi^{pooled}(2)$, which is defined, among $f_\theta^{pooled}(2)$ and $f_\theta^{pooled}(3)$. The prior for $\phi$ yields no information to assist in this choice, because knowing which value $\phi$ takes on does not give us any information about whether $\theta=2$ or $\theta=3$. Thus, the information we have about $\theta$ must be taken from $f_\theta(\theta)$.

That is, we can assign a probability for $f_\theta^{pooled}(2)$ by considering the probability that $\theta = 2$ relative to the probability $\theta =3$, computing


$$f_\theta^{pooled}(2) = f_\phi^{pooled}(2) \Big( \frac{f_\theta(2)}{f_\theta(2) + f_\theta(3)}\Big).$$ 

That is, if the probability $\theta$ takes on the value $2$ is lower in this case than the probability $\theta=3$ which we know from the prior on $\theta$, $f_\theta(\theta)$, then the pooled prior on $\theta$,  $f_\theta^{pooled}(2)$, should reflect this.

Using this reasoning, we have $f_\theta^{pooled}(2) = (0.7) \frac{0.2}{0.2+0.5} = 0.1862$ and $f_\theta^{pooled}(3) = (0.7) \frac{0.5}{0.2+0.5} = 0.4655$.

The result in this simple example, using $f_\theta(\theta)$ to determine how to distribute the probability for values of $\phi$ where multiple $\theta$ map to $\phi$, can be used to derive general formulas to compute $f_\theta^{pooled}(\theta)$ for discrete and continuous distributions [@poole2000]. 

### General Solution for the Discrete Case

Denote the possible values of $\theta$ as $A_1, A_2, \dots$, the possible values of $\phi$ as $B_1, B_2, \dots$, and a mapping $m: \mathbb{N} \to \mathbb{N}$ such that $M(A_i) = B_{m(i)}$ and $C_j = M^{-1}(B_j) = \{A_i : M(A_i) = B_j\}$. Then

$$f_\theta^{pooled}(A_i) = f_\phi^{pooled}(B_{m(i)}) \left( \frac{f_\theta(A_i)}{f_\phi^{induced}(B_{m(i)})} \right).$$

### General Solution for the Continuous Case


We denote $B = M(A) = \{M(\theta) : \theta \in A \}$ and $C = M^{-1}(B) = \{\theta: M(\theta) \in B \}$.


Then 

$$
f_\phi^{pooled} (M(\theta)) \left( \frac{f_\theta(\theta)}{f_\phi^{induced}(M(\theta))} \right) \tag{1}
$$

$$
=k_{\alpha} f_\theta(\theta) \left( \frac{f_\phi^{direct}(M(\theta))}{f_\phi^{induced}(M(\theta))} \right)^{1-\alpha} \tag{2}
$$
where $k_\alpha$ is a renormalizing constant for the choice of $\alpha$. However, how we get from (1) to (2) is unclear to me.

### Implementation through the Sampling-Importance-Resampling Algorithm 

The steps to the sampling-importance-resampling procedure to are:

1. We draw $\theta$ from its prior distribution $f_\theta(\theta)$.
2. For every $\theta_i$ we compute $\phi_i = M(\theta_i)$ to obtain a sample from the induced distribution.
3. Since the density $f_\phi^{induced}(\phi)$ is unlikely to have an analytical form, we can compute it via a density approximation such as kernel density estimation.
4. Construct weights proportional to the ratio of the prior on $\phi$ evaluated at $M(\theta_i)$ to the induced prior $f_\phi^{induced}$ evaluated at $M(\theta_i)$. If a likelihood $L_1(\theta)$ for the inputs and $L_2(\phi)$ is available, the weights are 
$$w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{1-\alpha}.$$
However, in this work, no likelihood is available for the variables of interest, so the likelihood is left out of the weights, leaving us with
$$w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{1-\alpha}.$$
5. Sample $\theta$ and $\phi$ from step (1) with probabilities proportional to the weights from (4).


\newpage

## Bayesian Melding Applied to COVID-19 Misclassification


|    In this work, we can relate the inputs $\theta =  \{P(S_1|\text{untested}), \alpha, \beta \}$ and  $\phi = P(S_0|\text{test}_+,\text{untested})$ by the deterministic model $M: \theta \to \phi$ given by
 $P(S_0|\text{test}_+, \text{untested}) = \dfrac{\beta(1 - P(S_1|\text{untested}))}{\beta(1-P(S_1|\text{untested})) + \alpha P(S_1|\text{untested})}.$ 
 The derivation of $M$ is in the [following section.](#derivation)

Now, we have two distributions on $\phi$: the distribution based on data on the asymptomatic rate of infection of COVID-19, and the distribution formed by taking $M(\theta)$ where $\theta$ represents the values from the defined distributions of $\alpha,\beta,$ and $P(S_1|\text{untested}$. With Bayesian melding, we pool these distributions using logarithmic pooling, and then implement the sampling-importance-resampling algorithm to obtain constrained distributions of the inputs $\theta$ that are in accordance with information about the asymptomatic rate of the virus. 

Due to the uncertainty around our definitions of $\alpha$ and $\beta$, it is particularly useful to leverage the information we have about the asymptomatic rate of the virus $P(S_0|\text{test}_+,\text{untested})$ because a large collection of studies has been published in this area. In a meta-analysis pooling data from 95 studies, the pooled estimate among the confirmed population that was asymptomatic was 40.50% [95% CI, 33.50%-47.50%] [@ma2021]. Another meta-analysis including 350 studies estimated the asymptomatic percentage to be 36.9% [95% CI: 31.8 to 42.4%], and, when restricting to screening studies, 47.3% (95% CI: 34.0% -61.0%) [@sah2021].

This means we have two priors on the asymptomatic rate $\phi$, that by taking $M(\theta)$ for sampled values of $\theta$, denoted $f_\phi^{induced}$ in the previous section, and that based on data about the asymptomatic rate, $f_\phi^{direct}$. 

\newpage 

### Distribution of $\theta = \{\alpha, \beta, P(S_1|\text{untested}) \}$

First, we obtain a sample $\theta_1, \theta_2, \dots, \theta_k$ from $\theta$ (Figure \ref{fig:theta}). 

```{r, include = FALSE}
library(latex2exp)



###############################################################
# BETA PARAMETERS FROM DESIRED MEAN AND VARIANCE
###############################################################
get_beta_params <- function(mu, sd) {
    var = sd^2
    alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
    beta <- alpha * (1 / mu - 1)
    return(params = list(alpha = alpha,
                         beta = beta))
}




###############################################################
# BETA DENSITY WITH DESIRED MEAN AND VARIANCE
###############################################################
beta_density <- function(x, mean, sd, bounds=NA) {
    shape_params <-  get_beta_params(
        mu = mean,
        sd = sd)

    if(!length(bounds) == 1){
        # message("here")
        dtrunc(x,
               spec = "beta",
               a = bounds[1],
               b = bounds[2],
              shape1 = shape_params$alpha,
              shape2 = shape_params$beta) %>%
            return()
    }else{
        dbeta(x,
          shape1 = shape_params$alpha,
          shape2 = shape_params$beta)  %>%
            return()
        }
}




###############################################################
# SAMPLE FROM BETA DENSITY WITH DESIRED MEAN AND VARIANCE
###############################################################

sample_beta_density <- function(n, mean, sd, bounds = NA) {

    shape_params <-  get_beta_params(
        mu = mean,
        sd = sd)

    rbeta(n,
          shape1 = shape_params$alpha,
          shape2 = shape_params$beta)

    if(!length(bounds) == 1){
        # message("here")
        rtrunc(n,
               spec = "beta",
               a = bounds[1],
               b = bounds[2],
               shape1 = shape_params$alpha,
               shape2 = shape_params$beta) %>%
            return()
    }else{
        rbeta(n,
              shape1 = shape_params$alpha,
              shape2 = shape_params$beta)  %>%
            return()
    }
}




###############################################################
# GAMMA PARAMETERS FROM DESIRED MEAN AND VARIANCE
###############################################################
get_gamma_params <- function(mu, sd) {
    var = (mu/sd)^2
    shape = (mu/sd)^2
    scale = sd^2/mu
    return(params = list(shape = shape,
                         scale = scale))
}


###############################################################
# GAMMA DENSITY WITH DESIRED MEAN AND VARIANCE
###############################################################
gamma_density <- function(x, mean, sd, bounds=NA) {

    shape_params <-  get_gamma_params(
        mu = mean,
        sd = sd)

    if(!length(bounds) == 1){
        #message("here")
        dtrunc(x,
               spec = "gamma",
               a = bounds[1],
               b = bounds[2],
               shape = shape_params$shape,
               scale = shape_params$scale) %>%
            return()
    }else{
        dgamma(x,
               shape = shape_params$shape,
               scale = shape_params$scale) %>%
            return()
    }
}


sample_gamma_density <- function(n, mean, sd, bounds = NA) {

    shape_params <-  get_gamma_params(
        mu = mean,
        sd = sd)

    if(!length(bounds) == 1){
        #message("here")
        rtrunc(n,
               spec = "gamma",
               a = bounds[1],
               b = bounds[2],
               shape = shape_params$shape,
               scale = shape_params$scale) %>%
            return()
    }else{
        rgamma(n,
               shape = shape_params$shape,
               scale = shape_params$scale) %>%
            return()
    }
}





###############################################################
# INDUCED PRIOR ON ASYMPTOMATIC RATE  P(S_0|test+,untested)
###############################################################

# input sampled values of theta and compute M(\theta)
est_P_A_testpos = function(P_S_untested, alpha, beta){
    (beta * (1 - P_S_untested)) / (( beta * (1 - P_S_untested)) + (alpha * P_S_untested))
}




```

```{r, fig.width =6.5, fig.height = 3, fig.align='center', fig.cap = "\\label{fig:theta}", fig.pos = "H"}

# set prior parameters
alpha_mean = .95
alpha_sd = 0.08
alpha_bounds = NA
 # alpha_bounds = c(.8,1),
beta_mean = .15
beta_sd =.09
beta_bounds = NA
#  beta_bounds = c(0.002, 0.4),
s_untested_mean = .03
s_untested_sd = .0225
#  s_untested_bounds = c(0.0018, Inf),
s_untested_bounds = NA
p_s0_pos_mean = .4
p_s0_pos_sd = .1225
p_s0_pos_bounds = NA
#  p_s0_pos_bounds = c(.25, .7),
pre_nsamp = 1e5
post_nsamp = 1e4

theta <- tibble(alpha = sample_gamma_density(pre_nsamp,
                                                mean = alpha_mean,
                                                sd = alpha_sd,
                                                bounds = alpha_bounds),
                    beta= sample_beta_density(pre_nsamp,
                                              mean = beta_mean,
                                              sd = beta_sd,
                                              bounds = beta_bounds),
                    P_S_untested = sample_beta_density(pre_nsamp,
                                                       mean = s_untested_mean,
                                                       sd = s_untested_sd,
                                                       bounds = s_untested_bounds)) %>%
        mutate(phi_induced = est_P_A_testpos(P_S_untested = P_S_untested,
                                             alpha = alpha,
                                             beta=beta))



theta %>% select(-phi_induced) %>%
  rename( "$\\alpha$" = alpha,
          "$\\beta$" = beta,
         "$P(S_1|untested)$" = P_S_untested ) %>%
  pivot_longer(cols= everything()) %>%
  ggplot(aes(x=value)) +
  geom_density(alpha = .8, fill = "black") +
  theme_bw() +
   theme(plot.title = element_text(size = 16, 
                                   face="bold",
                                   hjust = .5),
          strip.text = element_text(size = 14),
         axis.title = element_text(size = 14)
          )  +
   labs(x = "Value",
        y = "Density",
        title = TeX("Sampling from $\\theta = \\alpha,\\beta, P(S_1|untested)$")
       ) +
  facet_wrap(~name,
             labeller=  as_labeller(TeX,
                                    default = label_parsed)
             )

# ggsave("./img/theta.png", dpi = 800)

```

### Direct Prior and Induced Prior Distributions for $P(S_0|\text{test}_+,\text{untested})$ 

Then, taking $M(\theta)$, we can compute the induced distribution $f_\phi^{induced}(M(\theta))$ and compare it to our prior on $\phi$ from meta-analyses on the asymptomatic rate, $f_\phi^{direct}(\phi)$ (\ref{fig:prior-induced}).

```{r, fig.cap ="\\label{fig:prior-induced}", fig.width = 6.5, fig.height = 3}



########################################################################################
# COMPARING DIRECT AND INDUCED PRIORS ON ASYMPTOMATIC RATE  P(S_0|test+,untested)
########################################################################################


theta %>%
  mutate(phi_prior = sample_beta_density(pre_nsamp, 
                                         mean = p_s0_pos_mean, 
                                         sd =p_s0_pos_sd,
                                         bounds = p_s0_pos_bounds)) %>%
  pivot_longer(c(phi_prior, 
                 phi_induced), names_to = "density") %>%
  mutate(density = ifelse(density == "phi_induced", 
                          "Induced Prior Distribution", 
                          "Original Prior Distribution")) %>%
  ggplot(aes(x=value, fill = density)) +
  geom_density(alpha = .8) +
  theme_bw() + 
  theme(legend.position = "right",
          legend.text = element_text(size = 14),
          plot.title = element_text(size = 16),
          axis.text.y = element_blank()) +
  scale_fill_manual(values = c("#577C9C", "#56BFA8")) +
  labs(title = TeX("Direct and Induced Distributions on $P(S_0|untested, +)$"),
       fill = "",
       y = "Density",
       x = "Value")

```

\newpage 

### Pooling

At this point, we want to obtain the logarithmically pooled distribution, denoted $f^{pooled}$.


Now, as described in greater detail in the section on the [Sampling-Importance-Resampling algorithm](#logpooled), the weights are $w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{1-\alpha}.$

We perform a kernel density estimation to approximate the density of $f_\phi^{induced}(\phi)$ at the coordinates $\phi_1, \dots, \phi_M$. To compute $f_\phi^{direct}(\phi)$, we can use the density function $f_\phi^{direct}$.



```{r}


# theta contains values sampled from alpha, beta, P_S_untested, and M(theta) = phi_induced
# induced phi
phi <- theta$phi_induced

# approximate $f_\phi^{induced}(\phi)$ via a density approximation
phi_induced_density <- density(x = phi, n = pre_nsamp, adjust = 2, kernel = "gaussian")




indexes <- findInterval(phi, phi_induced_density$x)


phi_sampled_density <- phi_induced_density$y[indexes]


dp_s0_pos <- function(x) {

      beta_density(x,
                   mean=p_s0_pos_mean,
                   sd = p_s0_pos_sd,
                   bounds=p_s0_pos_bounds)
}


weights <- (phi_sampled_density/ dp_s0_pos(phi))^(.5)


```


Once we have these weights, we resample the $\phi_1,\dots,\phi_M$ to obtain a sample from the target distribution $t(\alpha) \Big( f^{induced}(M(\theta)) \Big)^{0.5} \Big( f^{direct} (M(\theta)) \Big)^{0.5}$, where $t(\alpha)$ is the normalizing constant needed to make the pooled density valid. We resample $\theta_1, \dots, \theta_k$ with the same weights to obtain the constrained distributions for the inputs. 

We see the melded distributions and pre-melding distributions in Figure \ref{fig:melded}. 


```{r, fig.cap = "\\label{fig:melded}", fig.height =5, fig.width = 6.5}

# resample the posterior
post_samp_ind <-sample.int(n=pre_nsamp,
                           size=post_nsamp, 
                           prob=1/weights,
                           replace=T)


pi_samp <- cbind(theta[post_samp_ind,], 
                 P_A_testpos =  phi[post_samp_ind]) %>%
  select(-phi_induced)

pi_samp_long <- pi_samp %>%
  pivot_longer(cols=everything()) %>%
  mutate(type = "After Melding")

compare_melded <- theta %>%
  mutate(P_A_testpos = sample_beta_density(pre_nsamp, 
                                         mean = p_s0_pos_mean, 
                                         sd =p_s0_pos_sd,
                                         bounds = p_s0_pos_bounds)) %>%
  pivot_longer(cols=everything()) %>%
  mutate(type = ifelse(
    name == "phi_induced",
  "Induced", "Before Melding")) %>%
  mutate(name = ifelse(name == "phi_induced", "P_A_testpos", name)) %>%
  bind_rows(pi_samp_long) %>%
  mutate(name = case_when(
    name == "alpha" ~"$\\alpha$",
    name == "beta" ~"$\\beta$",
    name == "P_A_testpos" ~ "$P(S_0|test+,untested)$",
    name == "P_S_untested" ~ "$P(S_1|untested)$")
  ) %>%
  mutate(name = factor(name,
                       levels = c(
                         "$\\alpha$",
                         "$\\beta$",
                         "$P(S_1|untested)$",
                         "$P(S_0|test+,untested)$"))) 

library(cowplot)


p <- compare_melded %>%
  ggplot(aes(x = value, fill = type)) +
  geom_density(alpha = .5) +
  facet_wrap(~name,
             labeller = as_labeller(
               TeX,
               default = label_parsed), ncol = 3) + 
  theme_c(
    # axis.text.y = element_blank(),
       #   axis.ticks.y = element_blank(),
          axis.title = element_text(size = 12),
          axis.text.x = element_text(size = 12),
          plot.title =element_text(size = 14, margin =margin(.5,.5,.5,.5)),
          strip.text = element_text(size = 12),
          legend.text = element_text(size = 12)) +
  labs(
       fill = "",
       y = "Density") +
  scale_fill_manual(values = c("#5670BF", "#418F6A", "#B28542"))



p1 <- p %+% subset(compare_melded, 
                   name %in% c( "$\\alpha$",
                         "$\\beta$",
                         "$P(S_1|untested)$")) %+%
  theme(legend.position = "none")

p2 <-   p %+% subset(compare_melded, 
                     name == "$P(S_0|test+,untested)$")

legend <- get_legend(
  p1
)

cowplot::plot_grid(
  p1,
  p2,
  nrow = 2
)



# ggsave("./img/melded.png", dpi = 700)

```

Comparing the induced and direct priors on $P(S_0| \text{test}_+, \text{untested})$ above, we see that although they have shared support, some values from the induced distribution we acquire by using $M$ to generate values of $\phi$ from sampled values of $\theta$ are very unlikely to be in accordance with the information we know about the prevalence of SARS-CoV-2 asymptomatic infection. This is where Bayesian melding comes into play. Pooling these distributions enable us to take both the prior on  $f^{direct}$ from published analyses on asymptomatic infection, and the induced prior, $f^{induced}$, into account to constrain the distributions of both the model inputs $\theta = \{ \alpha, \beta, P(S_1 | \text{untested})\}$ and model output $\phi = P(S_0|\text{test}_+, \text{untested})$ to be in accordance with both prior distributions. We then use these constrained distributions as inputs in the probabilistic bias analysis.

\newpage 

### Derivation of $M$ {#derivation}


\indent We define $\theta$ as the set of bias parameters $\{P(S_1|\text{untested}), \alpha, \beta \}$. The parameters $\alpha$ and $\beta$ relate the observed overall test positivity rate to the test positivity rate we would obtain if we tested the asymptomatic and symptomatic partitions of the untested population. We define:

* $\alpha = \dfrac{P(\text{test}_+|S_1,\text{untested})}{P(\text{test}_+|\text{tested})}$
* $\beta = \dfrac{P(\text{test}_+|S_0,\text{untested})}{P(\text{test}_+|\text{tested})}$.

The parameter $P(S_1|\text{untested})$ reflects the probability someone among the untested population has moderate to severe COVID-like symptoms.

We relate this set of parameters to the asymptomatic infection rate $\phi = P(S_0|\text{test}_+, \text{untested})$ by the function $M: \theta \to \phi$: 

\begin{tcolorbox}
\vspace{2 mm}
\begin{align*}   
 M(\theta)  = \dfrac{\beta (1- P(S_1|\text{untested}))}{\beta(1- P(S_1|\text{untested})) + \alpha(P(S_1|\text{untested})} = P(S_0|\text{test}_+, \text{untested}).\\
\end{align*}
\end{tcolorbox}


In what follows, we show this equality holds.

\noindent Since we have $\alpha = \frac{P(\text{test}_+|S_1, \text{untested})}{P(\text{test}_+|tested)}$ and $\beta = \dfrac{P(\text{test}_+|S_0, \text{untested})}{P(\text{test}_+|tested)}$, we can write 

\begin{align*}  &= \dfrac{\dfrac{P(\text{test}_+|S_0, \text{untested})}{P(\text{test}_+|tested)}(1 - P(S_1|\text{untested}))}{\dfrac{P(\text{test}_+|S_0, \text{untested})}{P(\text{test}_+|tested)}(1-P(S_1|\text{untested})) + \dfrac{P(\text{test}_+|S_1, \text{untested})}{P(\text{test}_+|tested)} P(S_1|\text{untested})}
\end{align*}
and cancelling out the term $P(\text{test}_+|tested)$ we have


$$ = \dfrac{{P(\text{test}_+|S_0, \text{untested})}(1 - P(S_1|\text{untested}))}{P(\text{test}_+|S_0, \text{untested})(1-P(S_1|\text{untested})) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}.$$

\noindent Since $P(S_0|\text{untested}) = 1 - P(S_1|\text{untested})$,

\begin{align*} 
&=  \dfrac{{P(\text{test}_+|S_0, \text{untested})}P(S_0|\text{untested})}{P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}.
\end{align*}

Applying the definition of conditional probability to  the term 
$P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested})$ in the numerator,

\begin{align*}
&=
    \dfrac{\Big( \dfrac{P(\text{test}_+,S_0, \text{untested})}{P(S_0, \text{untested})} \Big) \Big(\dfrac{P(S_0, \text{untested})}{P(\text{untested})}\Big)}{P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}\\ 
    &= \dfrac{\Big( \dfrac{P(\text{test}_+,S_0, \text{untested})}{P(S_0, \text{untested})} \Big) \Big(\dfrac{P(S_0, \text{untested})}{P(\text{untested})}\Big)}{P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}\\
    &=  \dfrac{\dfrac{P(\text{test}_+,S_0, \text{untested})}{P(\text{untested})}}{P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}\\
    &=  \dfrac{{P(\text{test}_+,S_0|\text{untested})}}{P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})}.
\end{align*}

\noindent We can substitute this result in for the $P(\text{test}_+|S_0, \text{untested})P(S_0|\text{untested})$ term in the denominator to yield
\begin{align*}
  &=  \dfrac{{P(\text{test}_+,S_0|\text{untested})}}{P(\text{test}_+,S_0|\text{untested}) + P(\text{test}_+|S_1, \text{untested}) P(S_1|\text{untested})} \hspace{ 20 mm }
\end{align*}

With same reasoning, we can simplify 
\begin{align*}
P(\text{test}_+|S_1, \text{untested})P(S_1|\text{untested}) = P(S_1, \text{test}_+|\text{untested}),
\end{align*} giving us

\begin{align*}
  &=  \dfrac{{P(\text{test}_+,S_0|\text{untested})}}{P(\text{test}_+,S_0|\text{untested}) +  P(S_1, \text{test}_+|\text{untested})} \hspace{45 mm }\\ 
   &=  \dfrac{{P(\text{test}_+,S_0|\text{untested})}}{P(\text{test}_+|\text{untested}) } \\
   &= \dfrac{\dfrac{P(S_0, \text{test}_+, \text{untested})}{P(\text{untested})}}{ \dfrac{P(\text{test}_+,\text{untested})}{P(\text{untested})}} \\ 
  &=\dfrac{P(S_0, \text{test}_+, \text{untested})}{P(\text{test}_+,\text{untested})} \\
  &= P(S_0 |\text{test}_+, \text{untested}).
\end{align*}

\noindent Hence, we have 


\begin{align*}
P(S_0 |\text{test}_+, \text{untested}) = \dfrac{\beta (1- P(S_1|\text{untested}))}{\beta(1- P(S_1|\text{untested})) + \alpha(P(S_1|\text{untested})}
\end{align*}

\noindent as desired. 
\qed


\newpage


## Sampling-Importance-Resampling Algorithm \label{sampling}

### Overview 

|    The Sampling-Importance-Resampling Algorithm, introduced in @rubin1987, is a non-iterative method for approximating a sample from a target probability density function $f$.


The two main steps are the sampling step and importance resampling step. We have two (generally distinct) sample sizes, where $m$ is the initial sample size and $r$ is the posterior sample size.

In the sampling step, we draw an independent and identically distributed sample of size $m$ from $g$, $Y_1, Y_2, \dots, Y_m$. Then, we compute weights $h(Y)$ such that $g \cdot h \propto f$. That is, we set the weights

$w_i = h(Y_i) = \dfrac{\frac{f(Y_i) } {g(Y_i)} }{\sum_{i=1}^m\frac{f(Y_i) } {g(Y_i)} }.$

We resample with these defined weights to obtain a sample of size $r$ from $Y_1, Y_2, \dots, Y_m$. We denote this resample $Z_1,\dots, Z_r$.

The method is most efficient when $g$ is a good approximation of $f$. Also of interest is setting the sampling size and posterior sample size. The algorithm generates independent and identically distributed samples as $m/r \to \infty$, but in most applications $m/r$ between 10 and 20 is appropriate [@rubin2004].   

Sometimes the resulting distribution will have a closed form. 


#### Example 1:

Suppose $Y \sim Exp(\lambda)$, so we have the PDF $f_Y(y) = \lambda e^{-\lambda y}$, and we sample $Z_1,\dots,Z_r$ from $Y_1, \dots, Y_m$ with weights direction proportional to $X$, that is, $h(Y) = Y$.

Then  $Z_1,\dots Z_r$ is approximately a sample from $h(x) \; f_Y(y)  =  y  \; \lambda e^{-\lambda y}$.

From the PDF of the gamma distribution, $\dfrac{\beta^\alpha}{\Gamma(\alpha) }y^{\alpha - 1} e^{-\beta y}$ we can recognize that $y \cdot e^{-\lambda y}$ corresponds to the gamma distribution with $\alpha = 2$ and $\beta = \lambda$. 

We can see this result by considering $Y$ before and after resampling below (Figure \ref{fig:ex1}).


```{r, fig.width = 6, fig.height = 2.5, fig.cap = "\\label{fig:ex1}"}
library(latex2exp)
library(viridis)

pre_nsamp <- 1e6
post_nsamp <- 1e5



##########################################
# EXAMPLE 1
##########################################

# weights are proportional to random variable itself
input_rate = .2
phi_sim <- rexp(pre_nsamp, rate = input_rate)
post_samp_ind <- sample.int(n = pre_nsamp, size = post_nsamp, replace=TRUE, prob=phi_sim)


dat <- tibble(name = "Before Resampling",
              value = phi_sim) %>%
  bind_rows( tibble(
    name = "After Sampling",
    value = phi_sim[post_samp_ind]
  )) %>%
  mutate(name = factor(name, 
                       levels = c("Before Resampling", 
                                  "After Sampling")))



##########################################
# PLOT *NOT* INCLUDING THEORETICAL DENSITY
##########################################
dat %>% 
  # filter(name == "post_melding") %>%
  ggplot()  +
  geom_density(aes(x=value, fill = name),
               alpha = .6,
               color = NA) +
  labs(title = TeX(
    paste0("Sampling from $Y \\sim Exp(\\lambda =  ",
           input_rate, ")$ with Weights $h(y) =y$")),
  # subtitle =TeX(paste0("PDF of Gamma$(2, \\lambda)$ in red")),
  fill = "",
  y= "Density") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, size = 12),
        plot.subtitle = element_text(hjust = .5, size = 12),
        legend.text = element_text(size = 12)) +
  scale_fill_manual(values = c("#0C2B67", "#DE8600")) 


```

Then, we can see that the PDF of the the gamma distribution with $\alpha = 2$ and $\beta = \lambda$ corresponds to the post-sampling distribution as expected (Figure \ref{fig:ex12}). 


```{r,fig.width = 6, fig.height = 2.5, fig.cap = "\\label{fig:ex12}" }

library(ggtext)

##########################################
# PLOT INCLUDING THEORETICAL DENSITY
##########################################
color_lab <-  paste0("PDF of Gamma(2, ", input_rate, ")")

dat %>% 
  ggplot()  +
  geom_density(aes(x=value, fill = name),
               alpha = .6,
               color = NA) +
  labs(title = TeX(
    paste0("Sampling from $Y \\sim Exp(\\lambda =  ",
           input_rate, ")$ with Weights $h(y) =y$")),
  subtitle =(paste0("PDF of Gamma(2, ",
                       input_rate,
                       ") in <span style = 'color:darkred'>Red</span>")),
  fill = "",
  y = "Density") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, size = 12),
        plot.subtitle = element_markdown(hjust = .5, size = 12),
        legend.text = element_text(size = 12)) +
  stat_function(fun = dgamma, 
                args=list(shape=2,
                          scale=1/input_rate), 
                aes(color = color_lab),
                size = 1.2) +
  scale_color_manual(name = "", 
                     values = c("darkred")) +
  scale_fill_manual(values = c("#0C2B67", "#DE8600")) +
  guides(color = guide_legend(
    override.aes = list(size = 4)))


```


#### Example 2:

Similarly, again suppose $Y \sim Exp(\lambda)$, so $f_Y(y) = \lambda e^{-\lambda y}$. However, now we sample with weights defined by $h(y)= e^{-\lambda y}$. 
Then our sample $Z_1,\dots,Z_r$ is approximately a sample from 
\begin{align*} 
h(y) \; f_Y(y) &=   e^{-\lambda y} \cdot \lambda e^{-\lambda y}\\
&= \ e^{-2 \lambda y}  
\end{align*}
which is proportional to the exponential distribution with parameter $2\lambda$. 

The distributions before and after resampling are shown in Figure \ref{fig:ex2}.


```{r,fig.width = 6, fig.height = 2.5, fig.cap = "\\label{fig:ex2}"}

################################################################
# EXAMPLE 2
################################################################

# weights are proportional to exp(-rate * random_variable)
input_rate = .2
phi_sim <- rexp(pre_nsamp, rate = input_rate)
post_samp_ind <- sample.int(n = pre_nsamp, size = post_nsamp, replace=TRUE, prob=exp(phi_sim*-1*input_rate))


dat <-  tibble(name = "Before Resampling",
              value = phi_sim) %>%
  bind_rows( tibble(
    name = "After Sampling",
    value = phi_sim[post_samp_ind]
  )) %>%
  mutate(name = factor(name, 
                       levels = c("Before Resampling", 
                                  "After Sampling")))

library(latex2exp)

##########################################
# PLOT *NOT* INCLUDING THEORETICAL DENSITY
##########################################
dat %>% 
  ggplot()  +
  geom_density(aes(x=value, fill = name),
               alpha = .6,
               color = NA) +
  # stat_function(fun = dexp, 
  #               args=list(rate = 2*input_rate), 
  #               color = "red") +
  labs(title = TeX(
    paste0("Sampling from $Y \\sim Exp(\\lambda=", input_rate, ") $ with Weights ",
  "$e^{-\\lambda \\; y}$")),
  #subtitle =TeX("PDF of $Exp(2*\\lambda)$ in Red"),
  fill = "",
  y = "Density") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = .5, size = 12),
        plot.subtitle = element_text(hjust = .5, size = 12),
        legend.text = element_text(size = 12)) +
  scale_fill_manual(values = c("#0C2B67", "#DE8600"))

```

and then plotting the PDF of the exponential distribution with parameter $2\lambda$ we can see the correspondence to the post-sampling distribution (Figure \ref{fig:ex22}).

```{r, fig.width =6, fig.height = 2.5, fig.cap = "\\label{fig:ex22}"}

color_lab <-  paste0("PDF of Exp(2 * ", input_rate, ")")

##########################################
# PLOT INCLUDING THEORETICAL DENSITY
##########################################
dat %>% 
  ggplot()  +
  geom_density(aes(x=value, fill = name),
               alpha = .6,
               color = NA) +
  stat_function(fun = dexp,
                args=list(rate = 2*input_rate),
           #     color = "darkred",
                size=1.2,
                aes(color = color_lab)) +
  labs(title = TeX(
    paste0("Sampling from $Y \\sim Exp(\\lambda=", 
           input_rate, 
           ") $ with Weights ",
  "$e^{-\\lambda \\; y}$")),
  subtitle =TeX("PDF of $Exp(2*\\lambda)$ in Red"),
  fill = "",
  y = "Density") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = .5, size = 12),
        plot.subtitle = element_text(hjust = .5, size = 12),
        legend.text = element_text(size = 12)) +
  scale_fill_manual(values = c("#0C2B67", "#DE8600"))  +
  scale_color_manual(name = "", 
                     values = c("darkred")) +
  guides(color = guide_legend(
    override.aes = list(size = 4)))



```


\newpage


### Proof that Algorithm Obtains Approximate Sample from Target Distribution {#proof}


The Sampling-Importance-Resampling is fundamental to the implementation of Bayesian melding. To gain further insight into how sampling with weights
$w_i = \left( \frac{f_\phi^{direct}(M(\theta_i))}{f_\phi^{induced}(M(\theta_i))} \right)^{0.5}$
 approximates a sample from the target distribution the logarithmically pooled distribution $f^{pooled}$, we first prove a more general result. 



\begin{tcolorbox}
Suppose we sample $Y_1, Y_2, \dots, Y_m$ independently and identically distributed with probability density function  $g$ and compute the weights
\[ w_i =\dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }\]
for some nonnegative function $h$ defined on the support of $Y$.
% \[ w_i = h(Y_i) = \dfrac{\frac{f(Y_i)}{g(Y_i)}}{\sum_{j=1}^m \frac{f(Y_j)}{g(Y_j)}}\]

If  we sample $Z_1, \dots, Z_r$ from the discrete distribution $Y_1,\dots, Y_m$ such that 

\[ P(Z = Y_i) = \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) } = w_i ,\]
then $Z_1, \dots, Z_r$ is approximately a sample with density proportional to $h \cdot g$.

\end{tcolorbox}
\vspace{5 mm}

Since $Z$ is sampled from $Y$, we have
\[ P(Z \leq x ) = \sum_{z_i \leq x} P(Z=z_i) = \sum_{Y_i \leq x} P(Z=Y_i) .\]

We can take this sum to be over all possible values of $Y$ by including the indicator function $\mathbb{I} (Y_i \leq x)$, yielding
\[  = \sum_{i = 1}^m P(Z=y_i)\;\;\mathbb{I} (Y_i \leq x).  \]
and since $P(Z=Y_i) = \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }$ by definition we have

\begin{align*} 
&= \sum_{i = 1}^m \dfrac{h(Y_i)}{\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x)   \\
&=  \left( \dfrac{1}{ {\sum_{i=1}^mh(Y_i) }} \right) {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x)   \\
&=   \dfrac{ {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\sum_{i=1}^mh(Y_i) } \\
&=   \dfrac{ \frac 1m {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\frac 1m \sum_{i=1}^mh(Y_i) }. \\
\end{align*} 


Now, we need the Weak Law of Large Numbers. That is, if we have a sequence of random variables $X_1, X_2, \dots$ with finite variance, then,
\[ \lim_{n \to \infty} \left( \frac{1}{n} \sum_{i=1}^n X_i \right)  = E(X_i). \]

Applying this law to both the numerator and denominator, we obtain
\begin{align*}  \lim_{m \to \infty} \left( \dfrac{ \frac 1m {\sum_{i=1}^mh(Y_i) }  \;\;\mathbb{I} (Y_i \leq x) }{\frac 1m \sum_{i=1}^mh(Y_i) } \right) &= \dfrac{ E_g[ h(Y) \;\; \mathbb I (Y \leq x) ]  }{ E_g[ h(Y) ]  }\\
&= \dfrac{\int_{-\infty}^\infty h(y) \;\; \mathbb I (y \leq x) \; g(y) \; dy}{\int_{-\infty}^\infty h(y) \, g(y) \;dy}\\
&= \dfrac{\int_{-\infty}^x h(y) \, g(y) dy}{\int_{-\infty}^\infty h(y) \, g(y) \;dy}\\
&\propto \int_{-\infty}^x h(y) \, g(y) dy. 
\end{align*}

It follows that the probability density function of $Z$ is proportional to $h \cdot g$.

\vspace{3 mm}


\qed


### Obtaining Logarithmic Pooled Distribution with the Sampling-Importance-Resampling Algorithm {#logpooled}


As outlined in @carvalho2023, we can formally define logarithmic pooling as follows.


If we have a set of densities $\{ f_1(\phi), f_2(\phi), \ldots, f_n(\phi)\}$ and corresponding pooling weights $\boldsymbol{\alpha}=\{\alpha_1, \alpha_2, \ldots, \alpha_n\}$, then the pooled density is 

$$ t(\boldsymbol{\alpha}) \prod_{i=0}^n f_i(\phi)^{\alpha_i}$$ where $t(\boldsymbol{\alpha})$ is the normalizing constant $t(\boldsymbol{\alpha}) = \dfrac{1}{ \int_{\Phi}\prod_{i=0}^n f_i(\phi)^{\alpha_i} d\phi}$ to ensure the pooled density is a valid probability density. 


The case for this work is more simple: we only have two densities we wish to pool, $f_\phi^{induced}$ and $f_\phi^{direct}$, and we assign them equal weights by letting $\boldsymbol{\alpha} = \{.5, .5\}$. This  yields

$$f^{pooled}(\phi) = t(\boldsymbol \alpha) \left( f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi) \right)^{0.5}.$$

Since our target distribution is $t(\boldsymbol \alpha) \left( f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi) \right)^{0.5}$, and we have a sample from $f^{induced}$, we compute the weights such that 

\begin{align*} w_i &\propto \dfrac{ \left( f^{induced} (\phi_i) \right)^{0.5} \left( f^{direct} (\phi_i) \right)^{0.5} } {f^{induced}(\phi_i)} \\
&=  \dfrac{ \left( f^{direct} (\phi_i) \right)^{0.5} } {\left( f^{induced} (\phi_i) \right)^{0.5} } \\
&=   \left( \dfrac{  f^{direct} (\phi_i) } { f^{induced} (\phi_i) }  \right)^{0.5}. \\
\end{align*} 

Sampling from $f^{induced}$ with these weights will yield a sample with approximately the target density $t(\alpha) \left(f^{induced} (\phi) \right)^{0.5} \left( f^{direct} (\phi)\right)^{0.5}$ from the result in the [previous section](#proof). 



```{r,eval=FALSE,include=FALSE}
### Theorem 1: Distribution After Resampling with Sampling Weights 


After some searching, I found a formulation of this problem [here](https://stats.stackexchange.com/questions/599440/density-of-sampled-exponential-data-with-sampling-weights-proportional-to-x-its). 

Suppose we have a random variable $X$ with PDF $f_X$ and are sampling with weights $h(x)$ for a nonnegative function $h$. We are interested in determining the distribution after sampling.

Letting $U$ denote the event that $X$ was sampled, the probability that a given value of $X$ is sampled is $P(U=1|X=x) = h(x)$. We want the post-sampling distribution, that is, $P(X \leq z | U = 1)$. 

By the definition of conditional probability we have $P(X \leq z | U = 1) = \dfrac{P(X \leq z , U = 1)}{ P(U=1)}$,

and since we will always be sampling at least some values, $P(U=1)$ will always be a nonzero constant.

This gives us 

$$P(X \leq z | U = 1) = \dfrac{1}{ P(U=1)} P(X \leq z , U = 1) \tag{1}.$$
Since the joint probability is $$P(X \leq z, U= 1) = P(U=1|X\leq z) \; P(X\leq z),$$  we write it as the integral $$P(X \leq z, U = 1) = \int^z P(U=1|X=x) \; f_X(x) \; dx.$$

Substituting this expression of the joint probability into (1), we have


$$P(X \leq z | U = 1) = \dfrac{1}{ P(U=1)} \int^z P(U=1|X=x) \; f_X(x) \; dx.$$

As defined, $h(x) = P(U=1|X=x)$, so we have 

$$P(X \leq z | U = 1) = \dfrac{1}{ P(U=1)} \int^z h(x) \; f_X(x) \; dx.$$
$$ \propto \int^z h(x) \; f_X(x) \; dx.$$

```
\newpage 

## LOESS Smoothing 

### Introduction

Locally estimated scatterplot smoothing (LOESS) fits a collection of local regression models to obtain a smooth curve through the observed data (Figure \ref{fig:loess}). It is highly flexible in the sense that we do not have to specify the functional relationship between the predictor and response variable for the entire range of the predictor, which may be impossible in various settings. It is particularly useful when working with time series data with substantial noise.


```{r, fig.cap="\\label{fig:loess}", echo = FALSE, fig.asp = .6}

set.seed(123)
smoothing_span <- .2
y<- rnorm(1e3, mean = 10, sd = 80)
div <- seq(1,5, length = 1000)
y <- cumsum(y)
y <- y /div

data <- tibble(x = 1:1000, y = y)


smoothed <- loess(y ~ x, data = data, span = smoothing_span) %>%
  predict()

data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = .7, alpha = .5) +
  geom_line(aes(y= smoothed), 
            color = "darkred",
            size = 1.2) +
  labs(title = "Locally Estimated Scatterplot (LOESS)\nSmoothing Curve",
       subtitle = paste0("Span = ", smoothing_span)) +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, face="bold", size = 14),
        plot.subtitle = element_text(hjust = .5, face="italic", size = 14),
        axis.title = element_text(size = 14))


```


To perform LOESS smoothing, we estimate a set of local regressions [@chambers1997]. To do this, we must specify the span; this smoothing parameter is the fraction of the data that is used for the local polynomial fit. With a smaller span, the resulting curve will fit the trends more closely, while a larger span reflects broader trends (Figure \ref{fig:smooth-spans}).

```{r, fig.cap="\\label{fig:smooth-spans}", fig.asp = .67}


smoothed_all <- map_df(c(.1,.3, .5,.7, .9), ~
                         {
                          smoothed <- loess(y ~ x, 
                                            data = data, 
                                            span = .x) %>%
                            predict()
                          
                           data %>%
                             mutate(span = .x,
                                    smoothed = smoothed)
                         })


smoothed_all %>%
  mutate(span = as.factor(span)) %>%
  ggplot(aes(x=x, y = y))+
  geom_point(size = .5, alpha = .4) +
  geom_line(aes(y= smoothed, color = span), 
            size = 1) +
  viridis::scale_color_viridis(discrete=TRUE, 
                               direction = -1, end = .9)+
  labs(title = "Locally Estimated Scatterplot (LOESS)\nSmoothing Curve",
       subtitle = paste0("By Span")) +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, face="bold", size = 14),
        plot.subtitle = element_text(hjust = .5, face="italic", size = 14),
        axis.title = element_text(size = 14))

  



```

### Fitting the LOESS Curve

To introduce some notation for the model at hand, we have a dependent variable $\mathbf y$ and independent variable $\mathbf  x$, where $\mathbf  y$ and $\mathbf x$ are related by some unknown function $g$, that is,  $y = g(x) + \boldsymbol \epsilon$^[Recall we use bold type for vectors, e.g., $\mathbf x \in \mathbb R^n$ is a vector with observations $x_i \in \mathbb R$.]. When we want to use LOESS smoothing to estimate $g$, often this function is complex, so we break up the problem into estimating a set of local regressions.

To obtain a predicted value $\hat g(x^*)$ for a particular value of the independent variable $x^*$,  we fit a polynomial with greatest weight placed on points in the neighborhood of $x^*$, where the width of this neighborhood is defined by the choice of smoothing span. Let $\alpha \in (0,1]$ denote the chosen smoothing span.

For a particular value of $x^*$, we estimate the predicted value $\hat g(x^*)$ by fitting a local regression. We first compute the weights by computing the vector of distances from this point $x^*$, that is,

$$\Delta (x^*) = |\mathbf x -x^* | $$

We define $q = \text{floor}(\alpha n)$, and take  $\Delta_q(x^*) \in \mathbb R$  to be the $q^th$ smallest distance of $\Delta (x^*)$.

The vector of weights is then 
$$T(\Delta(x^*), \Delta_q(x^*))$$

where $T$ is the tricube weight function given by

$$
T(x) = \begin{cases} (1-(x)^3)^3 \hspace{9 mm}  \text{ for } |x| < 1\\
0  \hspace{25 mm} \text{ for |x| } \geq 1 \end{cases}.
$$
Essentially, this process gives weight to points in the neighborhood of $x^*$. Consider $x^* = 500$ and $\text{smoothing span} = \alpha = .2$.

Then the weights we obtain are given in Figure \ref{fig:weights}.

```{r, warning = FALSE, fig.cap = "\\label{fig:weights} The only values with nonzero weights are those within the interval $(500 - \\alpha (n), 500 - \\alpha (n))$. That is, the proportion $\\alpha$ of the data points closest to $x^*$ will have nonzero weights.", fig.asp= 0.61, fig.width =7}



tricube <- function(x) {
  ifelse(abs(x) < 1, (1-(abs(x))^3)^3, 0) 
}



alpha <- 0.2
x_star <- 500
width <- alpha*length(data$x)

delta_q <- sort(abs(x_star-data$x), decreasing = FALSE)[floor(alpha*1000)]
weights <- tricube( abs(x_star - data$x )/(delta_q))


data %>%
  mutate(Weight = weights,
         lower = x_star -width/2,
         upper = x_star + width/2) %>%
  ggplot(aes(x = x, y =weights)) +
  geom_point(size = .8) +
  geom_vline(aes(xintercept = upper),
             color = "darkred", alpha = .6) +
  geom_vline(aes(xintercept = lower),
                 color = "darkred", alpha = .6) +
  annotate(x = 325, geom= "text", y = .5, 
           label = TeX("$500 - (\\alpha)(n)$"), parse =TRUE) +
  annotate(x = 675, geom= "text", y = .5, 
           label = TeX("$500 + (\\alpha)(n)$"), parse =TRUE) +
  theme_bw()+
  theme(plot.title = element_text(hjust = .5, face="bold", size = 14),
        plot.subtitle = element_text(hjust = .5, face="italic", size = 14),
        axis.title = element_text(size = 14)) +
  labs(title = TeX("Weights Computed for $x^* = 500$"))




```

\newpage 

We fit a linear regression with polynomial terms, typically with degree up to 2, with these weights. For example, fitting the model for this same $x^*=500$, we obtain the polynomial in Figure \ref{fig:ex-poly}.

\begin{flushleft}

```{r, fig.cap = "\\label{fig:ex-poly}", fig.asp = .52, fig.width = 5}

model <- lm(y ~ x + I(x^2), weights = weights, data = data)

data %>%
  mutate(Weight = weights,
         predicted = predict(model)) %>%
  ggplot(aes(x = x, y =y)) +
  geom_point(size = .5, alpha = .6) +
  geom_line(aes(y = predicted), size = 1.3, color = "#728BC8") +
  geom_point(x =500, 
             y = data[data$x==x_star,]$y,
             size =4,
             color = "orange") +
  theme_bw()+
  theme(plot.title = element_text(hjust = .5, face="bold", size = 12),
        plot.subtitle = element_text(hjust = .5, face="italic", size = 12),
        axis.title = element_text(size = 12)) +
  labs(title = TeX("Local Model Estimated for Neighborhood about $x^* = 500$")) +
  ylim(min(data$y)- 50, max(data$y) + 50)


```

\end{flushleft}

By fitting the model for every point in $\mathbf x$, we obtain the smoothed line shown in red in Figure \ref{fig:loess-all}.

```{r, fig.cap = "\\label{smooth-functions}"}

get_predicted <- function(x_i, all = FALSE) {

  delta_q <- sort((abs(x_i-data$x)), decreasing = FALSE)[floor(alpha*1000)]

  weights <- tricube( abs(x_i - data$x )/(delta_q))
  
  model <- lm(y ~ x + I(x^2), weights = weights, data = data)
  
  if(all) predict(model) 
  else predict(model, newdata = list(x=x_i))
}



get_predicted_all <- function(x_i) {


  delta_q <- sort(abs(x_i-data$x), decreasing = FALSE)[floor(alpha*1000)]
 # message(delta_q)
  
  weights <- tricube((data$x -x_i)/(delta_q))
  
  # message(head(weights))
  model <- lm(y ~ x + I(x^2), weights = weights, data = data)
  
   predict(model)
  
}

```

```{r loess-all, fig.cap = "\\label{loess-all}", fig.asp = .6}

predicted_all <- map_df(data$x, ~tibble(fit = paste0("fit_", .x), 
                                        x_pred = .x,
                                        x = data$x,
                                        y = data$y,
                                        fitted = get_predicted(.x, all = TRUE)))


predicted_all %>%
  group_by(x_pred) %>%
  # get smoothed line
  mutate(fitted_star =  fitted[which(x==x_pred)]) %>%
  ungroup() %>%
  ggplot() +
  geom_line(aes(x = x, y = fitted, 
                color = fit, 
                group = fit),
            alpha = .5,
            show.legend=FALSE) +
  geom_line(aes(x = x_pred, y = fitted_star), color = "darkred", size = 1.2) +
#  geom_point(alpha = .5, aes(x =x, y = y), show.legend = FALSE) +
  ylim(min(data$y)-10, max(data$y) + 10) +
  viridis::scale_color_viridis(option="mako", discrete = TRUE) +
  theme_bw()+
  theme(plot.title = element_text(hjust = .5, 
                                  face="bold",
                                  size = 12),
        plot.subtitle = element_text(hjust = .5, 
                                     face="italic",
                                     size = 12),
        axis.title = element_text(size = 12)) +
  labs(title = TeX("All Local Models to Produce LOESS Curve"),
       y  = TeX("$\\hat{y}$"))


```

```{r, include = FALSE, eval = FALSE}
dat <- data %>%
  mutate(pred = map_dbl(x, get_predicted))

dat %>%
  mutate(pred2 = predict(smoothed)) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  geom_line(aes(y= pred), color = "darkred") +
  geom_line(aes(y = pred2))


# predicted_all %>%
#   filter(fit %in% paste0("fit_", c(500, 600, 700))) %>%
#   ggplot(aes(x =x,y = fitted, color = fit, group = fit)) +
#   geom_line(show.legend=FALSE) +
#   geom_line(alpha = .5, aes(y = y)) +
#   ylim(min(data$y)-10, max(data$y) + 10)




y<- rnorm(1e3, mean = 10, sd = 80)
div <- seq(1,5, length = 1000)
y <- cumsum(y)
y <- y /div

data <- tibble(x = 1:1000, y = y)

smoothed_1 <- loess(y ~ x, data = data[1:500,], span = 1) %>%
  predict()
smoothed_2 <- loess(y ~ x, data = data[501:1000,], span = 1) %>%
  predict()

smoothed_3 <-  loess(y ~ x, data = data[1:1000,], 
                     span = .5, 
                     model = TRUE,
                     control = loess.control( surface = c("interpolate")),
                     cell = 1) %>%
  predict()


alpha <- .2


# compute fitted value for y_i

x_i <- 1

weights <- c()



tricube <- function(x) {
  ifelse((x) < 1, (1-((x))^3)^3, 0) 
}

# weights <- tricube( u = (x_i - data$x ), t = (min(x_i-data$x)))


delta_q <- sort(x_i-data$x, decreasing = TRUE)[floor(alpha*1000)]


weights <- tricube(abs(x_i - data$x )/(delta_q))



model <- lm(y ~ x + I(x^2), weights = weights, data = data)

data %>% mutate(predicted = predict(model)) %>%
  filter(x==x_i)

model$coefficients


data %>% mutate(first_fit = predict(model)) %>%
  ggplot(aes(x = x, y = first_fit)) +
   geom_line() +
  geom_point(aes(x=x,y=y)) +
  ylim(min(NA), max(data$y))


# automate 




tricube <- function(x) {
  ifelse(abs(x) < 1, (1-(abs(x))^3)^3, 0) 
}


get_predicted <- function(x_i, all = FALSE) {

  delta_q <- sort((abs(x_i-data$x)), decreasing = FALSE)[floor(alpha*1000)]

  weights <- tricube( abs(x_i - data$x )/(delta_q))
  
  model <- lm(y ~ x + I(x^2), weights = weights, data = data)
  
  if(all) predict(model) 
  else predict(model, newdata = list(x=x_i))
}



get_predicted_all <- function(x_i) {


  delta_q <- sort(abs(x_i-data$x), decreasing = FALSE)[floor(alpha*1000)]
 # message(delta_q)
  
  weights <- tricube((data$x -x_i)/(delta_q))
  
  # message(head(weights))
  model <- lm(y ~ x + I(x^2), weights = weights, data = data)
  
   predict(model)
  
}

predicted_all <- map_df(data$x, ~tibble(fit = paste0("fit_", .x), 
                                        x = data$x,
                                        y = data$y,
                                        fitted = get_predicted(.x, all = TRUE)))



predicted_all %>%
  filter(fit %in% paste0("fit_", c(500, 600, 700))) %>%
  ggplot(aes(x =x,y = fitted, color = fit, group = fit)) +
  geom_line(show.legend=FALSE) +
  geom_line(alpha = .5, aes(y = y)) +
  ylim(min(data$y)-10, max(data$y) + 10)

predicted_all %>%
#  filter(fit %in% paste0("fit_", c(500, 600, 700))) %>%
  ggplot(aes(x =x,y = fitted, color = fit, group = fit)) +
  geom_line(show.legend=FALSE) +
  geom_line(alpha = .5, aes(y = y), show.legend = FALSE) +
  ylim(min(data$y)-10, max(data$y) + 10) +
  viridis::scale_color_viridis(option="mako", discrete = TRUE)

dat <- data %>%
  mutate(pred = map_dbl(x, get_predicted))

dat %>%
  mutate(pred2 = predict(smoothed)) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  geom_line(aes(y= pred), color = "darkred") +
  geom_line(aes(y = pred2))


```

```{r, include = FALSE, eval = FALSE}
smoothed <- loess(y ~ x, data =data, span = .2)

# verify correspondence with loess function
dat %>%
  mutate(prediction_loess = predict(smoothed)) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  geom_line(aes(y= pred), color = "darkred") +
  geom_line(aes(y = prediction_loess))


```


Smoothing methods are sensitive to the choice of smoothing parameter $h$, which represents the fraction of the data that is used for the local polynomial fit.


Methods exist for picking the smoothing parameter $h$ that minimizes the mean squared error between the predicted values from the estimated line and observed values of the dependent variable, for example, leave-one-out cross-validation or generalized cross-validation. 

However, for this work, we used LOESS smoothing to smooth survey data from the COVID-19 Trends and Impact Survey [@reinhart2021]. 
We choose the smoothing parameter for each variable based on domain knowledge regarding the level of noise present for each variable of interest. For example, there is substantial noise in the screening test positivity data that reflect trends that do not represent meaningful differences in the screening test positivity. Some trends in the screening sensitivity may be due to scheduled workplace screenings happening at regular time intervals, and some of the variation may be due to the frequency of screening testing due to other variables, such as the access and cost of testing.

Since the ratio  $\frac{\text{screening test positivity}}{\text{overall test positivity}}$ is used to estimate $\beta = \frac{P(\text{test}_+| S_0, untested)}{P(\text{test}_+|tested)}$, the variability in the screening positivity creates substantial variability in our estimates of $\beta$.

In light of this variability and the presence of other trends regarding the screening test positivity, we set the span  to $\frac{4}{12} = 0.33$ to fit the local regressions for 4-month intervals with the aim to capture the broader trends over time.

INCLUDE FIGURE OF SMOOTHED ESTIMATES HERE 

There was less variabiity in the smoothing span for the weighted percentage of COVID-like Illness, the estimate of $P(S_1|untested)$. Hence, we set the smoothing parameter to $0.2$ detect trends at a finer time scale.

Sensitivity analyses with modified versions of the smoothing span of $\beta$ are included in the appendix in the section INCLUDE SECTION. 


\newpage



## Kernel Density Estimation


## Sampling Importance Resampling

